{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deepfake_Image_Generation.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucG2_aTVrA3C"
      },
      "source": [
        "# Importing the required libraries which we will be using in the course of this implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSP6uxa1qlLm"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os,pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynzuv-BirKua"
      },
      "source": [
        "# A sample image in the CelebA Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "9X3fZdglsxyl",
        "outputId": "3821f7f9-190d-420a-e656-3fa117fc6efa"
      },
      "source": [
        "path='/content/drive/MyDrive/celeba'+'/161979.jpg'\n",
        "img=cv2.imread(path)\n",
        "img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3f9cea4310>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rUhApbUlB8M",
        "outputId": "34db390c-8ccb-4978-d3da-88c6a33285ff"
      },
      "source": [
        "print(img.shape) #To show the size of the image\n",
        "print(type(img)) #Data type of the image"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 64, 3)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fUAl9hDlPkb"
      },
      "source": [
        "##Creating our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6HsAY7rm4Zi"
      },
      "source": [
        "#24396 images in the celeba dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEWNU0rgr0io"
      },
      "source": [
        "## Reading in 5,000 images for the purposes of training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB5AMfK5lGaE"
      },
      "source": [
        "path_d='/content/drive/MyDrive/celeba'\n",
        "ct=161979\n",
        "X = np.zeros((5000,64 , 64, 3), dtype=np.float32) #Creating a numpy array for 5,000 RGB Images\n",
        "for path in pathlib.Path(path_d).iterdir(): #Iterating over the directory of images\n",
        "  img = cv2.imread(str(path)) #Reading in this images\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Converting the image from BGR format (i.e the default format it is read in), to RGB\n",
        "  img = img.astype('float32') #Setting the datatype as a 32 bit float value\n",
        "  X[ct-161979]=img/255 #Normalizing images for avoiding the problem of exploding gradients and improving convergence speed\n",
        "  if (ct-161979)==4999:\n",
        "    break\n",
        "  ct+=1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtMm1SgIh2s1"
      },
      "source": [
        "##Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1F9ROFsnBd9"
      },
      "source": [
        "input_img = layers.Input(shape=(64, 64, 3)) #Creating our encoder network\n",
        "x = layers.Conv2D(128,kernel_size=5, strides=2, padding='same',activation='relu')(input_img) #Downsampling the image using a 128 unit Conv2D layer\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(256,kernel_size=5, strides=2, padding='same',activation='relu')(x) #Downsampling the image using a 256 unit Conv2D layer\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(512,kernel_size=5, strides=2, padding='same',activation='relu')(x)  #Downsampling the image using a 128 unit Conv2D layer\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Flatten()(x) #Flattening the image and then, converting it into a dense 4608 length vector, 4608=3*3*512\n",
        "x = layers.Dense(4608)(x)\n",
        "encoded = layers.Reshape((3,3,512))(x) # Reshaping the image into this format for implementing the process of Upsampling\n",
        "encoder = keras.Model(input_img, encoded,name=\"encoder\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ZE3Mxxh9uf"
      },
      "source": [
        "##Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWgAGa7ah4km"
      },
      "source": [
        "decoder_input= layers.Input(shape=((3,3,512))) #Reading the earlier input from the encoder\n",
        "x = layers.Conv2D(512,kernel_size=5, strides=2, padding='same',activation='relu')(decoder_input) #Upsampling the image starting with a 512 unit Conv2D layer\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(256,kernel_size=5, strides=2, padding='same',activation='relu')(x) #Upsampling the image using a 256 unit Conv2D layer\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(128,kernel_size=5, strides=2, padding='same',activation='relu')(x) #Upsampling the image using a 128 unit Conv2D layer\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(np.prod((64, 64, 3)))(x) #Flattening the image and then, converting it into a (64,64,3) image output\n",
        "decoded = layers.Reshape((64, 64, 3))(x)\n",
        "decoder = keras.Model(decoder_input, decoded,name=\"decoder\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lM63ZlBCJvV"
      },
      "source": [
        "## Creating the encoder and decoder (combined) model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAvh3LF6h_Dc",
        "outputId": "2bf9c47b-a191-405b-bf78-d6a432644840"
      },
      "source": [
        "auto_input = layers.Input(shape=(64,64,3)) \n",
        "encoded = encoder(auto_input)\n",
        "decoded = decoder(encoded)\n",
        " \n",
        "autoencoder = keras.Model(auto_input, decoded,name=\"autoencoder\")\n",
        "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(lr=5e-5, beta_1=0.5, beta_2=0.999), loss='mae')\n",
        "# loss used here is 'mean absolute error'\n",
        "autoencoder.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 3, 3, 512)         6470400   \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 64, 64, 3)         16954240  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,424,640\n",
            "Trainable params: 23,424,640\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1ou2w-LCgd6"
      },
      "source": [
        "##Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBMYMvesiB5z"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,X, test_size=0.20, random_state=0) #Doing an 80-20 split"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-utjkgMCidz"
      },
      "source": [
        "##Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdUK2i0uCkCQ"
      },
      "source": [
        "Training the model for 1,000 epochs with a batch size of 128 and saving the model with the least validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8NSwOkFlrWA",
        "outputId": "f129cc62-fbe3-49fe-c384-a11dc8ccb257"
      },
      "source": [
        "checkpoint1 = ModelCheckpoint(\"autoencoder_celeba.hdf5\", monitor='val_loss', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "history1 = autoencoder.fit(X_train, X_train, epochs=1000, batch_size=128, shuffle=True, validation_data=(X_test, X_test), callbacks=[checkpoint1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/1000\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.3384\n",
            "Epoch 00001: val_loss improved from inf to 0.16625, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 20s 93ms/step - loss: 0.3384 - val_loss: 0.1662\n",
            "Epoch 2/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1575\n",
            "Epoch 00002: val_loss improved from 0.16625 to 0.15499, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.1575 - val_loss: 0.1550\n",
            "Epoch 3/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1557\n",
            "Epoch 00003: val_loss improved from 0.15499 to 0.15435, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 3s 93ms/step - loss: 0.1559 - val_loss: 0.1543\n",
            "Epoch 4/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1547\n",
            "Epoch 00004: val_loss improved from 0.15435 to 0.15387, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 3s 89ms/step - loss: 0.1547 - val_loss: 0.1539\n",
            "Epoch 5/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1542\n",
            "Epoch 00005: val_loss did not improve from 0.15387\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.1543 - val_loss: 0.1553\n",
            "Epoch 6/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1535\n",
            "Epoch 00006: val_loss did not improve from 0.15387\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1535 - val_loss: 0.1572\n",
            "Epoch 7/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1545\n",
            "Epoch 00007: val_loss improved from 0.15387 to 0.15347, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 71ms/step - loss: 0.1545 - val_loss: 0.1535\n",
            "Epoch 8/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1534\n",
            "Epoch 00008: val_loss did not improve from 0.15347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1535 - val_loss: 0.1553\n",
            "Epoch 9/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1546\n",
            "Epoch 00009: val_loss did not improve from 0.15347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1546 - val_loss: 0.1544\n",
            "Epoch 10/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1538\n",
            "Epoch 00010: val_loss did not improve from 0.15347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1536 - val_loss: 0.1561\n",
            "Epoch 11/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1536\n",
            "Epoch 00011: val_loss did not improve from 0.15347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1536 - val_loss: 0.1540\n",
            "Epoch 12/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1531\n",
            "Epoch 00012: val_loss improved from 0.15347 to 0.14855, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.1531 - val_loss: 0.1485\n",
            "Epoch 13/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1487\n",
            "Epoch 00013: val_loss did not improve from 0.14855\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.1487 - val_loss: 0.1493\n",
            "Epoch 14/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1409\n",
            "Epoch 00014: val_loss improved from 0.14855 to 0.13754, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.1410 - val_loss: 0.1375\n",
            "Epoch 15/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1381\n",
            "Epoch 00015: val_loss did not improve from 0.13754\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1380 - val_loss: 0.1396\n",
            "Epoch 16/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1374\n",
            "Epoch 00016: val_loss did not improve from 0.13754\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1374 - val_loss: 0.1389\n",
            "Epoch 17/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1369\n",
            "Epoch 00017: val_loss improved from 0.13754 to 0.13658, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.1369 - val_loss: 0.1366\n",
            "Epoch 18/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1363\n",
            "Epoch 00018: val_loss improved from 0.13658 to 0.13408, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 3s 83ms/step - loss: 0.1362 - val_loss: 0.1341\n",
            "Epoch 19/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1341\n",
            "Epoch 00019: val_loss improved from 0.13408 to 0.13150, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.1341 - val_loss: 0.1315\n",
            "Epoch 20/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1311\n",
            "Epoch 00020: val_loss improved from 0.13150 to 0.12831, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 75ms/step - loss: 0.1310 - val_loss: 0.1283\n",
            "Epoch 21/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1272\n",
            "Epoch 00021: val_loss did not improve from 0.12831\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1272 - val_loss: 0.1340\n",
            "Epoch 22/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1257\n",
            "Epoch 00022: val_loss did not improve from 0.12831\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1257 - val_loss: 0.1310\n",
            "Epoch 23/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1246\n",
            "Epoch 00023: val_loss improved from 0.12831 to 0.12666, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.1246 - val_loss: 0.1267\n",
            "Epoch 24/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1239\n",
            "Epoch 00024: val_loss improved from 0.12666 to 0.12505, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 3s 83ms/step - loss: 0.1239 - val_loss: 0.1251\n",
            "Epoch 25/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1242\n",
            "Epoch 00025: val_loss improved from 0.12505 to 0.12323, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.1243 - val_loss: 0.1232\n",
            "Epoch 26/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1228\n",
            "Epoch 00026: val_loss did not improve from 0.12323\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1227 - val_loss: 0.1246\n",
            "Epoch 27/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1231\n",
            "Epoch 00027: val_loss did not improve from 0.12323\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1230 - val_loss: 0.1244\n",
            "Epoch 28/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1223\n",
            "Epoch 00028: val_loss did not improve from 0.12323\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1224 - val_loss: 0.1240\n",
            "Epoch 29/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1205\n",
            "Epoch 00029: val_loss improved from 0.12323 to 0.11933, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 71ms/step - loss: 0.1205 - val_loss: 0.1193\n",
            "Epoch 30/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1200\n",
            "Epoch 00030: val_loss did not improve from 0.11933\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1199 - val_loss: 0.1200\n",
            "Epoch 31/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1180\n",
            "Epoch 00031: val_loss did not improve from 0.11933\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1179 - val_loss: 0.1195\n",
            "Epoch 32/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1171\n",
            "Epoch 00032: val_loss improved from 0.11933 to 0.11829, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.1171 - val_loss: 0.1183\n",
            "Epoch 33/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1153\n",
            "Epoch 00033: val_loss improved from 0.11829 to 0.11743, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 77ms/step - loss: 0.1152 - val_loss: 0.1174\n",
            "Epoch 34/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1153\n",
            "Epoch 00034: val_loss improved from 0.11743 to 0.11451, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.1153 - val_loss: 0.1145\n",
            "Epoch 35/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1142\n",
            "Epoch 00035: val_loss did not improve from 0.11451\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1143 - val_loss: 0.1162\n",
            "Epoch 36/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1118\n",
            "Epoch 00036: val_loss improved from 0.11451 to 0.11293, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 76ms/step - loss: 0.1118 - val_loss: 0.1129\n",
            "Epoch 37/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1082\n",
            "Epoch 00037: val_loss improved from 0.11293 to 0.10895, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.1082 - val_loss: 0.1089\n",
            "Epoch 38/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1078\n",
            "Epoch 00038: val_loss improved from 0.10895 to 0.10835, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 76ms/step - loss: 0.1078 - val_loss: 0.1083\n",
            "Epoch 39/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1080\n",
            "Epoch 00039: val_loss improved from 0.10835 to 0.10808, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 77ms/step - loss: 0.1079 - val_loss: 0.1081\n",
            "Epoch 40/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1059\n",
            "Epoch 00040: val_loss improved from 0.10808 to 0.10660, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 3s 92ms/step - loss: 0.1058 - val_loss: 0.1066\n",
            "Epoch 41/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1058\n",
            "Epoch 00041: val_loss did not improve from 0.10660\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1058 - val_loss: 0.1093\n",
            "Epoch 42/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1051\n",
            "Epoch 00042: val_loss did not improve from 0.10660\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1051 - val_loss: 0.1074\n",
            "Epoch 43/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1048\n",
            "Epoch 00043: val_loss improved from 0.10660 to 0.10578, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.1049 - val_loss: 0.1058\n",
            "Epoch 44/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1047\n",
            "Epoch 00044: val_loss improved from 0.10578 to 0.10388, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.1048 - val_loss: 0.1039\n",
            "Epoch 45/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1036\n",
            "Epoch 00045: val_loss did not improve from 0.10388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1037 - val_loss: 0.1072\n",
            "Epoch 46/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1041\n",
            "Epoch 00046: val_loss improved from 0.10388 to 0.10294, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.1041 - val_loss: 0.1029\n",
            "Epoch 47/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1036\n",
            "Epoch 00047: val_loss did not improve from 0.10294\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1036 - val_loss: 0.1046\n",
            "Epoch 48/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1037\n",
            "Epoch 00048: val_loss did not improve from 0.10294\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1038 - val_loss: 0.1060\n",
            "Epoch 49/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1031\n",
            "Epoch 00049: val_loss did not improve from 0.10294\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1031 - val_loss: 0.1053\n",
            "Epoch 50/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1027\n",
            "Epoch 00050: val_loss did not improve from 0.10294\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1027 - val_loss: 0.1038\n",
            "Epoch 51/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1027\n",
            "Epoch 00051: val_loss improved from 0.10294 to 0.10215, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.1027 - val_loss: 0.1022\n",
            "Epoch 52/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1020\n",
            "Epoch 00052: val_loss did not improve from 0.10215\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1020 - val_loss: 0.1043\n",
            "Epoch 53/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1008\n",
            "Epoch 00053: val_loss did not improve from 0.10215\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1008 - val_loss: 0.1084\n",
            "Epoch 54/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1021\n",
            "Epoch 00054: val_loss did not improve from 0.10215\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1021 - val_loss: 0.1022\n",
            "Epoch 55/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1008\n",
            "Epoch 00055: val_loss improved from 0.10215 to 0.10116, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.1008 - val_loss: 0.1012\n",
            "Epoch 56/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1007\n",
            "Epoch 00056: val_loss improved from 0.10116 to 0.10060, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 79ms/step - loss: 0.1007 - val_loss: 0.1006\n",
            "Epoch 57/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1006\n",
            "Epoch 00057: val_loss did not improve from 0.10060\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1005 - val_loss: 0.1007\n",
            "Epoch 58/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1003\n",
            "Epoch 00058: val_loss did not improve from 0.10060\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.1002 - val_loss: 0.1025\n",
            "Epoch 59/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1005\n",
            "Epoch 00059: val_loss improved from 0.10060 to 0.10035, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.1006 - val_loss: 0.1003\n",
            "Epoch 60/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0999\n",
            "Epoch 00060: val_loss improved from 0.10035 to 0.09930, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 3s 84ms/step - loss: 0.0998 - val_loss: 0.0993\n",
            "Epoch 61/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0999\n",
            "Epoch 00061: val_loss did not improve from 0.09930\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.1000 - val_loss: 0.1003\n",
            "Epoch 62/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0987\n",
            "Epoch 00062: val_loss did not improve from 0.09930\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0987 - val_loss: 0.1007\n",
            "Epoch 63/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0987\n",
            "Epoch 00063: val_loss did not improve from 0.09930\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0987 - val_loss: 0.1000\n",
            "Epoch 64/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0987\n",
            "Epoch 00064: val_loss improved from 0.09930 to 0.09601, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 71ms/step - loss: 0.0987 - val_loss: 0.0960\n",
            "Epoch 65/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0976\n",
            "Epoch 00065: val_loss did not improve from 0.09601\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0976 - val_loss: 0.0990\n",
            "Epoch 66/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0972\n",
            "Epoch 00066: val_loss did not improve from 0.09601\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0971 - val_loss: 0.1012\n",
            "Epoch 67/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0967\n",
            "Epoch 00067: val_loss did not improve from 0.09601\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0967 - val_loss: 0.1007\n",
            "Epoch 68/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0958\n",
            "Epoch 00068: val_loss did not improve from 0.09601\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0958 - val_loss: 0.1021\n",
            "Epoch 69/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0950\n",
            "Epoch 00069: val_loss did not improve from 0.09601\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0950 - val_loss: 0.0985\n",
            "Epoch 70/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0952\n",
            "Epoch 00070: val_loss improved from 0.09601 to 0.09462, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0951 - val_loss: 0.0946\n",
            "Epoch 71/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0940\n",
            "Epoch 00071: val_loss improved from 0.09462 to 0.09334, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 3s 81ms/step - loss: 0.0940 - val_loss: 0.0933\n",
            "Epoch 72/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0941\n",
            "Epoch 00072: val_loss did not improve from 0.09334\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0941 - val_loss: 0.0935\n",
            "Epoch 73/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0931\n",
            "Epoch 00073: val_loss did not improve from 0.09334\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0931 - val_loss: 0.0992\n",
            "Epoch 74/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0930\n",
            "Epoch 00074: val_loss improved from 0.09334 to 0.09195, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 71ms/step - loss: 0.0930 - val_loss: 0.0920\n",
            "Epoch 75/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0922\n",
            "Epoch 00075: val_loss did not improve from 0.09195\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0921 - val_loss: 0.0950\n",
            "Epoch 76/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0930\n",
            "Epoch 00076: val_loss did not improve from 0.09195\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0929 - val_loss: 0.0955\n",
            "Epoch 77/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0919\n",
            "Epoch 00077: val_loss did not improve from 0.09195\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0919 - val_loss: 0.0956\n",
            "Epoch 78/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0919\n",
            "Epoch 00078: val_loss did not improve from 0.09195\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0919 - val_loss: 0.0941\n",
            "Epoch 79/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0909\n",
            "Epoch 00079: val_loss did not improve from 0.09195\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0910 - val_loss: 0.0948\n",
            "Epoch 80/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0914\n",
            "Epoch 00080: val_loss improved from 0.09195 to 0.09070, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0914 - val_loss: 0.0907\n",
            "Epoch 81/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0913\n",
            "Epoch 00081: val_loss did not improve from 0.09070\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0913 - val_loss: 0.0916\n",
            "Epoch 82/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0898\n",
            "Epoch 00082: val_loss did not improve from 0.09070\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0898 - val_loss: 0.0909\n",
            "Epoch 83/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0903\n",
            "Epoch 00083: val_loss improved from 0.09070 to 0.08907, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0902 - val_loss: 0.0891\n",
            "Epoch 84/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0895\n",
            "Epoch 00084: val_loss did not improve from 0.08907\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0894 - val_loss: 0.0908\n",
            "Epoch 85/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0897\n",
            "Epoch 00085: val_loss did not improve from 0.08907\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0897 - val_loss: 0.0896\n",
            "Epoch 86/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0889\n",
            "Epoch 00086: val_loss improved from 0.08907 to 0.08904, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0888 - val_loss: 0.0890\n",
            "Epoch 87/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0879\n",
            "Epoch 00087: val_loss improved from 0.08904 to 0.08808, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0879 - val_loss: 0.0881\n",
            "Epoch 88/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0888\n",
            "Epoch 00088: val_loss did not improve from 0.08808\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0888 - val_loss: 0.0906\n",
            "Epoch 89/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0883\n",
            "Epoch 00089: val_loss did not improve from 0.08808\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0883 - val_loss: 0.0915\n",
            "Epoch 90/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0881\n",
            "Epoch 00090: val_loss improved from 0.08808 to 0.08758, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0881 - val_loss: 0.0876\n",
            "Epoch 91/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0869\n",
            "Epoch 00091: val_loss did not improve from 0.08758\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0869 - val_loss: 0.0883\n",
            "Epoch 92/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0877\n",
            "Epoch 00092: val_loss did not improve from 0.08758\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0877 - val_loss: 0.0901\n",
            "Epoch 93/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0874\n",
            "Epoch 00093: val_loss did not improve from 0.08758\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0874 - val_loss: 0.0881\n",
            "Epoch 94/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0866\n",
            "Epoch 00094: val_loss improved from 0.08758 to 0.08654, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0865 - val_loss: 0.0865\n",
            "Epoch 95/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0862\n",
            "Epoch 00095: val_loss did not improve from 0.08654\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0862 - val_loss: 0.0876\n",
            "Epoch 96/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0869\n",
            "Epoch 00096: val_loss did not improve from 0.08654\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0869 - val_loss: 0.0870\n",
            "Epoch 97/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0859\n",
            "Epoch 00097: val_loss did not improve from 0.08654\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0859 - val_loss: 0.0876\n",
            "Epoch 98/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0858\n",
            "Epoch 00098: val_loss did not improve from 0.08654\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0858 - val_loss: 0.0879\n",
            "Epoch 99/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0855\n",
            "Epoch 00099: val_loss improved from 0.08654 to 0.08581, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0855 - val_loss: 0.0858\n",
            "Epoch 100/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0854\n",
            "Epoch 00100: val_loss did not improve from 0.08581\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0854 - val_loss: 0.0862\n",
            "Epoch 101/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0854\n",
            "Epoch 00101: val_loss improved from 0.08581 to 0.08555, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0853 - val_loss: 0.0855\n",
            "Epoch 102/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0847\n",
            "Epoch 00102: val_loss did not improve from 0.08555\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0847 - val_loss: 0.0928\n",
            "Epoch 103/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0850\n",
            "Epoch 00103: val_loss improved from 0.08555 to 0.08545, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0849 - val_loss: 0.0854\n",
            "Epoch 104/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0844\n",
            "Epoch 00104: val_loss did not improve from 0.08545\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0845 - val_loss: 0.0863\n",
            "Epoch 105/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0845\n",
            "Epoch 00105: val_loss did not improve from 0.08545\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0845 - val_loss: 0.0878\n",
            "Epoch 106/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0843\n",
            "Epoch 00106: val_loss improved from 0.08545 to 0.08373, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0844 - val_loss: 0.0837\n",
            "Epoch 107/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0835\n",
            "Epoch 00107: val_loss did not improve from 0.08373\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0835 - val_loss: 0.0875\n",
            "Epoch 108/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0837\n",
            "Epoch 00108: val_loss did not improve from 0.08373\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0837 - val_loss: 0.0879\n",
            "Epoch 109/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0833\n",
            "Epoch 00109: val_loss did not improve from 0.08373\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0833 - val_loss: 0.0852\n",
            "Epoch 110/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0836\n",
            "Epoch 00110: val_loss did not improve from 0.08373\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0836 - val_loss: 0.0901\n",
            "Epoch 111/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0827\n",
            "Epoch 00111: val_loss improved from 0.08373 to 0.08286, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0827 - val_loss: 0.0829\n",
            "Epoch 112/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0827\n",
            "Epoch 00112: val_loss did not improve from 0.08286\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0827 - val_loss: 0.0841\n",
            "Epoch 113/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0825\n",
            "Epoch 00113: val_loss did not improve from 0.08286\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0825 - val_loss: 0.0829\n",
            "Epoch 114/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0832\n",
            "Epoch 00114: val_loss improved from 0.08286 to 0.08259, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0832 - val_loss: 0.0826\n",
            "Epoch 115/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0822\n",
            "Epoch 00115: val_loss did not improve from 0.08259\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0822 - val_loss: 0.0830\n",
            "Epoch 116/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0823\n",
            "Epoch 00116: val_loss did not improve from 0.08259\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0823 - val_loss: 0.0835\n",
            "Epoch 117/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0819\n",
            "Epoch 00117: val_loss improved from 0.08259 to 0.08178, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0819 - val_loss: 0.0818\n",
            "Epoch 118/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0815\n",
            "Epoch 00118: val_loss did not improve from 0.08178\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0815 - val_loss: 0.0822\n",
            "Epoch 119/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0818\n",
            "Epoch 00119: val_loss did not improve from 0.08178\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0818 - val_loss: 0.0828\n",
            "Epoch 120/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0813\n",
            "Epoch 00120: val_loss did not improve from 0.08178\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0813 - val_loss: 0.0829\n",
            "Epoch 121/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0813\n",
            "Epoch 00121: val_loss did not improve from 0.08178\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0812 - val_loss: 0.0855\n",
            "Epoch 122/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0809\n",
            "Epoch 00122: val_loss did not improve from 0.08178\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0809 - val_loss: 0.0840\n",
            "Epoch 123/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0811\n",
            "Epoch 00123: val_loss did not improve from 0.08178\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0811 - val_loss: 0.0827\n",
            "Epoch 124/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0810\n",
            "Epoch 00124: val_loss did not improve from 0.08178\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0810 - val_loss: 0.0831\n",
            "Epoch 125/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0805\n",
            "Epoch 00125: val_loss did not improve from 0.08178\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0805 - val_loss: 0.0827\n",
            "Epoch 126/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0807\n",
            "Epoch 00126: val_loss did not improve from 0.08178\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0807 - val_loss: 0.0829\n",
            "Epoch 127/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0806\n",
            "Epoch 00127: val_loss did not improve from 0.08178\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0806 - val_loss: 0.0824\n",
            "Epoch 128/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0795\n",
            "Epoch 00128: val_loss did not improve from 0.08178\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0795 - val_loss: 0.0822\n",
            "Epoch 129/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0802\n",
            "Epoch 00129: val_loss improved from 0.08178 to 0.08028, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0802 - val_loss: 0.0803\n",
            "Epoch 130/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0796\n",
            "Epoch 00130: val_loss improved from 0.08028 to 0.07955, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 76ms/step - loss: 0.0796 - val_loss: 0.0795\n",
            "Epoch 131/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0794\n",
            "Epoch 00131: val_loss did not improve from 0.07955\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0794 - val_loss: 0.0801\n",
            "Epoch 132/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0795\n",
            "Epoch 00132: val_loss did not improve from 0.07955\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0795 - val_loss: 0.0833\n",
            "Epoch 133/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0798\n",
            "Epoch 00133: val_loss did not improve from 0.07955\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0798 - val_loss: 0.0812\n",
            "Epoch 134/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0791\n",
            "Epoch 00134: val_loss did not improve from 0.07955\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0791 - val_loss: 0.0812\n",
            "Epoch 135/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0794\n",
            "Epoch 00135: val_loss did not improve from 0.07955\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0794 - val_loss: 0.0821\n",
            "Epoch 136/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0788\n",
            "Epoch 00136: val_loss did not improve from 0.07955\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0789 - val_loss: 0.0843\n",
            "Epoch 137/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0792\n",
            "Epoch 00137: val_loss did not improve from 0.07955\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0792 - val_loss: 0.0797\n",
            "Epoch 138/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0780\n",
            "Epoch 00138: val_loss improved from 0.07955 to 0.07866, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 71ms/step - loss: 0.0780 - val_loss: 0.0787\n",
            "Epoch 139/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0782\n",
            "Epoch 00139: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0782 - val_loss: 0.0817\n",
            "Epoch 140/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0790\n",
            "Epoch 00140: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0789 - val_loss: 0.0811\n",
            "Epoch 141/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0787\n",
            "Epoch 00141: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0787 - val_loss: 0.0801\n",
            "Epoch 142/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0775\n",
            "Epoch 00142: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0775 - val_loss: 0.0852\n",
            "Epoch 143/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0788\n",
            "Epoch 00143: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0788 - val_loss: 0.0824\n",
            "Epoch 144/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0784\n",
            "Epoch 00144: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0784 - val_loss: 0.0796\n",
            "Epoch 145/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0777\n",
            "Epoch 00145: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0777 - val_loss: 0.0798\n",
            "Epoch 146/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0773\n",
            "Epoch 00146: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0773 - val_loss: 0.0789\n",
            "Epoch 147/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0774\n",
            "Epoch 00147: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0774 - val_loss: 0.0811\n",
            "Epoch 148/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0776\n",
            "Epoch 00148: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0776 - val_loss: 0.0817\n",
            "Epoch 149/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0775\n",
            "Epoch 00149: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0775 - val_loss: 0.0796\n",
            "Epoch 150/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0772\n",
            "Epoch 00150: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0772 - val_loss: 0.0804\n",
            "Epoch 151/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0774\n",
            "Epoch 00151: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0774 - val_loss: 0.0798\n",
            "Epoch 152/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0769\n",
            "Epoch 00152: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0769 - val_loss: 0.0805\n",
            "Epoch 153/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0769\n",
            "Epoch 00153: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0769 - val_loss: 0.0815\n",
            "Epoch 154/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0764\n",
            "Epoch 00154: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0764 - val_loss: 0.0810\n",
            "Epoch 155/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0766\n",
            "Epoch 00155: val_loss did not improve from 0.07866\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0766 - val_loss: 0.0795\n",
            "Epoch 156/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0764\n",
            "Epoch 00156: val_loss improved from 0.07866 to 0.07739, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0764 - val_loss: 0.0774\n",
            "Epoch 157/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0763\n",
            "Epoch 00157: val_loss did not improve from 0.07739\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0763 - val_loss: 0.0796\n",
            "Epoch 158/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0763\n",
            "Epoch 00158: val_loss did not improve from 0.07739\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0763 - val_loss: 0.0785\n",
            "Epoch 159/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0762\n",
            "Epoch 00159: val_loss did not improve from 0.07739\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0762 - val_loss: 0.0825\n",
            "Epoch 160/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0752\n",
            "Epoch 00160: val_loss did not improve from 0.07739\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0752 - val_loss: 0.0779\n",
            "Epoch 161/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0760\n",
            "Epoch 00161: val_loss did not improve from 0.07739\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0760 - val_loss: 0.0774\n",
            "Epoch 162/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0755\n",
            "Epoch 00162: val_loss did not improve from 0.07739\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0755 - val_loss: 0.0793\n",
            "Epoch 163/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0750\n",
            "Epoch 00163: val_loss did not improve from 0.07739\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0750 - val_loss: 0.0799\n",
            "Epoch 164/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0760\n",
            "Epoch 00164: val_loss did not improve from 0.07739\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0760 - val_loss: 0.0788\n",
            "Epoch 165/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0756\n",
            "Epoch 00165: val_loss improved from 0.07739 to 0.07697, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0756 - val_loss: 0.0770\n",
            "Epoch 166/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0750\n",
            "Epoch 00166: val_loss did not improve from 0.07697\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0750 - val_loss: 0.0780\n",
            "Epoch 167/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0747\n",
            "Epoch 00167: val_loss did not improve from 0.07697\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0746 - val_loss: 0.0808\n",
            "Epoch 168/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0753\n",
            "Epoch 00168: val_loss did not improve from 0.07697\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0753 - val_loss: 0.0791\n",
            "Epoch 169/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0747\n",
            "Epoch 00169: val_loss did not improve from 0.07697\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0746 - val_loss: 0.0784\n",
            "Epoch 170/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0750\n",
            "Epoch 00170: val_loss improved from 0.07697 to 0.07602, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0750 - val_loss: 0.0760\n",
            "Epoch 171/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0743\n",
            "Epoch 00171: val_loss did not improve from 0.07602\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0743 - val_loss: 0.0781\n",
            "Epoch 172/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0745\n",
            "Epoch 00172: val_loss did not improve from 0.07602\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0744 - val_loss: 0.0761\n",
            "Epoch 173/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0745\n",
            "Epoch 00173: val_loss did not improve from 0.07602\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0745 - val_loss: 0.0763\n",
            "Epoch 174/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0744\n",
            "Epoch 00174: val_loss did not improve from 0.07602\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0745 - val_loss: 0.0765\n",
            "Epoch 175/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0744\n",
            "Epoch 00175: val_loss improved from 0.07602 to 0.07563, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0744 - val_loss: 0.0756\n",
            "Epoch 176/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0739\n",
            "Epoch 00176: val_loss did not improve from 0.07563\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0739 - val_loss: 0.0819\n",
            "Epoch 177/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0738\n",
            "Epoch 00177: val_loss improved from 0.07563 to 0.07506, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0738 - val_loss: 0.0751\n",
            "Epoch 178/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0739\n",
            "Epoch 00178: val_loss did not improve from 0.07506\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0739 - val_loss: 0.0759\n",
            "Epoch 179/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0736\n",
            "Epoch 00179: val_loss did not improve from 0.07506\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0736 - val_loss: 0.0788\n",
            "Epoch 180/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0725\n",
            "Epoch 00180: val_loss did not improve from 0.07506\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0725 - val_loss: 0.0788\n",
            "Epoch 181/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0742\n",
            "Epoch 00181: val_loss did not improve from 0.07506\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0742 - val_loss: 0.0772\n",
            "Epoch 182/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0736\n",
            "Epoch 00182: val_loss did not improve from 0.07506\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0737 - val_loss: 0.0753\n",
            "Epoch 183/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0733\n",
            "Epoch 00183: val_loss did not improve from 0.07506\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0733 - val_loss: 0.0785\n",
            "Epoch 184/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0738\n",
            "Epoch 00184: val_loss did not improve from 0.07506\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0738 - val_loss: 0.0771\n",
            "Epoch 185/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0734\n",
            "Epoch 00185: val_loss improved from 0.07506 to 0.07461, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0733 - val_loss: 0.0746\n",
            "Epoch 186/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0728\n",
            "Epoch 00186: val_loss did not improve from 0.07461\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0729 - val_loss: 0.0780\n",
            "Epoch 187/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0730\n",
            "Epoch 00187: val_loss did not improve from 0.07461\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0730 - val_loss: 0.0761\n",
            "Epoch 188/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0729\n",
            "Epoch 00188: val_loss did not improve from 0.07461\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0728 - val_loss: 0.0747\n",
            "Epoch 189/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0728\n",
            "Epoch 00189: val_loss did not improve from 0.07461\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0728 - val_loss: 0.0771\n",
            "Epoch 190/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0727\n",
            "Epoch 00190: val_loss did not improve from 0.07461\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0727 - val_loss: 0.0766\n",
            "Epoch 191/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0729\n",
            "Epoch 00191: val_loss did not improve from 0.07461\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0729 - val_loss: 0.0776\n",
            "Epoch 192/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0728\n",
            "Epoch 00192: val_loss did not improve from 0.07461\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0728 - val_loss: 0.0748\n",
            "Epoch 193/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0720\n",
            "Epoch 00193: val_loss improved from 0.07461 to 0.07443, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 71ms/step - loss: 0.0720 - val_loss: 0.0744\n",
            "Epoch 194/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0719\n",
            "Epoch 00194: val_loss did not improve from 0.07443\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0720 - val_loss: 0.0770\n",
            "Epoch 195/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0728\n",
            "Epoch 00195: val_loss did not improve from 0.07443\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0728 - val_loss: 0.0768\n",
            "Epoch 196/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0725\n",
            "Epoch 00196: val_loss did not improve from 0.07443\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0725 - val_loss: 0.0758\n",
            "Epoch 197/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0720\n",
            "Epoch 00197: val_loss did not improve from 0.07443\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0720 - val_loss: 0.0759\n",
            "Epoch 198/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0717\n",
            "Epoch 00198: val_loss did not improve from 0.07443\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0717 - val_loss: 0.0761\n",
            "Epoch 199/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0721\n",
            "Epoch 00199: val_loss did not improve from 0.07443\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0721 - val_loss: 0.0751\n",
            "Epoch 200/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0716\n",
            "Epoch 00200: val_loss did not improve from 0.07443\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0716 - val_loss: 0.0763\n",
            "Epoch 201/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0721\n",
            "Epoch 00201: val_loss improved from 0.07443 to 0.07292, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0721 - val_loss: 0.0729\n",
            "Epoch 202/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0714\n",
            "Epoch 00202: val_loss did not improve from 0.07292\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0714 - val_loss: 0.0770\n",
            "Epoch 203/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0717\n",
            "Epoch 00203: val_loss did not improve from 0.07292\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0717 - val_loss: 0.0731\n",
            "Epoch 204/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0713\n",
            "Epoch 00204: val_loss did not improve from 0.07292\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0713 - val_loss: 0.0764\n",
            "Epoch 205/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0714\n",
            "Epoch 00205: val_loss did not improve from 0.07292\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0714 - val_loss: 0.0730\n",
            "Epoch 206/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0714\n",
            "Epoch 00206: val_loss did not improve from 0.07292\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0715 - val_loss: 0.0738\n",
            "Epoch 207/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0705\n",
            "Epoch 00207: val_loss did not improve from 0.07292\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0706 - val_loss: 0.0765\n",
            "Epoch 208/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0715\n",
            "Epoch 00208: val_loss did not improve from 0.07292\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0715 - val_loss: 0.0733\n",
            "Epoch 209/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0700\n",
            "Epoch 00209: val_loss did not improve from 0.07292\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0701 - val_loss: 0.0768\n",
            "Epoch 210/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0715\n",
            "Epoch 00210: val_loss did not improve from 0.07292\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0715 - val_loss: 0.0751\n",
            "Epoch 211/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0713\n",
            "Epoch 00211: val_loss did not improve from 0.07292\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0713 - val_loss: 0.0739\n",
            "Epoch 212/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0708\n",
            "Epoch 00212: val_loss improved from 0.07292 to 0.07277, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0708 - val_loss: 0.0728\n",
            "Epoch 213/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0709\n",
            "Epoch 00213: val_loss did not improve from 0.07277\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0708 - val_loss: 0.0760\n",
            "Epoch 214/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0710\n",
            "Epoch 00214: val_loss improved from 0.07277 to 0.07251, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0710 - val_loss: 0.0725\n",
            "Epoch 215/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0704\n",
            "Epoch 00215: val_loss did not improve from 0.07251\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0704 - val_loss: 0.0736\n",
            "Epoch 216/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0703\n",
            "Epoch 00216: val_loss did not improve from 0.07251\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0703 - val_loss: 0.0741\n",
            "Epoch 217/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0704\n",
            "Epoch 00217: val_loss did not improve from 0.07251\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0704 - val_loss: 0.0734\n",
            "Epoch 218/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0702\n",
            "Epoch 00218: val_loss did not improve from 0.07251\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0703 - val_loss: 0.0731\n",
            "Epoch 219/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0706\n",
            "Epoch 00219: val_loss did not improve from 0.07251\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0707 - val_loss: 0.0743\n",
            "Epoch 220/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0702\n",
            "Epoch 00220: val_loss did not improve from 0.07251\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0702 - val_loss: 0.0742\n",
            "Epoch 221/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0704\n",
            "Epoch 00221: val_loss did not improve from 0.07251\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0704 - val_loss: 0.0728\n",
            "Epoch 222/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0701\n",
            "Epoch 00222: val_loss did not improve from 0.07251\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0702 - val_loss: 0.0727\n",
            "Epoch 223/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0694\n",
            "Epoch 00223: val_loss improved from 0.07251 to 0.07176, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0693 - val_loss: 0.0718\n",
            "Epoch 224/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0701\n",
            "Epoch 00224: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0701 - val_loss: 0.0730\n",
            "Epoch 225/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0700\n",
            "Epoch 00225: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0700 - val_loss: 0.0751\n",
            "Epoch 226/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0700\n",
            "Epoch 00226: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0700 - val_loss: 0.0737\n",
            "Epoch 227/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0701\n",
            "Epoch 00227: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0701 - val_loss: 0.0737\n",
            "Epoch 228/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0695\n",
            "Epoch 00228: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0695 - val_loss: 0.0727\n",
            "Epoch 229/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0700\n",
            "Epoch 00229: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0700 - val_loss: 0.0748\n",
            "Epoch 230/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0694\n",
            "Epoch 00230: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0694 - val_loss: 0.0736\n",
            "Epoch 231/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0693\n",
            "Epoch 00231: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0693 - val_loss: 0.0743\n",
            "Epoch 232/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0696\n",
            "Epoch 00232: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0696 - val_loss: 0.0735\n",
            "Epoch 233/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0691\n",
            "Epoch 00233: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0691 - val_loss: 0.0739\n",
            "Epoch 234/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0697\n",
            "Epoch 00234: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0697 - val_loss: 0.0737\n",
            "Epoch 235/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0693\n",
            "Epoch 00235: val_loss did not improve from 0.07176\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0693 - val_loss: 0.0739\n",
            "Epoch 236/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0690\n",
            "Epoch 00236: val_loss improved from 0.07176 to 0.07088, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0690 - val_loss: 0.0709\n",
            "Epoch 237/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0687\n",
            "Epoch 00237: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0687 - val_loss: 0.0727\n",
            "Epoch 238/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0691\n",
            "Epoch 00238: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0691 - val_loss: 0.0716\n",
            "Epoch 239/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0686\n",
            "Epoch 00239: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0687 - val_loss: 0.0720\n",
            "Epoch 240/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0688\n",
            "Epoch 00240: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0688 - val_loss: 0.0720\n",
            "Epoch 241/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0684\n",
            "Epoch 00241: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0684 - val_loss: 0.0722\n",
            "Epoch 242/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0687\n",
            "Epoch 00242: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0687 - val_loss: 0.0732\n",
            "Epoch 243/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0682\n",
            "Epoch 00243: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0683 - val_loss: 0.0787\n",
            "Epoch 244/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0692\n",
            "Epoch 00244: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0692 - val_loss: 0.0731\n",
            "Epoch 245/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0688\n",
            "Epoch 00245: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0688 - val_loss: 0.0719\n",
            "Epoch 246/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0684\n",
            "Epoch 00246: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0684 - val_loss: 0.0713\n",
            "Epoch 247/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0680\n",
            "Epoch 00247: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0680 - val_loss: 0.0734\n",
            "Epoch 248/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0687\n",
            "Epoch 00248: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0687 - val_loss: 0.0735\n",
            "Epoch 249/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0682\n",
            "Epoch 00249: val_loss did not improve from 0.07088\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0682 - val_loss: 0.0732\n",
            "Epoch 250/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0680\n",
            "Epoch 00250: val_loss improved from 0.07088 to 0.07026, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0681 - val_loss: 0.0703\n",
            "Epoch 251/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0678\n",
            "Epoch 00251: val_loss did not improve from 0.07026\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0678 - val_loss: 0.0717\n",
            "Epoch 252/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0681\n",
            "Epoch 00252: val_loss did not improve from 0.07026\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0681 - val_loss: 0.0710\n",
            "Epoch 253/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0683\n",
            "Epoch 00253: val_loss did not improve from 0.07026\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0683 - val_loss: 0.0704\n",
            "Epoch 254/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0675\n",
            "Epoch 00254: val_loss did not improve from 0.07026\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0675 - val_loss: 0.0710\n",
            "Epoch 255/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0678\n",
            "Epoch 00255: val_loss did not improve from 0.07026\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0678 - val_loss: 0.0720\n",
            "Epoch 256/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0683\n",
            "Epoch 00256: val_loss did not improve from 0.07026\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0683 - val_loss: 0.0737\n",
            "Epoch 257/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0677\n",
            "Epoch 00257: val_loss did not improve from 0.07026\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0677 - val_loss: 0.0724\n",
            "Epoch 258/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0677\n",
            "Epoch 00258: val_loss improved from 0.07026 to 0.07003, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0677 - val_loss: 0.0700\n",
            "Epoch 259/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0670\n",
            "Epoch 00259: val_loss did not improve from 0.07003\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0670 - val_loss: 0.0709\n",
            "Epoch 260/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0679\n",
            "Epoch 00260: val_loss did not improve from 0.07003\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0678 - val_loss: 0.0705\n",
            "Epoch 261/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0672\n",
            "Epoch 00261: val_loss did not improve from 0.07003\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0672 - val_loss: 0.0721\n",
            "Epoch 262/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0677\n",
            "Epoch 00262: val_loss did not improve from 0.07003\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0677 - val_loss: 0.0706\n",
            "Epoch 263/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0669\n",
            "Epoch 00263: val_loss did not improve from 0.07003\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0669 - val_loss: 0.0714\n",
            "Epoch 264/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0672\n",
            "Epoch 00264: val_loss did not improve from 0.07003\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0673 - val_loss: 0.0726\n",
            "Epoch 265/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0673\n",
            "Epoch 00265: val_loss did not improve from 0.07003\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0673 - val_loss: 0.0712\n",
            "Epoch 266/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0669\n",
            "Epoch 00266: val_loss did not improve from 0.07003\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0669 - val_loss: 0.0725\n",
            "Epoch 267/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0668\n",
            "Epoch 00267: val_loss did not improve from 0.07003\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0668 - val_loss: 0.0703\n",
            "Epoch 268/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0668\n",
            "Epoch 00268: val_loss improved from 0.07003 to 0.06990, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0669 - val_loss: 0.0699\n",
            "Epoch 269/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0670\n",
            "Epoch 00269: val_loss improved from 0.06990 to 0.06939, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 3s 87ms/step - loss: 0.0669 - val_loss: 0.0694\n",
            "Epoch 270/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0666\n",
            "Epoch 00270: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0666 - val_loss: 0.0715\n",
            "Epoch 271/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0668\n",
            "Epoch 00271: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0668 - val_loss: 0.0705\n",
            "Epoch 272/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0668\n",
            "Epoch 00272: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0668 - val_loss: 0.0695\n",
            "Epoch 273/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0666\n",
            "Epoch 00273: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0666 - val_loss: 0.0714\n",
            "Epoch 274/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0665\n",
            "Epoch 00274: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0665 - val_loss: 0.0717\n",
            "Epoch 275/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0667\n",
            "Epoch 00275: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0667 - val_loss: 0.0708\n",
            "Epoch 276/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0664\n",
            "Epoch 00276: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0664 - val_loss: 0.0709\n",
            "Epoch 277/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0667\n",
            "Epoch 00277: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0667 - val_loss: 0.0717\n",
            "Epoch 278/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0664\n",
            "Epoch 00278: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0664 - val_loss: 0.0697\n",
            "Epoch 279/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0668\n",
            "Epoch 00279: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0668 - val_loss: 0.0704\n",
            "Epoch 280/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0658\n",
            "Epoch 00280: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0658 - val_loss: 0.0718\n",
            "Epoch 281/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0657\n",
            "Epoch 00281: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0658 - val_loss: 0.0699\n",
            "Epoch 282/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0661\n",
            "Epoch 00282: val_loss did not improve from 0.06939\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0661 - val_loss: 0.0702\n",
            "Epoch 283/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0650\n",
            "Epoch 00283: val_loss improved from 0.06939 to 0.06930, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0650 - val_loss: 0.0693\n",
            "Epoch 284/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0663\n",
            "Epoch 00284: val_loss did not improve from 0.06930\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0663 - val_loss: 0.0714\n",
            "Epoch 285/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0658\n",
            "Epoch 00285: val_loss did not improve from 0.06930\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0659 - val_loss: 0.0695\n",
            "Epoch 286/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0661\n",
            "Epoch 00286: val_loss did not improve from 0.06930\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0661 - val_loss: 0.0712\n",
            "Epoch 287/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0657\n",
            "Epoch 00287: val_loss did not improve from 0.06930\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0656 - val_loss: 0.0698\n",
            "Epoch 288/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0651\n",
            "Epoch 00288: val_loss improved from 0.06930 to 0.06910, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0652 - val_loss: 0.0691\n",
            "Epoch 289/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0656\n",
            "Epoch 00289: val_loss did not improve from 0.06910\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0656 - val_loss: 0.0691\n",
            "Epoch 290/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0657\n",
            "Epoch 00290: val_loss did not improve from 0.06910\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0658 - val_loss: 0.0700\n",
            "Epoch 291/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0660\n",
            "Epoch 00291: val_loss did not improve from 0.06910\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0661 - val_loss: 0.0704\n",
            "Epoch 292/1000\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.0658\n",
            "Epoch 00292: val_loss did not improve from 0.06910\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0658 - val_loss: 0.0742\n",
            "Epoch 293/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0657\n",
            "Epoch 00293: val_loss did not improve from 0.06910\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0657 - val_loss: 0.0703\n",
            "Epoch 294/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0649\n",
            "Epoch 00294: val_loss did not improve from 0.06910\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0650 - val_loss: 0.0714\n",
            "Epoch 295/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0652\n",
            "Epoch 00295: val_loss did not improve from 0.06910\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0652 - val_loss: 0.0737\n",
            "Epoch 296/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0647\n",
            "Epoch 00296: val_loss improved from 0.06910 to 0.06875, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0647 - val_loss: 0.0688\n",
            "Epoch 297/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0652\n",
            "Epoch 00297: val_loss did not improve from 0.06875\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0652 - val_loss: 0.0729\n",
            "Epoch 298/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0656\n",
            "Epoch 00298: val_loss did not improve from 0.06875\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0656 - val_loss: 0.0690\n",
            "Epoch 299/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0646\n",
            "Epoch 00299: val_loss did not improve from 0.06875\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0646 - val_loss: 0.0697\n",
            "Epoch 300/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0642\n",
            "Epoch 00300: val_loss did not improve from 0.06875\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0642 - val_loss: 0.0717\n",
            "Epoch 301/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0653\n",
            "Epoch 00301: val_loss did not improve from 0.06875\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0653 - val_loss: 0.0727\n",
            "Epoch 302/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0649\n",
            "Epoch 00302: val_loss did not improve from 0.06875\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0650 - val_loss: 0.0697\n",
            "Epoch 303/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0646\n",
            "Epoch 00303: val_loss did not improve from 0.06875\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0646 - val_loss: 0.0721\n",
            "Epoch 304/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0644\n",
            "Epoch 00304: val_loss did not improve from 0.06875\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0644 - val_loss: 0.0692\n",
            "Epoch 305/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0641\n",
            "Epoch 00305: val_loss did not improve from 0.06875\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0641 - val_loss: 0.0693\n",
            "Epoch 306/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0649\n",
            "Epoch 00306: val_loss did not improve from 0.06875\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0650 - val_loss: 0.0703\n",
            "Epoch 307/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0648\n",
            "Epoch 00307: val_loss did not improve from 0.06875\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0648 - val_loss: 0.0688\n",
            "Epoch 308/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0647\n",
            "Epoch 00308: val_loss improved from 0.06875 to 0.06708, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 71ms/step - loss: 0.0646 - val_loss: 0.0671\n",
            "Epoch 309/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0631\n",
            "Epoch 00309: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0631 - val_loss: 0.0673\n",
            "Epoch 310/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0643\n",
            "Epoch 00310: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0643 - val_loss: 0.0694\n",
            "Epoch 311/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0646\n",
            "Epoch 00311: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0646 - val_loss: 0.0696\n",
            "Epoch 312/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0646\n",
            "Epoch 00312: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0646 - val_loss: 0.0676\n",
            "Epoch 313/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0644\n",
            "Epoch 00313: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0644 - val_loss: 0.0680\n",
            "Epoch 314/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0635\n",
            "Epoch 00314: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0635 - val_loss: 0.0696\n",
            "Epoch 315/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0643\n",
            "Epoch 00315: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0643 - val_loss: 0.0692\n",
            "Epoch 316/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0636\n",
            "Epoch 00316: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0636 - val_loss: 0.0694\n",
            "Epoch 317/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0640\n",
            "Epoch 00317: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0640 - val_loss: 0.0701\n",
            "Epoch 318/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0635\n",
            "Epoch 00318: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0636 - val_loss: 0.0679\n",
            "Epoch 319/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0640\n",
            "Epoch 00319: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0640 - val_loss: 0.0690\n",
            "Epoch 320/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0630\n",
            "Epoch 00320: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0630 - val_loss: 0.0687\n",
            "Epoch 321/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0639\n",
            "Epoch 00321: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0638 - val_loss: 0.0707\n",
            "Epoch 322/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0638\n",
            "Epoch 00322: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0638 - val_loss: 0.0682\n",
            "Epoch 323/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0633\n",
            "Epoch 00323: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0633 - val_loss: 0.0686\n",
            "Epoch 324/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0637\n",
            "Epoch 00324: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0637 - val_loss: 0.0682\n",
            "Epoch 325/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0630\n",
            "Epoch 00325: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0630 - val_loss: 0.0724\n",
            "Epoch 326/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0633\n",
            "Epoch 00326: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0633 - val_loss: 0.0681\n",
            "Epoch 327/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0633\n",
            "Epoch 00327: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0633 - val_loss: 0.0702\n",
            "Epoch 328/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0624\n",
            "Epoch 00328: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0624 - val_loss: 0.0674\n",
            "Epoch 329/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0634\n",
            "Epoch 00329: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0633 - val_loss: 0.0703\n",
            "Epoch 330/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0627\n",
            "Epoch 00330: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0627 - val_loss: 0.0685\n",
            "Epoch 331/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0638\n",
            "Epoch 00331: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0638 - val_loss: 0.0671\n",
            "Epoch 332/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0624\n",
            "Epoch 00332: val_loss did not improve from 0.06708\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0624 - val_loss: 0.0677\n",
            "Epoch 333/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0630\n",
            "Epoch 00333: val_loss improved from 0.06708 to 0.06687, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0630 - val_loss: 0.0669\n",
            "Epoch 334/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0638\n",
            "Epoch 00334: val_loss improved from 0.06687 to 0.06672, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 75ms/step - loss: 0.0638 - val_loss: 0.0667\n",
            "Epoch 335/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0618\n",
            "Epoch 00335: val_loss did not improve from 0.06672\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0618 - val_loss: 0.0670\n",
            "Epoch 336/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0631\n",
            "Epoch 00336: val_loss did not improve from 0.06672\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0631 - val_loss: 0.0672\n",
            "Epoch 337/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0626\n",
            "Epoch 00337: val_loss did not improve from 0.06672\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0626 - val_loss: 0.0716\n",
            "Epoch 338/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0633\n",
            "Epoch 00338: val_loss did not improve from 0.06672\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0633 - val_loss: 0.0674\n",
            "Epoch 339/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0626\n",
            "Epoch 00339: val_loss did not improve from 0.06672\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0626 - val_loss: 0.0677\n",
            "Epoch 340/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0628\n",
            "Epoch 00340: val_loss did not improve from 0.06672\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0628 - val_loss: 0.0672\n",
            "Epoch 341/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0622\n",
            "Epoch 00341: val_loss did not improve from 0.06672\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0622 - val_loss: 0.0685\n",
            "Epoch 342/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0629\n",
            "Epoch 00342: val_loss did not improve from 0.06672\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0629 - val_loss: 0.0670\n",
            "Epoch 343/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0623\n",
            "Epoch 00343: val_loss improved from 0.06672 to 0.06634, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0623 - val_loss: 0.0663\n",
            "Epoch 344/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0617\n",
            "Epoch 00344: val_loss did not improve from 0.06634\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0617 - val_loss: 0.0680\n",
            "Epoch 345/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0621\n",
            "Epoch 00345: val_loss improved from 0.06634 to 0.06617, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.0621 - val_loss: 0.0662\n",
            "Epoch 346/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0622\n",
            "Epoch 00346: val_loss did not improve from 0.06617\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0622 - val_loss: 0.0687\n",
            "Epoch 347/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0623\n",
            "Epoch 00347: val_loss did not improve from 0.06617\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0623 - val_loss: 0.0679\n",
            "Epoch 348/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0624\n",
            "Epoch 00348: val_loss did not improve from 0.06617\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0624 - val_loss: 0.0689\n",
            "Epoch 349/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0619\n",
            "Epoch 00349: val_loss did not improve from 0.06617\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0619 - val_loss: 0.0667\n",
            "Epoch 350/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0618\n",
            "Epoch 00350: val_loss did not improve from 0.06617\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0617 - val_loss: 0.0671\n",
            "Epoch 351/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0621\n",
            "Epoch 00351: val_loss did not improve from 0.06617\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0621 - val_loss: 0.0680\n",
            "Epoch 352/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0624\n",
            "Epoch 00352: val_loss did not improve from 0.06617\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0624 - val_loss: 0.0666\n",
            "Epoch 353/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0615\n",
            "Epoch 00353: val_loss did not improve from 0.06617\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0615 - val_loss: 0.0679\n",
            "Epoch 354/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0620\n",
            "Epoch 00354: val_loss improved from 0.06617 to 0.06614, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0620 - val_loss: 0.0661\n",
            "Epoch 355/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0615\n",
            "Epoch 00355: val_loss did not improve from 0.06614\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0615 - val_loss: 0.0679\n",
            "Epoch 356/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0618\n",
            "Epoch 00356: val_loss did not improve from 0.06614\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0618 - val_loss: 0.0682\n",
            "Epoch 357/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0615\n",
            "Epoch 00357: val_loss did not improve from 0.06614\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0615 - val_loss: 0.0666\n",
            "Epoch 358/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0614\n",
            "Epoch 00358: val_loss did not improve from 0.06614\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0614 - val_loss: 0.0670\n",
            "Epoch 359/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0619\n",
            "Epoch 00359: val_loss did not improve from 0.06614\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0620 - val_loss: 0.0674\n",
            "Epoch 360/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0614\n",
            "Epoch 00360: val_loss improved from 0.06614 to 0.06593, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0614 - val_loss: 0.0659\n",
            "Epoch 361/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0604\n",
            "Epoch 00361: val_loss improved from 0.06593 to 0.06562, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0604 - val_loss: 0.0656\n",
            "Epoch 362/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0614\n",
            "Epoch 00362: val_loss did not improve from 0.06562\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0614 - val_loss: 0.0689\n",
            "Epoch 363/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0618\n",
            "Epoch 00363: val_loss did not improve from 0.06562\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0617 - val_loss: 0.0659\n",
            "Epoch 364/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0613\n",
            "Epoch 00364: val_loss did not improve from 0.06562\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0614 - val_loss: 0.0682\n",
            "Epoch 365/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0609\n",
            "Epoch 00365: val_loss did not improve from 0.06562\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0609 - val_loss: 0.0693\n",
            "Epoch 366/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0621\n",
            "Epoch 00366: val_loss did not improve from 0.06562\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0622 - val_loss: 0.0663\n",
            "Epoch 367/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0602\n",
            "Epoch 00367: val_loss did not improve from 0.06562\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0602 - val_loss: 0.0679\n",
            "Epoch 368/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0619\n",
            "Epoch 00368: val_loss did not improve from 0.06562\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0619 - val_loss: 0.0669\n",
            "Epoch 369/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0611\n",
            "Epoch 00369: val_loss did not improve from 0.06562\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0612 - val_loss: 0.0669\n",
            "Epoch 370/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0610\n",
            "Epoch 00370: val_loss did not improve from 0.06562\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0610 - val_loss: 0.0686\n",
            "Epoch 371/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0618\n",
            "Epoch 00371: val_loss did not improve from 0.06562\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0619 - val_loss: 0.0664\n",
            "Epoch 372/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0610\n",
            "Epoch 00372: val_loss improved from 0.06562 to 0.06536, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0610 - val_loss: 0.0654\n",
            "Epoch 373/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0608\n",
            "Epoch 00373: val_loss did not improve from 0.06536\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0607 - val_loss: 0.0660\n",
            "Epoch 374/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0606\n",
            "Epoch 00374: val_loss improved from 0.06536 to 0.06528, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0606 - val_loss: 0.0653\n",
            "Epoch 375/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0607\n",
            "Epoch 00375: val_loss did not improve from 0.06528\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0607 - val_loss: 0.0668\n",
            "Epoch 376/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0611\n",
            "Epoch 00376: val_loss improved from 0.06528 to 0.06524, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0611 - val_loss: 0.0652\n",
            "Epoch 377/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0605\n",
            "Epoch 00377: val_loss did not improve from 0.06524\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0605 - val_loss: 0.0663\n",
            "Epoch 378/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0609\n",
            "Epoch 00378: val_loss did not improve from 0.06524\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0609 - val_loss: 0.0658\n",
            "Epoch 379/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0608\n",
            "Epoch 00379: val_loss improved from 0.06524 to 0.06495, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0608 - val_loss: 0.0650\n",
            "Epoch 380/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0605\n",
            "Epoch 00380: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0605 - val_loss: 0.0657\n",
            "Epoch 381/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0603\n",
            "Epoch 00381: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0603 - val_loss: 0.0662\n",
            "Epoch 382/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0609\n",
            "Epoch 00382: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0609 - val_loss: 0.0655\n",
            "Epoch 383/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0606\n",
            "Epoch 00383: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0606 - val_loss: 0.0688\n",
            "Epoch 384/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0606\n",
            "Epoch 00384: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0606 - val_loss: 0.0658\n",
            "Epoch 385/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0598\n",
            "Epoch 00385: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0599 - val_loss: 0.0658\n",
            "Epoch 386/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0601\n",
            "Epoch 00386: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0601 - val_loss: 0.0669\n",
            "Epoch 387/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0606\n",
            "Epoch 00387: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0606 - val_loss: 0.0658\n",
            "Epoch 388/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0601\n",
            "Epoch 00388: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0601 - val_loss: 0.0676\n",
            "Epoch 389/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0608\n",
            "Epoch 00389: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0608 - val_loss: 0.0660\n",
            "Epoch 390/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0602\n",
            "Epoch 00390: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0602 - val_loss: 0.0655\n",
            "Epoch 391/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0601\n",
            "Epoch 00391: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0600 - val_loss: 0.0678\n",
            "Epoch 392/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0604\n",
            "Epoch 00392: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0604 - val_loss: 0.0661\n",
            "Epoch 393/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0597\n",
            "Epoch 00393: val_loss did not improve from 0.06495\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0597 - val_loss: 0.0660\n",
            "Epoch 394/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0603\n",
            "Epoch 00394: val_loss improved from 0.06495 to 0.06485, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0603 - val_loss: 0.0649\n",
            "Epoch 395/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0592\n",
            "Epoch 00395: val_loss did not improve from 0.06485\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0592 - val_loss: 0.0654\n",
            "Epoch 396/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0596\n",
            "Epoch 00396: val_loss did not improve from 0.06485\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0596 - val_loss: 0.0673\n",
            "Epoch 397/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0597\n",
            "Epoch 00397: val_loss did not improve from 0.06485\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0597 - val_loss: 0.0677\n",
            "Epoch 398/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0602\n",
            "Epoch 00398: val_loss did not improve from 0.06485\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0602 - val_loss: 0.0655\n",
            "Epoch 399/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0603\n",
            "Epoch 00399: val_loss did not improve from 0.06485\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0603 - val_loss: 0.0675\n",
            "Epoch 400/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0596\n",
            "Epoch 00400: val_loss did not improve from 0.06485\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0596 - val_loss: 0.0672\n",
            "Epoch 401/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0602\n",
            "Epoch 00401: val_loss did not improve from 0.06485\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0601 - val_loss: 0.0658\n",
            "Epoch 402/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0597\n",
            "Epoch 00402: val_loss did not improve from 0.06485\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0597 - val_loss: 0.0651\n",
            "Epoch 403/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0595\n",
            "Epoch 00403: val_loss did not improve from 0.06485\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0595 - val_loss: 0.0658\n",
            "Epoch 404/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0596\n",
            "Epoch 00404: val_loss improved from 0.06485 to 0.06471, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0596 - val_loss: 0.0647\n",
            "Epoch 405/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0593\n",
            "Epoch 00405: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0593 - val_loss: 0.0656\n",
            "Epoch 406/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0593\n",
            "Epoch 00406: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0594 - val_loss: 0.0649\n",
            "Epoch 407/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0596\n",
            "Epoch 00407: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0595 - val_loss: 0.0657\n",
            "Epoch 408/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0592\n",
            "Epoch 00408: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0592 - val_loss: 0.0669\n",
            "Epoch 409/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0602\n",
            "Epoch 00409: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0602 - val_loss: 0.0669\n",
            "Epoch 410/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0595\n",
            "Epoch 00410: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0594 - val_loss: 0.0653\n",
            "Epoch 411/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0595\n",
            "Epoch 00411: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0595 - val_loss: 0.0647\n",
            "Epoch 412/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0592\n",
            "Epoch 00412: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0592 - val_loss: 0.0662\n",
            "Epoch 413/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0595\n",
            "Epoch 00413: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0595 - val_loss: 0.0650\n",
            "Epoch 414/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0589\n",
            "Epoch 00414: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0589 - val_loss: 0.0649\n",
            "Epoch 415/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0587\n",
            "Epoch 00415: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0587 - val_loss: 0.0649\n",
            "Epoch 416/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0590\n",
            "Epoch 00416: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0590 - val_loss: 0.0660\n",
            "Epoch 417/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0588\n",
            "Epoch 00417: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0588 - val_loss: 0.0663\n",
            "Epoch 418/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0596\n",
            "Epoch 00418: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0596 - val_loss: 0.0682\n",
            "Epoch 419/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0588\n",
            "Epoch 00419: val_loss did not improve from 0.06471\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0587 - val_loss: 0.0652\n",
            "Epoch 420/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0592\n",
            "Epoch 00420: val_loss improved from 0.06471 to 0.06388, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0591 - val_loss: 0.0639\n",
            "Epoch 421/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0591\n",
            "Epoch 00421: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0591 - val_loss: 0.0681\n",
            "Epoch 422/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0595\n",
            "Epoch 00422: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0595 - val_loss: 0.0664\n",
            "Epoch 423/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0586\n",
            "Epoch 00423: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0586 - val_loss: 0.0655\n",
            "Epoch 424/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0590\n",
            "Epoch 00424: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0590 - val_loss: 0.0644\n",
            "Epoch 425/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0580\n",
            "Epoch 00425: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0581 - val_loss: 0.0654\n",
            "Epoch 426/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0585\n",
            "Epoch 00426: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0585 - val_loss: 0.0665\n",
            "Epoch 427/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0586\n",
            "Epoch 00427: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0586 - val_loss: 0.0644\n",
            "Epoch 428/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0587\n",
            "Epoch 00428: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0587 - val_loss: 0.0685\n",
            "Epoch 429/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0584\n",
            "Epoch 00429: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0584 - val_loss: 0.0668\n",
            "Epoch 430/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0592\n",
            "Epoch 00430: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0592 - val_loss: 0.0659\n",
            "Epoch 431/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0582\n",
            "Epoch 00431: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0582 - val_loss: 0.0646\n",
            "Epoch 432/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0583\n",
            "Epoch 00432: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0583 - val_loss: 0.0658\n",
            "Epoch 433/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0590\n",
            "Epoch 00433: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0590 - val_loss: 0.0654\n",
            "Epoch 434/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0579\n",
            "Epoch 00434: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0579 - val_loss: 0.0642\n",
            "Epoch 435/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0586\n",
            "Epoch 00435: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0585 - val_loss: 0.0663\n",
            "Epoch 436/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0572\n",
            "Epoch 00436: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0572 - val_loss: 0.0650\n",
            "Epoch 437/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0587\n",
            "Epoch 00437: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0587 - val_loss: 0.0661\n",
            "Epoch 438/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0585\n",
            "Epoch 00438: val_loss did not improve from 0.06388\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0585 - val_loss: 0.0670\n",
            "Epoch 439/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0583\n",
            "Epoch 00439: val_loss improved from 0.06388 to 0.06376, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0583 - val_loss: 0.0638\n",
            "Epoch 440/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0581\n",
            "Epoch 00440: val_loss did not improve from 0.06376\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0581 - val_loss: 0.0643\n",
            "Epoch 441/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0583\n",
            "Epoch 00441: val_loss did not improve from 0.06376\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0583 - val_loss: 0.0653\n",
            "Epoch 442/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0582\n",
            "Epoch 00442: val_loss did not improve from 0.06376\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0582 - val_loss: 0.0654\n",
            "Epoch 443/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0582\n",
            "Epoch 00443: val_loss improved from 0.06376 to 0.06347, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0582 - val_loss: 0.0635\n",
            "Epoch 444/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0573\n",
            "Epoch 00444: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0573 - val_loss: 0.0654\n",
            "Epoch 445/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0583\n",
            "Epoch 00445: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0583 - val_loss: 0.0641\n",
            "Epoch 446/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0574\n",
            "Epoch 00446: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0574 - val_loss: 0.0639\n",
            "Epoch 447/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0577\n",
            "Epoch 00447: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0577 - val_loss: 0.0645\n",
            "Epoch 448/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0574\n",
            "Epoch 00448: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0575 - val_loss: 0.0637\n",
            "Epoch 449/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0579\n",
            "Epoch 00449: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0579 - val_loss: 0.0635\n",
            "Epoch 450/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0577\n",
            "Epoch 00450: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0577 - val_loss: 0.0637\n",
            "Epoch 451/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0581\n",
            "Epoch 00451: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0581 - val_loss: 0.0658\n",
            "Epoch 452/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0576\n",
            "Epoch 00452: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0576 - val_loss: 0.0642\n",
            "Epoch 453/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0568\n",
            "Epoch 00453: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0568 - val_loss: 0.0650\n",
            "Epoch 454/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0577\n",
            "Epoch 00454: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0577 - val_loss: 0.0656\n",
            "Epoch 455/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0578\n",
            "Epoch 00455: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0578 - val_loss: 0.0644\n",
            "Epoch 456/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0571\n",
            "Epoch 00456: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0571 - val_loss: 0.0635\n",
            "Epoch 457/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0578\n",
            "Epoch 00457: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0578 - val_loss: 0.0658\n",
            "Epoch 458/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0579\n",
            "Epoch 00458: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0580 - val_loss: 0.0643\n",
            "Epoch 459/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0574\n",
            "Epoch 00459: val_loss did not improve from 0.06347\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0574 - val_loss: 0.0639\n",
            "Epoch 460/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0573\n",
            "Epoch 00460: val_loss improved from 0.06347 to 0.06313, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0573 - val_loss: 0.0631\n",
            "Epoch 461/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0567\n",
            "Epoch 00461: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0566 - val_loss: 0.0632\n",
            "Epoch 462/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0572\n",
            "Epoch 00462: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0572 - val_loss: 0.0658\n",
            "Epoch 463/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0575\n",
            "Epoch 00463: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0575 - val_loss: 0.0639\n",
            "Epoch 464/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0567\n",
            "Epoch 00464: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0567 - val_loss: 0.0648\n",
            "Epoch 465/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0571\n",
            "Epoch 00465: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0571 - val_loss: 0.0659\n",
            "Epoch 466/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0570\n",
            "Epoch 00466: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0570 - val_loss: 0.0647\n",
            "Epoch 467/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0573\n",
            "Epoch 00467: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0573 - val_loss: 0.0641\n",
            "Epoch 468/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0571\n",
            "Epoch 00468: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0571 - val_loss: 0.0639\n",
            "Epoch 469/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0575\n",
            "Epoch 00469: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0575 - val_loss: 0.0659\n",
            "Epoch 470/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0567\n",
            "Epoch 00470: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0567 - val_loss: 0.0639\n",
            "Epoch 471/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0566\n",
            "Epoch 00471: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0567 - val_loss: 0.0651\n",
            "Epoch 472/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0572\n",
            "Epoch 00472: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0572 - val_loss: 0.0640\n",
            "Epoch 473/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0568\n",
            "Epoch 00473: val_loss did not improve from 0.06313\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0568 - val_loss: 0.0643\n",
            "Epoch 474/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0567\n",
            "Epoch 00474: val_loss improved from 0.06313 to 0.06291, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0567 - val_loss: 0.0629\n",
            "Epoch 475/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0567\n",
            "Epoch 00475: val_loss did not improve from 0.06291\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0567 - val_loss: 0.0657\n",
            "Epoch 476/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0565\n",
            "Epoch 00476: val_loss did not improve from 0.06291\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0565 - val_loss: 0.0632\n",
            "Epoch 477/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0568\n",
            "Epoch 00477: val_loss did not improve from 0.06291\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0567 - val_loss: 0.0635\n",
            "Epoch 478/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0566\n",
            "Epoch 00478: val_loss did not improve from 0.06291\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0566 - val_loss: 0.0647\n",
            "Epoch 479/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0566\n",
            "Epoch 00479: val_loss improved from 0.06291 to 0.06287, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0566 - val_loss: 0.0629\n",
            "Epoch 480/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0557\n",
            "Epoch 00480: val_loss did not improve from 0.06287\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0557 - val_loss: 0.0655\n",
            "Epoch 481/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0571\n",
            "Epoch 00481: val_loss improved from 0.06287 to 0.06269, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 79ms/step - loss: 0.0570 - val_loss: 0.0627\n",
            "Epoch 482/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0562\n",
            "Epoch 00482: val_loss did not improve from 0.06269\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0563 - val_loss: 0.0635\n",
            "Epoch 483/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0560\n",
            "Epoch 00483: val_loss improved from 0.06269 to 0.06233, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0560 - val_loss: 0.0623\n",
            "Epoch 484/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0560\n",
            "Epoch 00484: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0560 - val_loss: 0.0674\n",
            "Epoch 485/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0568\n",
            "Epoch 00485: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0568 - val_loss: 0.0631\n",
            "Epoch 486/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0563\n",
            "Epoch 00486: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0563 - val_loss: 0.0644\n",
            "Epoch 487/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0563\n",
            "Epoch 00487: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0562 - val_loss: 0.0630\n",
            "Epoch 488/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0559\n",
            "Epoch 00488: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0559 - val_loss: 0.0641\n",
            "Epoch 489/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0558\n",
            "Epoch 00489: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0558 - val_loss: 0.0686\n",
            "Epoch 490/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0565\n",
            "Epoch 00490: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0565 - val_loss: 0.0628\n",
            "Epoch 491/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0557\n",
            "Epoch 00491: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0557 - val_loss: 0.0635\n",
            "Epoch 492/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0563\n",
            "Epoch 00492: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0563 - val_loss: 0.0645\n",
            "Epoch 493/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0560\n",
            "Epoch 00493: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0560 - val_loss: 0.0627\n",
            "Epoch 494/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0550\n",
            "Epoch 00494: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0550 - val_loss: 0.0636\n",
            "Epoch 495/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0562\n",
            "Epoch 00495: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0561 - val_loss: 0.0641\n",
            "Epoch 496/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0560\n",
            "Epoch 00496: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0560 - val_loss: 0.0654\n",
            "Epoch 497/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0562\n",
            "Epoch 00497: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0562 - val_loss: 0.0630\n",
            "Epoch 498/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0559\n",
            "Epoch 00498: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0559 - val_loss: 0.0631\n",
            "Epoch 499/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0563\n",
            "Epoch 00499: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0563 - val_loss: 0.0637\n",
            "Epoch 500/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0557\n",
            "Epoch 00500: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0557 - val_loss: 0.0628\n",
            "Epoch 501/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0558\n",
            "Epoch 00501: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0558 - val_loss: 0.0637\n",
            "Epoch 502/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0559\n",
            "Epoch 00502: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0558 - val_loss: 0.0636\n",
            "Epoch 503/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0550\n",
            "Epoch 00503: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0550 - val_loss: 0.0633\n",
            "Epoch 504/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0558\n",
            "Epoch 00504: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0559 - val_loss: 0.0653\n",
            "Epoch 505/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0551\n",
            "Epoch 00505: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0551 - val_loss: 0.0629\n",
            "Epoch 506/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0556\n",
            "Epoch 00506: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0556 - val_loss: 0.0630\n",
            "Epoch 507/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0543\n",
            "Epoch 00507: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0543 - val_loss: 0.0636\n",
            "Epoch 508/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0562\n",
            "Epoch 00508: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0562 - val_loss: 0.0631\n",
            "Epoch 509/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0556\n",
            "Epoch 00509: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0556 - val_loss: 0.0634\n",
            "Epoch 510/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0557\n",
            "Epoch 00510: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0557 - val_loss: 0.0625\n",
            "Epoch 511/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0551\n",
            "Epoch 00511: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0551 - val_loss: 0.0636\n",
            "Epoch 512/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0552\n",
            "Epoch 00512: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0552 - val_loss: 0.0643\n",
            "Epoch 513/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0551\n",
            "Epoch 00513: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0551 - val_loss: 0.0628\n",
            "Epoch 514/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0554\n",
            "Epoch 00514: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0554 - val_loss: 0.0630\n",
            "Epoch 515/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0554\n",
            "Epoch 00515: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0554 - val_loss: 0.0630\n",
            "Epoch 516/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0552\n",
            "Epoch 00516: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0552 - val_loss: 0.0644\n",
            "Epoch 517/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0552\n",
            "Epoch 00517: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0552 - val_loss: 0.0648\n",
            "Epoch 518/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0554\n",
            "Epoch 00518: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0554 - val_loss: 0.0632\n",
            "Epoch 519/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0544\n",
            "Epoch 00519: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0544 - val_loss: 0.0640\n",
            "Epoch 520/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0555\n",
            "Epoch 00520: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0555 - val_loss: 0.0638\n",
            "Epoch 521/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0546\n",
            "Epoch 00521: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0546 - val_loss: 0.0636\n",
            "Epoch 522/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0540\n",
            "Epoch 00522: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0540 - val_loss: 0.0641\n",
            "Epoch 523/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0560\n",
            "Epoch 00523: val_loss did not improve from 0.06233\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0560 - val_loss: 0.0652\n",
            "Epoch 524/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0552\n",
            "Epoch 00524: val_loss improved from 0.06233 to 0.06186, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0552 - val_loss: 0.0619\n",
            "Epoch 525/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0550\n",
            "Epoch 00525: val_loss did not improve from 0.06186\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0550 - val_loss: 0.0625\n",
            "Epoch 526/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0546\n",
            "Epoch 00526: val_loss did not improve from 0.06186\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0547 - val_loss: 0.0632\n",
            "Epoch 527/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0545\n",
            "Epoch 00527: val_loss did not improve from 0.06186\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0545 - val_loss: 0.0633\n",
            "Epoch 528/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0550\n",
            "Epoch 00528: val_loss did not improve from 0.06186\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0550 - val_loss: 0.0654\n",
            "Epoch 529/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0547\n",
            "Epoch 00529: val_loss did not improve from 0.06186\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0547 - val_loss: 0.0646\n",
            "Epoch 530/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0546\n",
            "Epoch 00530: val_loss did not improve from 0.06186\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0546 - val_loss: 0.0625\n",
            "Epoch 531/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0539\n",
            "Epoch 00531: val_loss improved from 0.06186 to 0.06168, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0539 - val_loss: 0.0617\n",
            "Epoch 532/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0543\n",
            "Epoch 00532: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0543 - val_loss: 0.0631\n",
            "Epoch 533/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0547\n",
            "Epoch 00533: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0547 - val_loss: 0.0622\n",
            "Epoch 534/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0549\n",
            "Epoch 00534: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0549 - val_loss: 0.0617\n",
            "Epoch 535/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0545\n",
            "Epoch 00535: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0545 - val_loss: 0.0629\n",
            "Epoch 536/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0544\n",
            "Epoch 00536: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0544 - val_loss: 0.0628\n",
            "Epoch 537/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0546\n",
            "Epoch 00537: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0546 - val_loss: 0.0625\n",
            "Epoch 538/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0535\n",
            "Epoch 00538: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0536 - val_loss: 0.0628\n",
            "Epoch 539/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0549\n",
            "Epoch 00539: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0549 - val_loss: 0.0639\n",
            "Epoch 540/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0545\n",
            "Epoch 00540: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0545 - val_loss: 0.0633\n",
            "Epoch 541/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0547\n",
            "Epoch 00541: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0547 - val_loss: 0.0632\n",
            "Epoch 542/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0538\n",
            "Epoch 00542: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0538 - val_loss: 0.0624\n",
            "Epoch 543/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0543\n",
            "Epoch 00543: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0543 - val_loss: 0.0630\n",
            "Epoch 544/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0541\n",
            "Epoch 00544: val_loss did not improve from 0.06168\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0541 - val_loss: 0.0622\n",
            "Epoch 545/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0547\n",
            "Epoch 00545: val_loss improved from 0.06168 to 0.06148, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0546 - val_loss: 0.0615\n",
            "Epoch 546/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0535\n",
            "Epoch 00546: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0535 - val_loss: 0.0628\n",
            "Epoch 547/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0544\n",
            "Epoch 00547: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0543 - val_loss: 0.0620\n",
            "Epoch 548/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0540\n",
            "Epoch 00548: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0540 - val_loss: 0.0636\n",
            "Epoch 549/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0543\n",
            "Epoch 00549: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0542 - val_loss: 0.0617\n",
            "Epoch 550/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0538\n",
            "Epoch 00550: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0538 - val_loss: 0.0630\n",
            "Epoch 551/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0536\n",
            "Epoch 00551: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0536 - val_loss: 0.0634\n",
            "Epoch 552/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0541\n",
            "Epoch 00552: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0541 - val_loss: 0.0625\n",
            "Epoch 553/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0538\n",
            "Epoch 00553: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0538 - val_loss: 0.0617\n",
            "Epoch 554/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0539\n",
            "Epoch 00554: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0539 - val_loss: 0.0625\n",
            "Epoch 555/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0534\n",
            "Epoch 00555: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0534 - val_loss: 0.0630\n",
            "Epoch 556/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0538\n",
            "Epoch 00556: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0538 - val_loss: 0.0632\n",
            "Epoch 557/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0535\n",
            "Epoch 00557: val_loss did not improve from 0.06148\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0535 - val_loss: 0.0668\n",
            "Epoch 558/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0543\n",
            "Epoch 00558: val_loss improved from 0.06148 to 0.06130, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0543 - val_loss: 0.0613\n",
            "Epoch 559/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0536\n",
            "Epoch 00559: val_loss did not improve from 0.06130\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0536 - val_loss: 0.0617\n",
            "Epoch 560/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0531\n",
            "Epoch 00560: val_loss did not improve from 0.06130\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0531 - val_loss: 0.0617\n",
            "Epoch 561/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0536\n",
            "Epoch 00561: val_loss did not improve from 0.06130\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0536 - val_loss: 0.0627\n",
            "Epoch 562/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0534\n",
            "Epoch 00562: val_loss did not improve from 0.06130\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0535 - val_loss: 0.0628\n",
            "Epoch 563/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0538\n",
            "Epoch 00563: val_loss improved from 0.06130 to 0.06083, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0539 - val_loss: 0.0608\n",
            "Epoch 564/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0535\n",
            "Epoch 00564: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0535 - val_loss: 0.0621\n",
            "Epoch 565/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0526\n",
            "Epoch 00565: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0526 - val_loss: 0.0619\n",
            "Epoch 566/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0538\n",
            "Epoch 00566: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0538 - val_loss: 0.0660\n",
            "Epoch 567/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0537\n",
            "Epoch 00567: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0537 - val_loss: 0.0627\n",
            "Epoch 568/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0529\n",
            "Epoch 00568: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0529 - val_loss: 0.0632\n",
            "Epoch 569/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0539\n",
            "Epoch 00569: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0539 - val_loss: 0.0639\n",
            "Epoch 570/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0537\n",
            "Epoch 00570: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0537 - val_loss: 0.0624\n",
            "Epoch 571/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0531\n",
            "Epoch 00571: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0531 - val_loss: 0.0614\n",
            "Epoch 572/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0524\n",
            "Epoch 00572: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0524 - val_loss: 0.0613\n",
            "Epoch 573/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0537\n",
            "Epoch 00573: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0537 - val_loss: 0.0637\n",
            "Epoch 574/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0536\n",
            "Epoch 00574: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0536 - val_loss: 0.0634\n",
            "Epoch 575/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0532\n",
            "Epoch 00575: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0532 - val_loss: 0.0617\n",
            "Epoch 576/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0532\n",
            "Epoch 00576: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0532 - val_loss: 0.0614\n",
            "Epoch 577/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0529\n",
            "Epoch 00577: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0529 - val_loss: 0.0619\n",
            "Epoch 578/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0529\n",
            "Epoch 00578: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0529 - val_loss: 0.0630\n",
            "Epoch 579/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0536\n",
            "Epoch 00579: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0536 - val_loss: 0.0622\n",
            "Epoch 580/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0530\n",
            "Epoch 00580: val_loss did not improve from 0.06083\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0530 - val_loss: 0.0619\n",
            "Epoch 581/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0521\n",
            "Epoch 00581: val_loss improved from 0.06083 to 0.06073, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0521 - val_loss: 0.0607\n",
            "Epoch 582/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0527\n",
            "Epoch 00582: val_loss did not improve from 0.06073\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0527 - val_loss: 0.0625\n",
            "Epoch 583/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0534\n",
            "Epoch 00583: val_loss did not improve from 0.06073\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0534 - val_loss: 0.0622\n",
            "Epoch 584/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0534\n",
            "Epoch 00584: val_loss did not improve from 0.06073\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0534 - val_loss: 0.0608\n",
            "Epoch 585/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0524\n",
            "Epoch 00585: val_loss did not improve from 0.06073\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0524 - val_loss: 0.0622\n",
            "Epoch 586/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0533\n",
            "Epoch 00586: val_loss did not improve from 0.06073\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0534 - val_loss: 0.0612\n",
            "Epoch 587/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0528\n",
            "Epoch 00587: val_loss did not improve from 0.06073\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0528 - val_loss: 0.0625\n",
            "Epoch 588/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0528\n",
            "Epoch 00588: val_loss did not improve from 0.06073\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0528 - val_loss: 0.0642\n",
            "Epoch 589/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0528\n",
            "Epoch 00589: val_loss did not improve from 0.06073\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0528 - val_loss: 0.0620\n",
            "Epoch 590/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0526\n",
            "Epoch 00590: val_loss did not improve from 0.06073\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0526 - val_loss: 0.0625\n",
            "Epoch 591/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0528\n",
            "Epoch 00591: val_loss did not improve from 0.06073\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0528 - val_loss: 0.0608\n",
            "Epoch 592/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0522\n",
            "Epoch 00592: val_loss did not improve from 0.06073\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0523 - val_loss: 0.0619\n",
            "Epoch 593/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0525\n",
            "Epoch 00593: val_loss improved from 0.06073 to 0.06066, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0525 - val_loss: 0.0607\n",
            "Epoch 594/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0525\n",
            "Epoch 00594: val_loss did not improve from 0.06066\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0525 - val_loss: 0.0616\n",
            "Epoch 595/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0526\n",
            "Epoch 00595: val_loss did not improve from 0.06066\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0526 - val_loss: 0.0626\n",
            "Epoch 596/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0529\n",
            "Epoch 00596: val_loss did not improve from 0.06066\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0529 - val_loss: 0.0616\n",
            "Epoch 597/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0525\n",
            "Epoch 00597: val_loss did not improve from 0.06066\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0525 - val_loss: 0.0611\n",
            "Epoch 598/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0519\n",
            "Epoch 00598: val_loss did not improve from 0.06066\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0519 - val_loss: 0.0635\n",
            "Epoch 599/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0522\n",
            "Epoch 00599: val_loss did not improve from 0.06066\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0522 - val_loss: 0.0611\n",
            "Epoch 600/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0519\n",
            "Epoch 00600: val_loss did not improve from 0.06066\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0519 - val_loss: 0.0624\n",
            "Epoch 601/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0532\n",
            "Epoch 00601: val_loss did not improve from 0.06066\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0532 - val_loss: 0.0608\n",
            "Epoch 602/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0527\n",
            "Epoch 00602: val_loss did not improve from 0.06066\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0527 - val_loss: 0.0607\n",
            "Epoch 603/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0523\n",
            "Epoch 00603: val_loss did not improve from 0.06066\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0523 - val_loss: 0.0614\n",
            "Epoch 604/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0523\n",
            "Epoch 00604: val_loss did not improve from 0.06066\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0523 - val_loss: 0.0616\n",
            "Epoch 605/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0524\n",
            "Epoch 00605: val_loss improved from 0.06066 to 0.06061, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0525 - val_loss: 0.0606\n",
            "Epoch 606/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0521\n",
            "Epoch 00606: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0521 - val_loss: 0.0632\n",
            "Epoch 607/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0519\n",
            "Epoch 00607: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0519 - val_loss: 0.0610\n",
            "Epoch 608/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0525\n",
            "Epoch 00608: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0525 - val_loss: 0.0623\n",
            "Epoch 609/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0521\n",
            "Epoch 00609: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0521 - val_loss: 0.0616\n",
            "Epoch 610/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0524\n",
            "Epoch 00610: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0524 - val_loss: 0.0615\n",
            "Epoch 611/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0519\n",
            "Epoch 00611: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0519 - val_loss: 0.0613\n",
            "Epoch 612/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0520\n",
            "Epoch 00612: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0520 - val_loss: 0.0616\n",
            "Epoch 613/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0522\n",
            "Epoch 00613: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0522 - val_loss: 0.0610\n",
            "Epoch 614/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0519\n",
            "Epoch 00614: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0519 - val_loss: 0.0624\n",
            "Epoch 615/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0520\n",
            "Epoch 00615: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0520 - val_loss: 0.0609\n",
            "Epoch 616/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0508\n",
            "Epoch 00616: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0508 - val_loss: 0.0614\n",
            "Epoch 617/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0514\n",
            "Epoch 00617: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0514 - val_loss: 0.0629\n",
            "Epoch 618/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0525\n",
            "Epoch 00618: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0525 - val_loss: 0.0626\n",
            "Epoch 619/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0523\n",
            "Epoch 00619: val_loss did not improve from 0.06061\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0523 - val_loss: 0.0612\n",
            "Epoch 620/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0510\n",
            "Epoch 00620: val_loss improved from 0.06061 to 0.06014, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0510 - val_loss: 0.0601\n",
            "Epoch 621/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0522\n",
            "Epoch 00621: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0522 - val_loss: 0.0615\n",
            "Epoch 622/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0526\n",
            "Epoch 00622: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0526 - val_loss: 0.0613\n",
            "Epoch 623/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0516\n",
            "Epoch 00623: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0516 - val_loss: 0.0605\n",
            "Epoch 624/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0515\n",
            "Epoch 00624: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0515 - val_loss: 0.0624\n",
            "Epoch 625/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0520\n",
            "Epoch 00625: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0520 - val_loss: 0.0606\n",
            "Epoch 626/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0513\n",
            "Epoch 00626: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0513 - val_loss: 0.0615\n",
            "Epoch 627/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0519\n",
            "Epoch 00627: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0519 - val_loss: 0.0612\n",
            "Epoch 628/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0515\n",
            "Epoch 00628: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0515 - val_loss: 0.0603\n",
            "Epoch 629/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0514\n",
            "Epoch 00629: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0514 - val_loss: 0.0610\n",
            "Epoch 630/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0509\n",
            "Epoch 00630: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0509 - val_loss: 0.0608\n",
            "Epoch 631/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0518\n",
            "Epoch 00631: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0517 - val_loss: 0.0613\n",
            "Epoch 632/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0519\n",
            "Epoch 00632: val_loss did not improve from 0.06014\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0519 - val_loss: 0.0632\n",
            "Epoch 633/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0515\n",
            "Epoch 00633: val_loss improved from 0.06014 to 0.06012, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0515 - val_loss: 0.0601\n",
            "Epoch 634/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 00634: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0506 - val_loss: 0.0609\n",
            "Epoch 635/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0515\n",
            "Epoch 00635: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0515 - val_loss: 0.0618\n",
            "Epoch 636/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0515\n",
            "Epoch 00636: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0515 - val_loss: 0.0617\n",
            "Epoch 637/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0515\n",
            "Epoch 00637: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0515 - val_loss: 0.0635\n",
            "Epoch 638/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0520\n",
            "Epoch 00638: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0520 - val_loss: 0.0614\n",
            "Epoch 639/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0509\n",
            "Epoch 00639: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0509 - val_loss: 0.0622\n",
            "Epoch 640/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0516\n",
            "Epoch 00640: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0516 - val_loss: 0.0633\n",
            "Epoch 641/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0513\n",
            "Epoch 00641: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0514 - val_loss: 0.0623\n",
            "Epoch 642/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 00642: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0506 - val_loss: 0.0602\n",
            "Epoch 643/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0509\n",
            "Epoch 00643: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0509 - val_loss: 0.0626\n",
            "Epoch 644/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0517\n",
            "Epoch 00644: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0517 - val_loss: 0.0609\n",
            "Epoch 645/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0509\n",
            "Epoch 00645: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0510 - val_loss: 0.0618\n",
            "Epoch 646/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 00646: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0504 - val_loss: 0.0603\n",
            "Epoch 647/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0510\n",
            "Epoch 00647: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0510 - val_loss: 0.0628\n",
            "Epoch 648/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0515\n",
            "Epoch 00648: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0515 - val_loss: 0.0614\n",
            "Epoch 649/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 00649: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0506 - val_loss: 0.0609\n",
            "Epoch 650/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0513\n",
            "Epoch 00650: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0512 - val_loss: 0.0641\n",
            "Epoch 651/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0510\n",
            "Epoch 00651: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0510 - val_loss: 0.0605\n",
            "Epoch 652/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0512\n",
            "Epoch 00652: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0512 - val_loss: 0.0618\n",
            "Epoch 653/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0512\n",
            "Epoch 00653: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0511 - val_loss: 0.0606\n",
            "Epoch 654/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 00654: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0507 - val_loss: 0.0614\n",
            "Epoch 655/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0509\n",
            "Epoch 00655: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0509 - val_loss: 0.0609\n",
            "Epoch 656/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 00656: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0506 - val_loss: 0.0621\n",
            "Epoch 657/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0512\n",
            "Epoch 00657: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0513 - val_loss: 0.0610\n",
            "Epoch 658/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0508\n",
            "Epoch 00658: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0508 - val_loss: 0.0643\n",
            "Epoch 659/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 00659: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0507 - val_loss: 0.0612\n",
            "Epoch 660/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 00660: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0507 - val_loss: 0.0623\n",
            "Epoch 661/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 00661: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0505 - val_loss: 0.0605\n",
            "Epoch 662/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 00662: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0502 - val_loss: 0.0621\n",
            "Epoch 663/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 00663: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0502 - val_loss: 0.0625\n",
            "Epoch 664/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0510\n",
            "Epoch 00664: val_loss did not improve from 0.06012\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0510 - val_loss: 0.0613\n",
            "Epoch 665/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 00665: val_loss improved from 0.06012 to 0.06003, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0507 - val_loss: 0.0600\n",
            "Epoch 666/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 00666: val_loss did not improve from 0.06003\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0501 - val_loss: 0.0627\n",
            "Epoch 667/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0508\n",
            "Epoch 00667: val_loss did not improve from 0.06003\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0508 - val_loss: 0.0635\n",
            "Epoch 668/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 00668: val_loss did not improve from 0.06003\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0505 - val_loss: 0.0631\n",
            "Epoch 669/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0509\n",
            "Epoch 00669: val_loss did not improve from 0.06003\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0509 - val_loss: 0.0612\n",
            "Epoch 670/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 00670: val_loss improved from 0.06003 to 0.06001, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0503 - val_loss: 0.0600\n",
            "Epoch 671/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 00671: val_loss did not improve from 0.06001\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0503 - val_loss: 0.0609\n",
            "Epoch 672/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 00672: val_loss did not improve from 0.06001\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0503 - val_loss: 0.0603\n",
            "Epoch 673/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0496\n",
            "Epoch 00673: val_loss did not improve from 0.06001\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0496 - val_loss: 0.0612\n",
            "Epoch 674/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 00674: val_loss did not improve from 0.06001\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0507 - val_loss: 0.0606\n",
            "Epoch 675/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 00675: val_loss did not improve from 0.06001\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0507 - val_loss: 0.0625\n",
            "Epoch 676/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 00676: val_loss improved from 0.06001 to 0.06000, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0507 - val_loss: 0.0600\n",
            "Epoch 677/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 00677: val_loss did not improve from 0.06000\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0504 - val_loss: 0.0605\n",
            "Epoch 678/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 00678: val_loss did not improve from 0.06000\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0505 - val_loss: 0.0601\n",
            "Epoch 679/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 00679: val_loss did not improve from 0.06000\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0499 - val_loss: 0.0603\n",
            "Epoch 680/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 00680: val_loss did not improve from 0.06000\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0502 - val_loss: 0.0623\n",
            "Epoch 681/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 00681: val_loss did not improve from 0.06000\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0501 - val_loss: 0.0601\n",
            "Epoch 682/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0497\n",
            "Epoch 00682: val_loss improved from 0.06000 to 0.05979, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0497 - val_loss: 0.0598\n",
            "Epoch 683/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0490\n",
            "Epoch 00683: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0490 - val_loss: 0.0613\n",
            "Epoch 684/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 00684: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0503 - val_loss: 0.0628\n",
            "Epoch 685/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 00685: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0506 - val_loss: 0.0629\n",
            "Epoch 686/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0497\n",
            "Epoch 00686: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0497 - val_loss: 0.0627\n",
            "Epoch 687/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 00687: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0499 - val_loss: 0.0619\n",
            "Epoch 688/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 00688: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0501 - val_loss: 0.0605\n",
            "Epoch 689/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 00689: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0500 - val_loss: 0.0626\n",
            "Epoch 690/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 00690: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0499 - val_loss: 0.0604\n",
            "Epoch 691/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 00691: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0499 - val_loss: 0.0634\n",
            "Epoch 692/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 00692: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0502 - val_loss: 0.0608\n",
            "Epoch 693/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 00693: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0499 - val_loss: 0.0605\n",
            "Epoch 694/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0493\n",
            "Epoch 00694: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0493 - val_loss: 0.0603\n",
            "Epoch 695/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0492\n",
            "Epoch 00695: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0492 - val_loss: 0.0607\n",
            "Epoch 696/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 00696: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0500 - val_loss: 0.0603\n",
            "Epoch 697/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 00697: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0500 - val_loss: 0.0610\n",
            "Epoch 698/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 00698: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0501 - val_loss: 0.0625\n",
            "Epoch 699/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0498\n",
            "Epoch 00699: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0498 - val_loss: 0.0614\n",
            "Epoch 700/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0498\n",
            "Epoch 00700: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0498 - val_loss: 0.0604\n",
            "Epoch 701/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 00701: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0499 - val_loss: 0.0599\n",
            "Epoch 702/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0496\n",
            "Epoch 00702: val_loss did not improve from 0.05979\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0496 - val_loss: 0.0611\n",
            "Epoch 703/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0493\n",
            "Epoch 00703: val_loss improved from 0.05979 to 0.05936, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0493 - val_loss: 0.0594\n",
            "Epoch 704/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0491\n",
            "Epoch 00704: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0491 - val_loss: 0.0601\n",
            "Epoch 705/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0495\n",
            "Epoch 00705: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0495 - val_loss: 0.0612\n",
            "Epoch 706/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 00706: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0500 - val_loss: 0.0602\n",
            "Epoch 707/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0493\n",
            "Epoch 00707: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0493 - val_loss: 0.0627\n",
            "Epoch 708/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0496\n",
            "Epoch 00708: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0496 - val_loss: 0.0603\n",
            "Epoch 709/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0489\n",
            "Epoch 00709: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0489 - val_loss: 0.0617\n",
            "Epoch 710/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0493\n",
            "Epoch 00710: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0493 - val_loss: 0.0618\n",
            "Epoch 711/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0497\n",
            "Epoch 00711: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0497 - val_loss: 0.0615\n",
            "Epoch 712/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0496\n",
            "Epoch 00712: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0496 - val_loss: 0.0622\n",
            "Epoch 713/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0490\n",
            "Epoch 00713: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0490 - val_loss: 0.0599\n",
            "Epoch 714/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0493\n",
            "Epoch 00714: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0493 - val_loss: 0.0618\n",
            "Epoch 715/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0494\n",
            "Epoch 00715: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0494 - val_loss: 0.0618\n",
            "Epoch 716/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0493\n",
            "Epoch 00716: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0494 - val_loss: 0.0599\n",
            "Epoch 717/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0484\n",
            "Epoch 00717: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0484 - val_loss: 0.0603\n",
            "Epoch 718/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0494\n",
            "Epoch 00718: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0494 - val_loss: 0.0618\n",
            "Epoch 719/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0497\n",
            "Epoch 00719: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0497 - val_loss: 0.0598\n",
            "Epoch 720/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0491\n",
            "Epoch 00720: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0491 - val_loss: 0.0620\n",
            "Epoch 721/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0489\n",
            "Epoch 00721: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0489 - val_loss: 0.0604\n",
            "Epoch 722/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0491\n",
            "Epoch 00722: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0491 - val_loss: 0.0600\n",
            "Epoch 723/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0496\n",
            "Epoch 00723: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0496 - val_loss: 0.0610\n",
            "Epoch 724/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0488\n",
            "Epoch 00724: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0488 - val_loss: 0.0602\n",
            "Epoch 725/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0490\n",
            "Epoch 00725: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0490 - val_loss: 0.0621\n",
            "Epoch 726/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0486\n",
            "Epoch 00726: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0486 - val_loss: 0.0614\n",
            "Epoch 727/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0496\n",
            "Epoch 00727: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0496 - val_loss: 0.0599\n",
            "Epoch 728/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0485\n",
            "Epoch 00728: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0485 - val_loss: 0.0617\n",
            "Epoch 729/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0490\n",
            "Epoch 00729: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0490 - val_loss: 0.0603\n",
            "Epoch 730/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0493\n",
            "Epoch 00730: val_loss did not improve from 0.05936\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0493 - val_loss: 0.0598\n",
            "Epoch 731/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0483\n",
            "Epoch 00731: val_loss improved from 0.05936 to 0.05926, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0483 - val_loss: 0.0593\n",
            "Epoch 732/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0487\n",
            "Epoch 00732: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0487 - val_loss: 0.0602\n",
            "Epoch 733/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0488\n",
            "Epoch 00733: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0488 - val_loss: 0.0607\n",
            "Epoch 734/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0491\n",
            "Epoch 00734: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0490 - val_loss: 0.0627\n",
            "Epoch 735/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0491\n",
            "Epoch 00735: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0492 - val_loss: 0.0604\n",
            "Epoch 736/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0487\n",
            "Epoch 00736: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0486 - val_loss: 0.0605\n",
            "Epoch 737/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0479\n",
            "Epoch 00737: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0480 - val_loss: 0.0602\n",
            "Epoch 738/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0489\n",
            "Epoch 00738: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0489 - val_loss: 0.0608\n",
            "Epoch 739/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0491\n",
            "Epoch 00739: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0491 - val_loss: 0.0615\n",
            "Epoch 740/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0486\n",
            "Epoch 00740: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0487 - val_loss: 0.0634\n",
            "Epoch 741/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0488\n",
            "Epoch 00741: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0488 - val_loss: 0.0602\n",
            "Epoch 742/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0485\n",
            "Epoch 00742: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0485 - val_loss: 0.0665\n",
            "Epoch 743/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0491\n",
            "Epoch 00743: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0491 - val_loss: 0.0602\n",
            "Epoch 744/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0484\n",
            "Epoch 00744: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0484 - val_loss: 0.0598\n",
            "Epoch 745/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0484\n",
            "Epoch 00745: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0484 - val_loss: 0.0598\n",
            "Epoch 746/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0486\n",
            "Epoch 00746: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0486 - val_loss: 0.0595\n",
            "Epoch 747/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0485\n",
            "Epoch 00747: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0485 - val_loss: 0.0597\n",
            "Epoch 748/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0485\n",
            "Epoch 00748: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0484 - val_loss: 0.0604\n",
            "Epoch 749/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0484\n",
            "Epoch 00749: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0484 - val_loss: 0.0593\n",
            "Epoch 750/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0484\n",
            "Epoch 00750: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0484 - val_loss: 0.0597\n",
            "Epoch 751/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0481\n",
            "Epoch 00751: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0481 - val_loss: 0.0595\n",
            "Epoch 752/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0487\n",
            "Epoch 00752: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0487 - val_loss: 0.0596\n",
            "Epoch 753/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0481\n",
            "Epoch 00753: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0481 - val_loss: 0.0595\n",
            "Epoch 754/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0486\n",
            "Epoch 00754: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0486 - val_loss: 0.0599\n",
            "Epoch 755/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0478\n",
            "Epoch 00755: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0478 - val_loss: 0.0594\n",
            "Epoch 756/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0482\n",
            "Epoch 00756: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0482 - val_loss: 0.0614\n",
            "Epoch 757/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0486\n",
            "Epoch 00757: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0486 - val_loss: 0.0609\n",
            "Epoch 758/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0481\n",
            "Epoch 00758: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0481 - val_loss: 0.0600\n",
            "Epoch 759/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0481\n",
            "Epoch 00759: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0481 - val_loss: 0.0630\n",
            "Epoch 760/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0485\n",
            "Epoch 00760: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0484 - val_loss: 0.0612\n",
            "Epoch 761/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0481\n",
            "Epoch 00761: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0481 - val_loss: 0.0598\n",
            "Epoch 762/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0480\n",
            "Epoch 00762: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0480 - val_loss: 0.0612\n",
            "Epoch 763/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0485\n",
            "Epoch 00763: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0485 - val_loss: 0.0609\n",
            "Epoch 764/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0485\n",
            "Epoch 00764: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0485 - val_loss: 0.0607\n",
            "Epoch 765/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00765: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0474 - val_loss: 0.0604\n",
            "Epoch 766/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0483\n",
            "Epoch 00766: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0483 - val_loss: 0.0601\n",
            "Epoch 767/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0481\n",
            "Epoch 00767: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0481 - val_loss: 0.0626\n",
            "Epoch 768/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0483\n",
            "Epoch 00768: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0483 - val_loss: 0.0594\n",
            "Epoch 769/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0476\n",
            "Epoch 00769: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0476 - val_loss: 0.0607\n",
            "Epoch 770/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0482\n",
            "Epoch 00770: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0482 - val_loss: 0.0605\n",
            "Epoch 771/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0478\n",
            "Epoch 00771: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0478 - val_loss: 0.0615\n",
            "Epoch 772/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0478\n",
            "Epoch 00772: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0478 - val_loss: 0.0595\n",
            "Epoch 773/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00773: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0474 - val_loss: 0.0596\n",
            "Epoch 774/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0481\n",
            "Epoch 00774: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0481 - val_loss: 0.0600\n",
            "Epoch 775/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0477\n",
            "Epoch 00775: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0477 - val_loss: 0.0643\n",
            "Epoch 776/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0481\n",
            "Epoch 00776: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0480 - val_loss: 0.0605\n",
            "Epoch 777/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0477\n",
            "Epoch 00777: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0477 - val_loss: 0.0617\n",
            "Epoch 778/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0477\n",
            "Epoch 00778: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0477 - val_loss: 0.0595\n",
            "Epoch 779/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0475\n",
            "Epoch 00779: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0475 - val_loss: 0.0611\n",
            "Epoch 780/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0477\n",
            "Epoch 00780: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0477 - val_loss: 0.0601\n",
            "Epoch 781/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0472\n",
            "Epoch 00781: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0472 - val_loss: 0.0613\n",
            "Epoch 782/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0483\n",
            "Epoch 00782: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0483 - val_loss: 0.0611\n",
            "Epoch 783/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0477\n",
            "Epoch 00783: val_loss did not improve from 0.05926\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0477 - val_loss: 0.0594\n",
            "Epoch 784/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0480\n",
            "Epoch 00784: val_loss improved from 0.05926 to 0.05920, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0480 - val_loss: 0.0592\n",
            "Epoch 785/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00785: val_loss improved from 0.05920 to 0.05897, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 3s 85ms/step - loss: 0.0475 - val_loss: 0.0590\n",
            "Epoch 786/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0478\n",
            "Epoch 00786: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0478 - val_loss: 0.0592\n",
            "Epoch 787/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0466\n",
            "Epoch 00787: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0466 - val_loss: 0.0629\n",
            "Epoch 788/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0481\n",
            "Epoch 00788: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0480 - val_loss: 0.0602\n",
            "Epoch 789/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0475\n",
            "Epoch 00789: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0475 - val_loss: 0.0612\n",
            "Epoch 790/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0480\n",
            "Epoch 00790: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0480 - val_loss: 0.0604\n",
            "Epoch 791/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00791: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0473 - val_loss: 0.0609\n",
            "Epoch 792/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00792: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0474 - val_loss: 0.0607\n",
            "Epoch 793/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0475\n",
            "Epoch 00793: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0475 - val_loss: 0.0597\n",
            "Epoch 794/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0471\n",
            "Epoch 00794: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0472 - val_loss: 0.0603\n",
            "Epoch 795/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0476\n",
            "Epoch 00795: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0476 - val_loss: 0.0609\n",
            "Epoch 796/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00796: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0474 - val_loss: 0.0609\n",
            "Epoch 797/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0475\n",
            "Epoch 00797: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0475 - val_loss: 0.0600\n",
            "Epoch 798/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0473\n",
            "Epoch 00798: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0472 - val_loss: 0.0600\n",
            "Epoch 799/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0470\n",
            "Epoch 00799: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0470 - val_loss: 0.0606\n",
            "Epoch 800/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0466\n",
            "Epoch 00800: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0466 - val_loss: 0.0605\n",
            "Epoch 801/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0464\n",
            "Epoch 00801: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0464 - val_loss: 0.0592\n",
            "Epoch 802/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0478\n",
            "Epoch 00802: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0478 - val_loss: 0.0600\n",
            "Epoch 803/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00803: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0474 - val_loss: 0.0599\n",
            "Epoch 804/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0472\n",
            "Epoch 00804: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0472 - val_loss: 0.0594\n",
            "Epoch 805/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00805: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0474 - val_loss: 0.0599\n",
            "Epoch 806/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00806: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0475 - val_loss: 0.0606\n",
            "Epoch 807/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0477\n",
            "Epoch 00807: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0477 - val_loss: 0.0590\n",
            "Epoch 808/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0468\n",
            "Epoch 00808: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0468 - val_loss: 0.0590\n",
            "Epoch 809/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0465\n",
            "Epoch 00809: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0465 - val_loss: 0.0599\n",
            "Epoch 810/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0464\n",
            "Epoch 00810: val_loss did not improve from 0.05897\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0464 - val_loss: 0.0629\n",
            "Epoch 811/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00811: val_loss improved from 0.05897 to 0.05894, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0473 - val_loss: 0.0589\n",
            "Epoch 812/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0467\n",
            "Epoch 00812: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0467 - val_loss: 0.0598\n",
            "Epoch 813/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0465\n",
            "Epoch 00813: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0465 - val_loss: 0.0599\n",
            "Epoch 814/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0473\n",
            "Epoch 00814: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0473 - val_loss: 0.0612\n",
            "Epoch 815/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0476\n",
            "Epoch 00815: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0476 - val_loss: 0.0592\n",
            "Epoch 816/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0464\n",
            "Epoch 00816: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0464 - val_loss: 0.0595\n",
            "Epoch 817/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0468\n",
            "Epoch 00817: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0469 - val_loss: 0.0601\n",
            "Epoch 818/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0475\n",
            "Epoch 00818: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0474 - val_loss: 0.0593\n",
            "Epoch 819/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0471\n",
            "Epoch 00819: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0471 - val_loss: 0.0609\n",
            "Epoch 820/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0473\n",
            "Epoch 00820: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0473 - val_loss: 0.0602\n",
            "Epoch 821/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0467\n",
            "Epoch 00821: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0467 - val_loss: 0.0607\n",
            "Epoch 822/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0471\n",
            "Epoch 00822: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0471 - val_loss: 0.0591\n",
            "Epoch 823/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00823: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0461 - val_loss: 0.0590\n",
            "Epoch 824/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0468\n",
            "Epoch 00824: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0468 - val_loss: 0.0595\n",
            "Epoch 825/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0471\n",
            "Epoch 00825: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0471 - val_loss: 0.0597\n",
            "Epoch 826/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0465\n",
            "Epoch 00826: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0465 - val_loss: 0.0601\n",
            "Epoch 827/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00827: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0461 - val_loss: 0.0591\n",
            "Epoch 828/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0468\n",
            "Epoch 00828: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0468 - val_loss: 0.0596\n",
            "Epoch 829/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0465\n",
            "Epoch 00829: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0465 - val_loss: 0.0591\n",
            "Epoch 830/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0469\n",
            "Epoch 00830: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0469 - val_loss: 0.0592\n",
            "Epoch 831/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0465\n",
            "Epoch 00831: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0465 - val_loss: 0.0597\n",
            "Epoch 832/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0469\n",
            "Epoch 00832: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0469 - val_loss: 0.0602\n",
            "Epoch 833/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0466\n",
            "Epoch 00833: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0466 - val_loss: 0.0594\n",
            "Epoch 834/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0464\n",
            "Epoch 00834: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0464 - val_loss: 0.0590\n",
            "Epoch 835/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0463\n",
            "Epoch 00835: val_loss did not improve from 0.05894\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0463 - val_loss: 0.0599\n",
            "Epoch 836/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0462\n",
            "Epoch 00836: val_loss improved from 0.05894 to 0.05887, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 71ms/step - loss: 0.0462 - val_loss: 0.0589\n",
            "Epoch 837/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0463\n",
            "Epoch 00837: val_loss did not improve from 0.05887\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0463 - val_loss: 0.0590\n",
            "Epoch 838/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0469\n",
            "Epoch 00838: val_loss did not improve from 0.05887\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0468 - val_loss: 0.0594\n",
            "Epoch 839/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0463\n",
            "Epoch 00839: val_loss did not improve from 0.05887\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0463 - val_loss: 0.0595\n",
            "Epoch 840/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0466\n",
            "Epoch 00840: val_loss did not improve from 0.05887\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0466 - val_loss: 0.0598\n",
            "Epoch 841/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0462\n",
            "Epoch 00841: val_loss did not improve from 0.05887\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0462 - val_loss: 0.0590\n",
            "Epoch 842/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0465\n",
            "Epoch 00842: val_loss did not improve from 0.05887\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0465 - val_loss: 0.0602\n",
            "Epoch 843/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0464\n",
            "Epoch 00843: val_loss improved from 0.05887 to 0.05874, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0464 - val_loss: 0.0587\n",
            "Epoch 844/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0459\n",
            "Epoch 00844: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0459 - val_loss: 0.0602\n",
            "Epoch 845/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0469\n",
            "Epoch 00845: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0469 - val_loss: 0.0605\n",
            "Epoch 846/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0461\n",
            "Epoch 00846: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0461 - val_loss: 0.0589\n",
            "Epoch 847/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0459\n",
            "Epoch 00847: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0459 - val_loss: 0.0588\n",
            "Epoch 848/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0467\n",
            "Epoch 00848: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0467 - val_loss: 0.0596\n",
            "Epoch 849/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00849: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0460 - val_loss: 0.0603\n",
            "Epoch 850/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0464\n",
            "Epoch 00850: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0464 - val_loss: 0.0589\n",
            "Epoch 851/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0457\n",
            "Epoch 00851: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0457 - val_loss: 0.0608\n",
            "Epoch 852/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0465\n",
            "Epoch 00852: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0465 - val_loss: 0.0590\n",
            "Epoch 853/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0455\n",
            "Epoch 00853: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0455 - val_loss: 0.0597\n",
            "Epoch 854/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0466\n",
            "Epoch 00854: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0465 - val_loss: 0.0615\n",
            "Epoch 855/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0466\n",
            "Epoch 00855: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0466 - val_loss: 0.0594\n",
            "Epoch 856/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0452\n",
            "Epoch 00856: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0451 - val_loss: 0.0598\n",
            "Epoch 857/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00857: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0460 - val_loss: 0.0605\n",
            "Epoch 858/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0466\n",
            "Epoch 00858: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0466 - val_loss: 0.0594\n",
            "Epoch 859/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00859: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0460 - val_loss: 0.0595\n",
            "Epoch 860/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00860: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0460 - val_loss: 0.0611\n",
            "Epoch 861/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0462\n",
            "Epoch 00861: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0462 - val_loss: 0.0591\n",
            "Epoch 862/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0453\n",
            "Epoch 00862: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0453 - val_loss: 0.0594\n",
            "Epoch 863/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0466\n",
            "Epoch 00863: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0466 - val_loss: 0.0593\n",
            "Epoch 864/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0459\n",
            "Epoch 00864: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0459 - val_loss: 0.0593\n",
            "Epoch 865/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0458\n",
            "Epoch 00865: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0458 - val_loss: 0.0589\n",
            "Epoch 866/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00866: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0460 - val_loss: 0.0596\n",
            "Epoch 867/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0457\n",
            "Epoch 00867: val_loss did not improve from 0.05874\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0457 - val_loss: 0.0599\n",
            "Epoch 868/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0464\n",
            "Epoch 00868: val_loss improved from 0.05874 to 0.05864, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0464 - val_loss: 0.0586\n",
            "Epoch 869/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0458\n",
            "Epoch 00869: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0458 - val_loss: 0.0589\n",
            "Epoch 870/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0456\n",
            "Epoch 00870: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0456 - val_loss: 0.0592\n",
            "Epoch 871/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0452\n",
            "Epoch 00871: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0452 - val_loss: 0.0593\n",
            "Epoch 872/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0458\n",
            "Epoch 00872: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0459 - val_loss: 0.0591\n",
            "Epoch 873/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00873: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0460 - val_loss: 0.0590\n",
            "Epoch 874/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0459\n",
            "Epoch 00874: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0459 - val_loss: 0.0590\n",
            "Epoch 875/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0455\n",
            "Epoch 00875: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0455 - val_loss: 0.0596\n",
            "Epoch 876/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0456\n",
            "Epoch 00876: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0456 - val_loss: 0.0618\n",
            "Epoch 877/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0455\n",
            "Epoch 00877: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0455 - val_loss: 0.0587\n",
            "Epoch 878/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0454\n",
            "Epoch 00878: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0454 - val_loss: 0.0592\n",
            "Epoch 879/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0458\n",
            "Epoch 00879: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0458 - val_loss: 0.0590\n",
            "Epoch 880/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0458\n",
            "Epoch 00880: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0458 - val_loss: 0.0594\n",
            "Epoch 881/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0459\n",
            "Epoch 00881: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0459 - val_loss: 0.0589\n",
            "Epoch 882/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0452\n",
            "Epoch 00882: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0452 - val_loss: 0.0590\n",
            "Epoch 883/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0455\n",
            "Epoch 00883: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0455 - val_loss: 0.0592\n",
            "Epoch 884/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0454\n",
            "Epoch 00884: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0454 - val_loss: 0.0591\n",
            "Epoch 885/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0449\n",
            "Epoch 00885: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0449 - val_loss: 0.0612\n",
            "Epoch 886/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0456\n",
            "Epoch 00886: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0456 - val_loss: 0.0604\n",
            "Epoch 887/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00887: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0460 - val_loss: 0.0594\n",
            "Epoch 888/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0456\n",
            "Epoch 00888: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0456 - val_loss: 0.0615\n",
            "Epoch 889/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0456\n",
            "Epoch 00889: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0456 - val_loss: 0.0590\n",
            "Epoch 890/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0448\n",
            "Epoch 00890: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0448 - val_loss: 0.0622\n",
            "Epoch 891/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0454\n",
            "Epoch 00891: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0454 - val_loss: 0.0600\n",
            "Epoch 892/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0448\n",
            "Epoch 00892: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0448 - val_loss: 0.0589\n",
            "Epoch 893/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0454\n",
            "Epoch 00893: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0454 - val_loss: 0.0606\n",
            "Epoch 894/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0456\n",
            "Epoch 00894: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0456 - val_loss: 0.0607\n",
            "Epoch 895/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0456\n",
            "Epoch 00895: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0456 - val_loss: 0.0590\n",
            "Epoch 896/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0453\n",
            "Epoch 00896: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0453 - val_loss: 0.0595\n",
            "Epoch 897/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0454\n",
            "Epoch 00897: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0454 - val_loss: 0.0593\n",
            "Epoch 898/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0452\n",
            "Epoch 00898: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0452 - val_loss: 0.0589\n",
            "Epoch 899/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0450\n",
            "Epoch 00899: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0450 - val_loss: 0.0589\n",
            "Epoch 900/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0453\n",
            "Epoch 00900: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0453 - val_loss: 0.0594\n",
            "Epoch 901/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0449\n",
            "Epoch 00901: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0449 - val_loss: 0.0604\n",
            "Epoch 902/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0452\n",
            "Epoch 00902: val_loss did not improve from 0.05864\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0452 - val_loss: 0.0605\n",
            "Epoch 903/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0452\n",
            "Epoch 00903: val_loss improved from 0.05864 to 0.05854, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0452 - val_loss: 0.0585\n",
            "Epoch 904/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0449\n",
            "Epoch 00904: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0449 - val_loss: 0.0604\n",
            "Epoch 905/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0454\n",
            "Epoch 00905: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0454 - val_loss: 0.0597\n",
            "Epoch 906/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0449\n",
            "Epoch 00906: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0449 - val_loss: 0.0601\n",
            "Epoch 907/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0449\n",
            "Epoch 00907: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0449 - val_loss: 0.0600\n",
            "Epoch 908/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0454\n",
            "Epoch 00908: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0454 - val_loss: 0.0617\n",
            "Epoch 909/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0455\n",
            "Epoch 00909: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0455 - val_loss: 0.0595\n",
            "Epoch 910/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0450\n",
            "Epoch 00910: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0450 - val_loss: 0.0587\n",
            "Epoch 911/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0448\n",
            "Epoch 00911: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0448 - val_loss: 0.0597\n",
            "Epoch 912/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00912: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0443 - val_loss: 0.0588\n",
            "Epoch 913/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0448\n",
            "Epoch 00913: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0448 - val_loss: 0.0610\n",
            "Epoch 914/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0452\n",
            "Epoch 00914: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0452 - val_loss: 0.0607\n",
            "Epoch 915/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0451\n",
            "Epoch 00915: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0451 - val_loss: 0.0603\n",
            "Epoch 916/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0451\n",
            "Epoch 00916: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0451 - val_loss: 0.0592\n",
            "Epoch 917/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0445\n",
            "Epoch 00917: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0445 - val_loss: 0.0599\n",
            "Epoch 918/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0449\n",
            "Epoch 00918: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0450 - val_loss: 0.0592\n",
            "Epoch 919/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0452\n",
            "Epoch 00919: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0452 - val_loss: 0.0599\n",
            "Epoch 920/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0450\n",
            "Epoch 00920: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0450 - val_loss: 0.0593\n",
            "Epoch 921/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0446\n",
            "Epoch 00921: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0446 - val_loss: 0.0609\n",
            "Epoch 922/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0449\n",
            "Epoch 00922: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0449 - val_loss: 0.0603\n",
            "Epoch 923/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0450\n",
            "Epoch 00923: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0450 - val_loss: 0.0600\n",
            "Epoch 924/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00924: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0443 - val_loss: 0.0593\n",
            "Epoch 925/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0451\n",
            "Epoch 00925: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0451 - val_loss: 0.0588\n",
            "Epoch 926/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0445\n",
            "Epoch 00926: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0445 - val_loss: 0.0590\n",
            "Epoch 927/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00927: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0444 - val_loss: 0.0587\n",
            "Epoch 928/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0440\n",
            "Epoch 00928: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0440 - val_loss: 0.0602\n",
            "Epoch 929/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0456\n",
            "Epoch 00929: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0456 - val_loss: 0.0605\n",
            "Epoch 930/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0449\n",
            "Epoch 00930: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0449 - val_loss: 0.0600\n",
            "Epoch 931/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0450\n",
            "Epoch 00931: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0449 - val_loss: 0.0600\n",
            "Epoch 932/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00932: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0443 - val_loss: 0.0592\n",
            "Epoch 933/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0440\n",
            "Epoch 00933: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0440 - val_loss: 0.0593\n",
            "Epoch 934/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0447\n",
            "Epoch 00934: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0447 - val_loss: 0.0587\n",
            "Epoch 935/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0448\n",
            "Epoch 00935: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0448 - val_loss: 0.0593\n",
            "Epoch 936/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0447\n",
            "Epoch 00936: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0447 - val_loss: 0.0592\n",
            "Epoch 937/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0437\n",
            "Epoch 00937: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0437 - val_loss: 0.0586\n",
            "Epoch 938/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0444\n",
            "Epoch 00938: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0443 - val_loss: 0.0586\n",
            "Epoch 939/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00939: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0443 - val_loss: 0.0608\n",
            "Epoch 940/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0452\n",
            "Epoch 00940: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0452 - val_loss: 0.0593\n",
            "Epoch 941/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0441\n",
            "Epoch 00941: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0441 - val_loss: 0.0595\n",
            "Epoch 942/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0444\n",
            "Epoch 00942: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0444 - val_loss: 0.0592\n",
            "Epoch 943/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0442\n",
            "Epoch 00943: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0442 - val_loss: 0.0603\n",
            "Epoch 944/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0448\n",
            "Epoch 00944: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0448 - val_loss: 0.0604\n",
            "Epoch 945/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0442\n",
            "Epoch 00945: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0442 - val_loss: 0.0598\n",
            "Epoch 946/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0446\n",
            "Epoch 00946: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0446 - val_loss: 0.0599\n",
            "Epoch 947/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0444\n",
            "Epoch 00947: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0444 - val_loss: 0.0610\n",
            "Epoch 948/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0444\n",
            "Epoch 00948: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0444 - val_loss: 0.0631\n",
            "Epoch 949/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0446\n",
            "Epoch 00949: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0445 - val_loss: 0.0590\n",
            "Epoch 950/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0442\n",
            "Epoch 00950: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0442 - val_loss: 0.0593\n",
            "Epoch 951/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0442\n",
            "Epoch 00951: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0442 - val_loss: 0.0588\n",
            "Epoch 952/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0441\n",
            "Epoch 00952: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0441 - val_loss: 0.0590\n",
            "Epoch 953/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0446\n",
            "Epoch 00953: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0446 - val_loss: 0.0591\n",
            "Epoch 954/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00954: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0443 - val_loss: 0.0591\n",
            "Epoch 955/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0436\n",
            "Epoch 00955: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0436 - val_loss: 0.0590\n",
            "Epoch 956/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0435\n",
            "Epoch 00956: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0436 - val_loss: 0.0600\n",
            "Epoch 957/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0446\n",
            "Epoch 00957: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0446 - val_loss: 0.0600\n",
            "Epoch 958/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0440\n",
            "Epoch 00958: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0441 - val_loss: 0.0613\n",
            "Epoch 959/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0438\n",
            "Epoch 00959: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0438 - val_loss: 0.0624\n",
            "Epoch 960/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00960: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0443 - val_loss: 0.0600\n",
            "Epoch 961/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0439\n",
            "Epoch 00961: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0439 - val_loss: 0.0591\n",
            "Epoch 962/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0437\n",
            "Epoch 00962: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0437 - val_loss: 0.0597\n",
            "Epoch 963/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0441\n",
            "Epoch 00963: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0441 - val_loss: 0.0604\n",
            "Epoch 964/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0448\n",
            "Epoch 00964: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0448 - val_loss: 0.0592\n",
            "Epoch 965/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0441\n",
            "Epoch 00965: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0441 - val_loss: 0.0592\n",
            "Epoch 966/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0441\n",
            "Epoch 00966: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0441 - val_loss: 0.0594\n",
            "Epoch 967/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0439\n",
            "Epoch 00967: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0439 - val_loss: 0.0600\n",
            "Epoch 968/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0442\n",
            "Epoch 00968: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0441 - val_loss: 0.0586\n",
            "Epoch 969/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0441\n",
            "Epoch 00969: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0441 - val_loss: 0.0596\n",
            "Epoch 970/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0440\n",
            "Epoch 00970: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0441 - val_loss: 0.0590\n",
            "Epoch 971/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0434\n",
            "Epoch 00971: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0434 - val_loss: 0.0588\n",
            "Epoch 972/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0440\n",
            "Epoch 00972: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0440 - val_loss: 0.0588\n",
            "Epoch 973/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0438\n",
            "Epoch 00973: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0438 - val_loss: 0.0593\n",
            "Epoch 974/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0432\n",
            "Epoch 00974: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 56ms/step - loss: 0.0432 - val_loss: 0.0596\n",
            "Epoch 975/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0441\n",
            "Epoch 00975: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0440 - val_loss: 0.0611\n",
            "Epoch 976/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0442\n",
            "Epoch 00976: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0442 - val_loss: 0.0593\n",
            "Epoch 977/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0438\n",
            "Epoch 00977: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0437 - val_loss: 0.0594\n",
            "Epoch 978/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0439\n",
            "Epoch 00978: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0439 - val_loss: 0.0605\n",
            "Epoch 979/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0434\n",
            "Epoch 00979: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0434 - val_loss: 0.0590\n",
            "Epoch 980/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0440\n",
            "Epoch 00980: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0440 - val_loss: 0.0593\n",
            "Epoch 981/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0441\n",
            "Epoch 00981: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0441 - val_loss: 0.0593\n",
            "Epoch 982/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0439\n",
            "Epoch 00982: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0439 - val_loss: 0.0600\n",
            "Epoch 983/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0435\n",
            "Epoch 00983: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0435 - val_loss: 0.0598\n",
            "Epoch 984/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0439\n",
            "Epoch 00984: val_loss did not improve from 0.05854\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0439 - val_loss: 0.0593\n",
            "Epoch 985/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0437\n",
            "Epoch 00985: val_loss improved from 0.05854 to 0.05847, saving model to autoencoder_celeba.hdf5\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.0436 - val_loss: 0.0585\n",
            "Epoch 986/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0436\n",
            "Epoch 00986: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0436 - val_loss: 0.0597\n",
            "Epoch 987/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0439\n",
            "Epoch 00987: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0438 - val_loss: 0.0592\n",
            "Epoch 988/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0433\n",
            "Epoch 00988: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0433 - val_loss: 0.0591\n",
            "Epoch 989/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0437\n",
            "Epoch 00989: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0437 - val_loss: 0.0604\n",
            "Epoch 990/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0435\n",
            "Epoch 00990: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0435 - val_loss: 0.0601\n",
            "Epoch 991/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0439\n",
            "Epoch 00991: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0439 - val_loss: 0.0602\n",
            "Epoch 992/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0432\n",
            "Epoch 00992: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0432 - val_loss: 0.0595\n",
            "Epoch 993/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0434\n",
            "Epoch 00993: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0434 - val_loss: 0.0599\n",
            "Epoch 994/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0436\n",
            "Epoch 00994: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0436 - val_loss: 0.0594\n",
            "Epoch 995/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0436\n",
            "Epoch 00995: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0436 - val_loss: 0.0600\n",
            "Epoch 996/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0431\n",
            "Epoch 00996: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0431 - val_loss: 0.0588\n",
            "Epoch 997/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0432\n",
            "Epoch 00997: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0432 - val_loss: 0.0601\n",
            "Epoch 998/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0436\n",
            "Epoch 00998: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0436 - val_loss: 0.0609\n",
            "Epoch 999/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0436\n",
            "Epoch 00999: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0436 - val_loss: 0.0590\n",
            "Epoch 1000/1000\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0432\n",
            "Epoch 01000: val_loss did not improve from 0.05847\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0432 - val_loss: 0.0602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQf9tlnbCx2v"
      },
      "source": [
        "##Viewing the outputs to the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "o9yfOk06l5jY",
        "outputId": "2706666a-a672-430b-d072-52be0a30baab"
      },
      "source": [
        "%matplotlib inline\n",
        "plt.figure()\n",
        "plt.imshow(X_test[6]) # A sample image in the test set\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBMeIWVqC4cr"
      },
      "source": [
        "## The predicted/ deepfake image of this image from the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwM-th9otK5p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "cf6ec00f-4572-437a-f0be-0e2ed72ab02c"
      },
      "source": [
        "output_image = autoencoder.predict(np.array([X_test[6]]))\n",
        "plt.figure()\n",
        "plt.imshow(output_image[0])\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhKdK3SFDBzK"
      },
      "source": [
        "## Loading the model with the least validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ymCX2tT8yQe"
      },
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "model_n = load_model('/content/autoencoder_celeba.hdf5')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbAdJD-JDFip"
      },
      "source": [
        "## Testing the 'least validation loss' model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "oykuMeJEBuOJ",
        "outputId": "befe45de-fa28-49e4-894e-60df9b43487c"
      },
      "source": [
        "%matplotlib inline\n",
        "plt.figure()\n",
        "plt.imshow(X_test[70])\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "tAVaulskBxOk",
        "outputId": "0d07e7cb-c2bb-4061-fe34-fee8f3d625c3"
      },
      "source": [
        "output_image = model_n.predict(np.array([X_test[70]]))\n",
        "plt.figure()\n",
        "plt.imshow(output_image[0])\n",
        "plt.show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_ZS2TWfDNoL"
      },
      "source": [
        "## Finally, the loss metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "A1LG2r0LB1SA",
        "outputId": "e6b9e4e9-4342-4477-f9ba-1fb75777456c"
      },
      "source": [
        "fig = plt.figure()\n",
        "#t = fig.set_title('Autoencoder for Deepfake Image prediction metrics', fontsize=12)\n",
        "EPOCHS=1000\n",
        "epoch_list = list(range(1,EPOCHS+1))\n",
        "ax2 = fig.add_subplot(111)\n",
        "ax2.plot(epoch_list, history1.history['loss'], label='Train Loss')\n",
        "ax2.plot(epoch_list, history1.history['val_loss'], label='Validation Loss')\n",
        "ax2.set_ylabel('Loss Value')\n",
        "ax2.set_xlabel('Epoch #')\n",
        "ax2.set_title('Loss metrics', fontsize=12)\n",
        "l2 = ax2.legend(loc=\"best\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agCWbYcUD00U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}