{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deepfake_Image_Generation.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSP6uxa1qlLm"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os,pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "9X3fZdglsxyl",
        "outputId": "6d12c12b-c224-4c93-8a7f-de7dc581b809"
      },
      "source": [
        "path='/content/drive/MyDrive/celeba'+'/161979.jpg'\n",
        "img=cv2.imread(path)\n",
        "img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3478816850>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rUhApbUlB8M",
        "outputId": "d8966927-0063-430a-9f08-4c919e904597"
      },
      "source": [
        "print(img.shape)\n",
        "print(type(img))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 64, 3)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fUAl9hDlPkb"
      },
      "source": [
        "##Creating our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6HsAY7rm4Zi"
      },
      "source": [
        "#24396 images in the celeba dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB5AMfK5lGaE"
      },
      "source": [
        "path_d='/content/drive/MyDrive/celeba'\n",
        "ct=161979\n",
        "X = np.zeros((1000,64 , 64, 3), dtype=np.float32)\n",
        "for path in pathlib.Path(path_d).iterdir():\n",
        "  img = cv2.imread(str(path))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = img.astype('float32')\n",
        "  X[ct-161979]=img/255\n",
        "  if (ct-161979)==999:\n",
        "    break\n",
        "  ct+=1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtMm1SgIh2s1"
      },
      "source": [
        "##Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1F9ROFsnBd9"
      },
      "source": [
        "input_img = layers.Input(shape=(64, 64, 3))\n",
        "x = layers.Conv2D(128,kernel_size=5, strides=2, padding='same',activation='relu')(input_img)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(256,kernel_size=5, strides=2, padding='same',activation='relu')(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(512,kernel_size=5, strides=2, padding='same',activation='relu')(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(4608)(x)\n",
        "encoded = layers.Reshape((3,3,512))(x)\n",
        "encoder = keras.Model(input_img, encoded,name=\"encoder\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ZE3Mxxh9uf"
      },
      "source": [
        "##Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWgAGa7ah4km"
      },
      "source": [
        "decoder_input= layers.Input(shape=((3,3,512)))\n",
        "x = layers.Conv2D(512,kernel_size=5, strides=2, padding='same',activation='relu')(decoder_input)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(256,kernel_size=5, strides=2, padding='same',activation='relu')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(128,kernel_size=5, strides=2, padding='same',activation='relu')(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(np.prod((64, 64, 3)))(x)\n",
        "decoded = layers.Reshape((64, 64, 3))(x)\n",
        "decoder = keras.Model(decoder_input, decoded,name=\"decoder\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAvh3LF6h_Dc",
        "outputId": "2013c975-783c-4d3e-d6c7-da024f01b518"
      },
      "source": [
        "auto_input = layers.Input(shape=(64,64,3))\n",
        "encoded = encoder(auto_input)\n",
        "decoded = decoder(encoded)\n",
        " \n",
        "autoencoder = keras.Model(auto_input, decoded,name=\"autoencoder\")\n",
        "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(lr=5e-5, beta_1=0.5, beta_2=0.999), loss='mae')\n",
        "autoencoder.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
            "_________________________________________________________________\n",
            "encoder (Functional)         (None, 3, 3, 512)         6470400   \n",
            "_________________________________________________________________\n",
            "decoder (Functional)         (None, 64, 64, 3)         16954240  \n",
            "=================================================================\n",
            "Total params: 23,424,640\n",
            "Trainable params: 23,424,640\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBMYMvesiB5z"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,X, test_size=0.20, random_state=0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8NSwOkFlrWA",
        "outputId": "accce40f-5a67-4a2e-d3a0-a631eda7f46a"
      },
      "source": [
        "checkpoint1 = ModelCheckpoint(\"autoencoder_celeba.hdf5\", monitor='val_loss', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "history1 = autoencoder.fit(X_train, X_train, epochs=1000, batch_size=64, shuffle=True, validation_data=(X_test, X_test), callbacks=[checkpoint1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/1000\n",
            "13/13 [==============================] - 19s 85ms/step - loss: 0.4547 - val_loss: 0.4309\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.43086, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 2/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.3267 - val_loss: 0.2081\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.43086 to 0.20806, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 3/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1763 - val_loss: 0.1603\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.20806 to 0.16031, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 4/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1599 - val_loss: 0.1569\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.16031 to 0.15687, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 5/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1554 - val_loss: 0.1672\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.15687\n",
            "Epoch 6/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1553 - val_loss: 0.1589\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.15687\n",
            "Epoch 7/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1570 - val_loss: 0.1568\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.15687 to 0.15684, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 8/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1560 - val_loss: 0.1558\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.15684 to 0.15583, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 9/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1534 - val_loss: 0.1633\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.15583\n",
            "Epoch 10/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1542 - val_loss: 0.1553\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.15583 to 0.15534, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 11/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1539 - val_loss: 0.1579\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.15534\n",
            "Epoch 12/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1540 - val_loss: 0.1712\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.15534\n",
            "Epoch 13/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1543 - val_loss: 0.1572\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.15534\n",
            "Epoch 14/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1541 - val_loss: 0.1601\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.15534\n",
            "Epoch 15/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1542 - val_loss: 0.1553\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.15534 to 0.15530, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 16/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1536 - val_loss: 0.1553\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.15530 to 0.15526, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 17/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1535 - val_loss: 0.1582\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.15526\n",
            "Epoch 18/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1546 - val_loss: 0.1552\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.15526 to 0.15516, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 19/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1551 - val_loss: 0.1553\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.15516\n",
            "Epoch 20/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1534 - val_loss: 0.1548\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.15516 to 0.15479, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 21/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.1534 - val_loss: 0.1567\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.15479\n",
            "Epoch 22/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1525 - val_loss: 0.1564\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.15479\n",
            "Epoch 23/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1538 - val_loss: 0.1550\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.15479\n",
            "Epoch 24/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1536 - val_loss: 0.1557\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.15479\n",
            "Epoch 25/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1521 - val_loss: 0.1576\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.15479\n",
            "Epoch 26/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1545 - val_loss: 0.1550\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.15479\n",
            "Epoch 27/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1527 - val_loss: 0.1569\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.15479\n",
            "Epoch 28/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1538 - val_loss: 0.1541\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.15479 to 0.15414, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 29/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1542 - val_loss: 0.1555\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.15414\n",
            "Epoch 30/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1529 - val_loss: 0.1538\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.15414 to 0.15382, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 31/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1535 - val_loss: 0.1540\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.15382\n",
            "Epoch 32/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1503 - val_loss: 0.1436\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.15382 to 0.14361, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 33/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1439 - val_loss: 0.1400\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.14361 to 0.14002, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 34/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1400 - val_loss: 0.1410\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.14002\n",
            "Epoch 35/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1424 - val_loss: 0.1379\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.14002 to 0.13786, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 36/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1418 - val_loss: 0.1364\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.13786 to 0.13641, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 37/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1369 - val_loss: 0.1408\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.13641\n",
            "Epoch 38/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1370 - val_loss: 0.1388\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.13641\n",
            "Epoch 39/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1385 - val_loss: 0.1392\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.13641\n",
            "Epoch 40/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1367 - val_loss: 0.1365\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.13641\n",
            "Epoch 41/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1356 - val_loss: 0.1396\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.13641\n",
            "Epoch 42/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1381 - val_loss: 0.1380\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.13641\n",
            "Epoch 43/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1378 - val_loss: 0.1355\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.13641 to 0.13551, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 44/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1343 - val_loss: 0.1351\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.13551 to 0.13505, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 45/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1351 - val_loss: 0.1382\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.13505\n",
            "Epoch 46/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1364 - val_loss: 0.1364\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.13505\n",
            "Epoch 47/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1345 - val_loss: 0.1380\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.13505\n",
            "Epoch 48/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1362 - val_loss: 0.1355\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.13505\n",
            "Epoch 49/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1355 - val_loss: 0.1376\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.13505\n",
            "Epoch 50/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1347 - val_loss: 0.1346\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.13505 to 0.13456, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 51/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1357 - val_loss: 0.1352\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.13456\n",
            "Epoch 52/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1357 - val_loss: 0.1361\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.13456\n",
            "Epoch 53/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1362 - val_loss: 0.1367\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.13456\n",
            "Epoch 54/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1355 - val_loss: 0.1360\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.13456\n",
            "Epoch 55/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1323 - val_loss: 0.1340\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.13456 to 0.13403, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 56/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1348 - val_loss: 0.1375\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.13403\n",
            "Epoch 57/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1367 - val_loss: 0.1319\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.13403 to 0.13187, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 58/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1333 - val_loss: 0.1315\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.13187 to 0.13146, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 59/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1310 - val_loss: 0.1458\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.13146\n",
            "Epoch 60/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1316 - val_loss: 0.1272\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.13146 to 0.12715, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 61/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1300 - val_loss: 0.1294\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.12715\n",
            "Epoch 62/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1255 - val_loss: 0.1265\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.12715 to 0.12648, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 63/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1262 - val_loss: 0.1361\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.12648\n",
            "Epoch 64/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1244 - val_loss: 0.1244\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.12648 to 0.12440, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 65/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1254 - val_loss: 0.1314\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.12440\n",
            "Epoch 66/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1231 - val_loss: 0.1270\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.12440\n",
            "Epoch 67/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1255 - val_loss: 0.1288\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.12440\n",
            "Epoch 68/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1233 - val_loss: 0.1222\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.12440 to 0.12217, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 69/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1208 - val_loss: 0.1256\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.12217\n",
            "Epoch 70/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1219 - val_loss: 0.1238\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.12217\n",
            "Epoch 71/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1215 - val_loss: 0.1203\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.12217 to 0.12033, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 72/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.1210 - val_loss: 0.1195\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.12033 to 0.11951, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 73/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1185 - val_loss: 0.1197\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.11951\n",
            "Epoch 74/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1226 - val_loss: 0.1200\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.11951\n",
            "Epoch 75/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1177 - val_loss: 0.1181\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.11951 to 0.11808, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 76/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1175 - val_loss: 0.1222\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.11808\n",
            "Epoch 77/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1186 - val_loss: 0.1234\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.11808\n",
            "Epoch 78/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1155 - val_loss: 0.1176\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.11808 to 0.11765, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 79/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1153 - val_loss: 0.1240\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.11765\n",
            "Epoch 80/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1174 - val_loss: 0.1191\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.11765\n",
            "Epoch 81/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1168 - val_loss: 0.1160\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.11765 to 0.11598, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 82/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1150 - val_loss: 0.1244\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.11598\n",
            "Epoch 83/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1147 - val_loss: 0.1148\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.11598 to 0.11485, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 84/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1129 - val_loss: 0.1184\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.11485\n",
            "Epoch 85/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1150 - val_loss: 0.1178\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.11485\n",
            "Epoch 86/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1131 - val_loss: 0.1158\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.11485\n",
            "Epoch 87/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1124 - val_loss: 0.1215\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.11485\n",
            "Epoch 88/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1140 - val_loss: 0.1151\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.11485\n",
            "Epoch 89/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1165 - val_loss: 0.1149\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.11485\n",
            "Epoch 90/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1133 - val_loss: 0.1147\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.11485 to 0.11467, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 91/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1122 - val_loss: 0.1172\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.11467\n",
            "Epoch 92/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1140 - val_loss: 0.1144\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.11467 to 0.11436, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 93/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1124 - val_loss: 0.1179\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.11436\n",
            "Epoch 94/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1131 - val_loss: 0.1160\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.11436\n",
            "Epoch 95/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1136 - val_loss: 0.1152\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.11436\n",
            "Epoch 96/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.1096 - val_loss: 0.1147\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.11436\n",
            "Epoch 97/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1119 - val_loss: 0.1165\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.11436\n",
            "Epoch 98/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1118 - val_loss: 0.1157\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.11436\n",
            "Epoch 99/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1103 - val_loss: 0.1131\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.11436 to 0.11308, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 100/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1109 - val_loss: 0.1174\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.11308\n",
            "Epoch 101/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1108 - val_loss: 0.1119\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.11308 to 0.11190, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 102/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1093 - val_loss: 0.1109\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.11190 to 0.11094, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 103/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1095 - val_loss: 0.1137\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.11094\n",
            "Epoch 104/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1088 - val_loss: 0.1117\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.11094\n",
            "Epoch 105/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1096 - val_loss: 0.1142\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.11094\n",
            "Epoch 106/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1085 - val_loss: 0.1096\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.11094 to 0.10960, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 107/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1074 - val_loss: 0.1135\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.10960\n",
            "Epoch 108/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1091 - val_loss: 0.1087\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.10960 to 0.10872, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 109/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.1086 - val_loss: 0.1080\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.10872 to 0.10796, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 110/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.1070 - val_loss: 0.1077\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.10796 to 0.10771, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 111/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1044 - val_loss: 0.1070\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.10771 to 0.10705, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 112/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1053 - val_loss: 0.1120\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.10705\n",
            "Epoch 113/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1075 - val_loss: 0.1081\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.10705\n",
            "Epoch 114/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1041 - val_loss: 0.1093\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.10705\n",
            "Epoch 115/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1067 - val_loss: 0.1067\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.10705 to 0.10670, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 116/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1028 - val_loss: 0.1092\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.10670\n",
            "Epoch 117/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1051 - val_loss: 0.1071\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.10670\n",
            "Epoch 118/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1047 - val_loss: 0.1062\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.10670 to 0.10624, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 119/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1036 - val_loss: 0.1115\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.10624\n",
            "Epoch 120/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1044 - val_loss: 0.1067\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.10624\n",
            "Epoch 121/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1032 - val_loss: 0.1057\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.10624 to 0.10568, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 122/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1013 - val_loss: 0.1061\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.10568\n",
            "Epoch 123/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1040 - val_loss: 0.1075\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.10568\n",
            "Epoch 124/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1037 - val_loss: 0.1048\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.10568 to 0.10478, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 125/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1031 - val_loss: 0.1047\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.10478 to 0.10467, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 126/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.1020 - val_loss: 0.1054\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.10467\n",
            "Epoch 127/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1010 - val_loss: 0.1064\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.10467\n",
            "Epoch 128/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1026 - val_loss: 0.1112\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.10467\n",
            "Epoch 129/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1038 - val_loss: 0.1064\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.10467\n",
            "Epoch 130/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1019 - val_loss: 0.1035\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.10467 to 0.10351, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 131/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1007 - val_loss: 0.1050\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.10351\n",
            "Epoch 132/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1020 - val_loss: 0.1049\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.10351\n",
            "Epoch 133/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1028 - val_loss: 0.1059\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.10351\n",
            "Epoch 134/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1021 - val_loss: 0.1049\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.10351\n",
            "Epoch 135/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1004 - val_loss: 0.1087\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.10351\n",
            "Epoch 136/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1018 - val_loss: 0.1031\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.10351 to 0.10308, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 137/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0998 - val_loss: 0.1052\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.10308\n",
            "Epoch 138/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1009 - val_loss: 0.1033\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.10308\n",
            "Epoch 139/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1008 - val_loss: 0.1027\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.10308 to 0.10270, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 140/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1015 - val_loss: 0.1039\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.10270\n",
            "Epoch 141/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1003 - val_loss: 0.1029\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.10270\n",
            "Epoch 142/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1004 - val_loss: 0.1045\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.10270\n",
            "Epoch 143/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1008 - val_loss: 0.1020\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.10270 to 0.10203, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 144/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0975 - val_loss: 0.1052\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.10203\n",
            "Epoch 145/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1005 - val_loss: 0.1056\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.10203\n",
            "Epoch 146/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0999 - val_loss: 0.1104\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.10203\n",
            "Epoch 147/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.1008 - val_loss: 0.1029\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.10203\n",
            "Epoch 148/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0992 - val_loss: 0.1031\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.10203\n",
            "Epoch 149/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0993 - val_loss: 0.1028\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.10203\n",
            "Epoch 150/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0985 - val_loss: 0.1019\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.10203 to 0.10193, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 151/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.1000 - val_loss: 0.1035\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.10193\n",
            "Epoch 152/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0976 - val_loss: 0.1039\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.10193\n",
            "Epoch 153/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0985 - val_loss: 0.1066\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.10193\n",
            "Epoch 154/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0989 - val_loss: 0.1009\n",
            "\n",
            "Epoch 00154: val_loss improved from 0.10193 to 0.10088, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 155/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0986 - val_loss: 0.1016\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.10088\n",
            "Epoch 156/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0971 - val_loss: 0.1024\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.10088\n",
            "Epoch 157/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0974 - val_loss: 0.1012\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.10088\n",
            "Epoch 158/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0967 - val_loss: 0.1008\n",
            "\n",
            "Epoch 00158: val_loss improved from 0.10088 to 0.10084, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 159/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0972 - val_loss: 0.1036\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.10084\n",
            "Epoch 160/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0975 - val_loss: 0.1046\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.10084\n",
            "Epoch 161/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0970 - val_loss: 0.0991\n",
            "\n",
            "Epoch 00161: val_loss improved from 0.10084 to 0.09908, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 162/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0967 - val_loss: 0.0986\n",
            "\n",
            "Epoch 00162: val_loss improved from 0.09908 to 0.09860, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 163/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0955 - val_loss: 0.0999\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.09860\n",
            "Epoch 164/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0956 - val_loss: 0.1007\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.09860\n",
            "Epoch 165/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0968 - val_loss: 0.1014\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.09860\n",
            "Epoch 166/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0966 - val_loss: 0.0987\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.09860\n",
            "Epoch 167/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0974 - val_loss: 0.0991\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.09860\n",
            "Epoch 168/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0950 - val_loss: 0.1027\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.09860\n",
            "Epoch 169/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0970 - val_loss: 0.0981\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.09860 to 0.09811, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 170/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0942 - val_loss: 0.1010\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.09811\n",
            "Epoch 171/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0950 - val_loss: 0.1007\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.09811\n",
            "Epoch 172/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0945 - val_loss: 0.0998\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.09811\n",
            "Epoch 173/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0957 - val_loss: 0.0982\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.09811\n",
            "Epoch 174/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0943 - val_loss: 0.0987\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.09811\n",
            "Epoch 175/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0940 - val_loss: 0.0997\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.09811\n",
            "Epoch 176/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0955 - val_loss: 0.1000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.09811\n",
            "Epoch 177/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0953 - val_loss: 0.0990\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.09811\n",
            "Epoch 178/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0939 - val_loss: 0.0971\n",
            "\n",
            "Epoch 00178: val_loss improved from 0.09811 to 0.09712, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 179/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0938 - val_loss: 0.0963\n",
            "\n",
            "Epoch 00179: val_loss improved from 0.09712 to 0.09628, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 180/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0926 - val_loss: 0.0989\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.09628\n",
            "Epoch 181/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0940 - val_loss: 0.1015\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.09628\n",
            "Epoch 182/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0938 - val_loss: 0.0978\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.09628\n",
            "Epoch 183/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0931 - val_loss: 0.0972\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.09628\n",
            "Epoch 184/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0907 - val_loss: 0.0983\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.09628\n",
            "Epoch 185/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0942 - val_loss: 0.0991\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.09628\n",
            "Epoch 186/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0936 - val_loss: 0.0991\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.09628\n",
            "Epoch 187/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0917 - val_loss: 0.0953\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.09628 to 0.09533, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 188/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0920 - val_loss: 0.1030\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.09533\n",
            "Epoch 189/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0932 - val_loss: 0.0967\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.09533\n",
            "Epoch 190/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0927 - val_loss: 0.0982\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.09533\n",
            "Epoch 191/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0920 - val_loss: 0.0974\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.09533\n",
            "Epoch 192/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0918 - val_loss: 0.0957\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.09533\n",
            "Epoch 193/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0911 - val_loss: 0.1059\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.09533\n",
            "Epoch 194/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0917 - val_loss: 0.0964\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.09533\n",
            "Epoch 195/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0922 - val_loss: 0.0961\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.09533\n",
            "Epoch 196/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0897 - val_loss: 0.0953\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.09533\n",
            "Epoch 197/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0922 - val_loss: 0.0958\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.09533\n",
            "Epoch 198/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0911 - val_loss: 0.0962\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.09533\n",
            "Epoch 199/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0907 - val_loss: 0.0952\n",
            "\n",
            "Epoch 00199: val_loss improved from 0.09533 to 0.09520, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 200/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0901 - val_loss: 0.0972\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.09520\n",
            "Epoch 201/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0890 - val_loss: 0.0977\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.09520\n",
            "Epoch 202/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0915 - val_loss: 0.0953\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.09520\n",
            "Epoch 203/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0902 - val_loss: 0.0973\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.09520\n",
            "Epoch 204/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0881 - val_loss: 0.0945\n",
            "\n",
            "Epoch 00204: val_loss improved from 0.09520 to 0.09453, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 205/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0891 - val_loss: 0.0965\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.09453\n",
            "Epoch 206/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0910 - val_loss: 0.0956\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.09453\n",
            "Epoch 207/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0907 - val_loss: 0.0950\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.09453\n",
            "Epoch 208/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0892 - val_loss: 0.0933\n",
            "\n",
            "Epoch 00208: val_loss improved from 0.09453 to 0.09331, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 209/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0885 - val_loss: 0.0942\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.09331\n",
            "Epoch 210/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0888 - val_loss: 0.0971\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.09331\n",
            "Epoch 211/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0898 - val_loss: 0.0968\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.09331\n",
            "Epoch 212/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0898 - val_loss: 0.0938\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.09331\n",
            "Epoch 213/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0891 - val_loss: 0.0968\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.09331\n",
            "Epoch 214/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0887 - val_loss: 0.0946\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.09331\n",
            "Epoch 215/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0892 - val_loss: 0.0933\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.09331\n",
            "Epoch 216/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0884 - val_loss: 0.0940\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.09331\n",
            "Epoch 217/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0878 - val_loss: 0.0935\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.09331\n",
            "Epoch 218/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0889 - val_loss: 0.0948\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.09331\n",
            "Epoch 219/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0890 - val_loss: 0.0966\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.09331\n",
            "Epoch 220/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0890 - val_loss: 0.0944\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.09331\n",
            "Epoch 221/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0886 - val_loss: 0.0937\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.09331\n",
            "Epoch 222/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0880 - val_loss: 0.0947\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.09331\n",
            "Epoch 223/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0864 - val_loss: 0.0933\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.09331\n",
            "Epoch 224/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0877 - val_loss: 0.0929\n",
            "\n",
            "Epoch 00224: val_loss improved from 0.09331 to 0.09290, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 225/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0885 - val_loss: 0.0924\n",
            "\n",
            "Epoch 00225: val_loss improved from 0.09290 to 0.09243, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 226/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0878 - val_loss: 0.0941\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.09243\n",
            "Epoch 227/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0871 - val_loss: 0.0929\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.09243\n",
            "Epoch 228/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0865 - val_loss: 0.0939\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.09243\n",
            "Epoch 229/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0876 - val_loss: 0.0925\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.09243\n",
            "Epoch 230/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0874 - val_loss: 0.0944\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.09243\n",
            "Epoch 231/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0881 - val_loss: 0.0922\n",
            "\n",
            "Epoch 00231: val_loss improved from 0.09243 to 0.09217, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 232/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0857 - val_loss: 0.0956\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.09217\n",
            "Epoch 233/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0873 - val_loss: 0.0924\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.09217\n",
            "Epoch 234/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0858 - val_loss: 0.0974\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.09217\n",
            "Epoch 235/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0884 - val_loss: 0.0935\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.09217\n",
            "Epoch 236/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0844 - val_loss: 0.0923\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.09217\n",
            "Epoch 237/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0870 - val_loss: 0.0946\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.09217\n",
            "Epoch 238/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0874 - val_loss: 0.0928\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.09217\n",
            "Epoch 239/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0864 - val_loss: 0.0938\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.09217\n",
            "Epoch 240/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0861 - val_loss: 0.0922\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.09217\n",
            "Epoch 241/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0861 - val_loss: 0.0940\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.09217\n",
            "Epoch 242/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0860 - val_loss: 0.0915\n",
            "\n",
            "Epoch 00242: val_loss improved from 0.09217 to 0.09151, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 243/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0840 - val_loss: 0.0913\n",
            "\n",
            "Epoch 00243: val_loss improved from 0.09151 to 0.09128, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 244/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0849 - val_loss: 0.0946\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.09128\n",
            "Epoch 245/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0860 - val_loss: 0.0994\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.09128\n",
            "Epoch 246/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0861 - val_loss: 0.0938\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.09128\n",
            "Epoch 247/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0855 - val_loss: 0.0946\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.09128\n",
            "Epoch 248/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0862 - val_loss: 0.0950\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.09128\n",
            "Epoch 249/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0868 - val_loss: 0.0975\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.09128\n",
            "Epoch 250/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0858 - val_loss: 0.0923\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.09128\n",
            "Epoch 251/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0858 - val_loss: 0.0913\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.09128\n",
            "Epoch 252/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0843 - val_loss: 0.0917\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.09128\n",
            "Epoch 253/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0854 - val_loss: 0.0909\n",
            "\n",
            "Epoch 00253: val_loss improved from 0.09128 to 0.09088, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 254/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0842 - val_loss: 0.0927\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.09088\n",
            "Epoch 255/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0843 - val_loss: 0.0921\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.09088\n",
            "Epoch 256/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0854 - val_loss: 0.0921\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.09088\n",
            "Epoch 257/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0838 - val_loss: 0.0911\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.09088\n",
            "Epoch 258/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0848 - val_loss: 0.0912\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.09088\n",
            "Epoch 259/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0831 - val_loss: 0.0927\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.09088\n",
            "Epoch 260/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0843 - val_loss: 0.0975\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.09088\n",
            "Epoch 261/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0844 - val_loss: 0.0919\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.09088\n",
            "Epoch 262/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0852 - val_loss: 0.0921\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.09088\n",
            "Epoch 263/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0852 - val_loss: 0.0919\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.09088\n",
            "Epoch 264/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0834 - val_loss: 0.0935\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.09088\n",
            "Epoch 265/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0846 - val_loss: 0.0929\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.09088\n",
            "Epoch 266/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0838 - val_loss: 0.0910\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.09088\n",
            "Epoch 267/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0843 - val_loss: 0.0951\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.09088\n",
            "Epoch 268/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0836 - val_loss: 0.0915\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.09088\n",
            "Epoch 269/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0838 - val_loss: 0.0894\n",
            "\n",
            "Epoch 00269: val_loss improved from 0.09088 to 0.08941, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 270/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0828 - val_loss: 0.0916\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.08941\n",
            "Epoch 271/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0841 - val_loss: 0.0902\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.08941\n",
            "Epoch 272/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0837 - val_loss: 0.0901\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.08941\n",
            "Epoch 273/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0827 - val_loss: 0.0902\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.08941\n",
            "Epoch 274/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0841 - val_loss: 0.0911\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.08941\n",
            "Epoch 275/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0826 - val_loss: 0.0901\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.08941\n",
            "Epoch 276/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0825 - val_loss: 0.0897\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.08941\n",
            "Epoch 277/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0823 - val_loss: 0.0912\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.08941\n",
            "Epoch 278/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0827 - val_loss: 0.0906\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.08941\n",
            "Epoch 279/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0825 - val_loss: 0.0896\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.08941\n",
            "Epoch 280/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0814 - val_loss: 0.0913\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.08941\n",
            "Epoch 281/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0836 - val_loss: 0.0902\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.08941\n",
            "Epoch 282/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0834 - val_loss: 0.0893\n",
            "\n",
            "Epoch 00282: val_loss improved from 0.08941 to 0.08933, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 283/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0820 - val_loss: 0.0890\n",
            "\n",
            "Epoch 00283: val_loss improved from 0.08933 to 0.08904, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 284/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0819 - val_loss: 0.0922\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.08904\n",
            "Epoch 285/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0832 - val_loss: 0.0890\n",
            "\n",
            "Epoch 00285: val_loss improved from 0.08904 to 0.08903, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 286/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0831 - val_loss: 0.0890\n",
            "\n",
            "Epoch 00286: val_loss improved from 0.08903 to 0.08901, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 287/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0800 - val_loss: 0.0897\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.08901\n",
            "Epoch 288/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0806 - val_loss: 0.0899\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.08901\n",
            "Epoch 289/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0829 - val_loss: 0.0919\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.08901\n",
            "Epoch 290/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0832 - val_loss: 0.0906\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.08901\n",
            "Epoch 291/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0822 - val_loss: 0.0888\n",
            "\n",
            "Epoch 00291: val_loss improved from 0.08901 to 0.08879, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 292/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0810 - val_loss: 0.0889\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.08879\n",
            "Epoch 293/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0816 - val_loss: 0.0896\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.08879\n",
            "Epoch 294/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0823 - val_loss: 0.0892\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.08879\n",
            "Epoch 295/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0820 - val_loss: 0.0892\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.08879\n",
            "Epoch 296/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0817 - val_loss: 0.0904\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.08879\n",
            "Epoch 297/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0823 - val_loss: 0.0890\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.08879\n",
            "Epoch 298/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0807 - val_loss: 0.0898\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.08879\n",
            "Epoch 299/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0813 - val_loss: 0.0896\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.08879\n",
            "Epoch 300/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0814 - val_loss: 0.0936\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.08879\n",
            "Epoch 301/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0817 - val_loss: 0.0914\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.08879\n",
            "Epoch 302/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0813 - val_loss: 0.0912\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.08879\n",
            "Epoch 303/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0815 - val_loss: 0.0896\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.08879\n",
            "Epoch 304/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0810 - val_loss: 0.0901\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.08879\n",
            "Epoch 305/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0803 - val_loss: 0.0905\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.08879\n",
            "Epoch 306/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0806 - val_loss: 0.0918\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.08879\n",
            "Epoch 307/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0813 - val_loss: 0.0912\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.08879\n",
            "Epoch 308/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0806 - val_loss: 0.0904\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.08879\n",
            "Epoch 309/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0812 - val_loss: 0.0884\n",
            "\n",
            "Epoch 00309: val_loss improved from 0.08879 to 0.08835, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 310/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0808 - val_loss: 0.0882\n",
            "\n",
            "Epoch 00310: val_loss improved from 0.08835 to 0.08820, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 311/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0793 - val_loss: 0.0885\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.08820\n",
            "Epoch 312/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0809 - val_loss: 0.0912\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.08820\n",
            "Epoch 313/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0809 - val_loss: 0.0904\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.08820\n",
            "Epoch 314/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0812 - val_loss: 0.0902\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.08820\n",
            "Epoch 315/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0805 - val_loss: 0.0885\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.08820\n",
            "Epoch 316/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0794 - val_loss: 0.0881\n",
            "\n",
            "Epoch 00316: val_loss improved from 0.08820 to 0.08808, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 317/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0802 - val_loss: 0.0888\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.08808\n",
            "Epoch 318/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0801 - val_loss: 0.0918\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.08808\n",
            "Epoch 319/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0803 - val_loss: 0.0933\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.08808\n",
            "Epoch 320/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0804 - val_loss: 0.0902\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.08808\n",
            "Epoch 321/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0800 - val_loss: 0.0890\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.08808\n",
            "Epoch 322/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0796 - val_loss: 0.0878\n",
            "\n",
            "Epoch 00322: val_loss improved from 0.08808 to 0.08779, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 323/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0791 - val_loss: 0.0903\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.08779\n",
            "Epoch 324/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0803 - val_loss: 0.0914\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.08779\n",
            "Epoch 325/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0807 - val_loss: 0.0878\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.08779\n",
            "Epoch 326/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0784 - val_loss: 0.0885\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.08779\n",
            "Epoch 327/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0799 - val_loss: 0.0882\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.08779\n",
            "Epoch 328/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0794 - val_loss: 0.0890\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.08779\n",
            "Epoch 329/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0793 - val_loss: 0.0888\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.08779\n",
            "Epoch 330/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0775 - val_loss: 0.0880\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.08779\n",
            "Epoch 331/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0796 - val_loss: 0.0912\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.08779\n",
            "Epoch 332/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0802 - val_loss: 0.0896\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.08779\n",
            "Epoch 333/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0806 - val_loss: 0.0882\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.08779\n",
            "Epoch 334/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0782 - val_loss: 0.0897\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.08779\n",
            "Epoch 335/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0790 - val_loss: 0.0875\n",
            "\n",
            "Epoch 00335: val_loss improved from 0.08779 to 0.08755, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 336/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0790 - val_loss: 0.0910\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.08755\n",
            "Epoch 337/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0802 - val_loss: 0.0900\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.08755\n",
            "Epoch 338/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0795 - val_loss: 0.0877\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.08755\n",
            "Epoch 339/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0792 - val_loss: 0.0875\n",
            "\n",
            "Epoch 00339: val_loss improved from 0.08755 to 0.08750, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 340/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0782 - val_loss: 0.0935\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.08750\n",
            "Epoch 341/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0801 - val_loss: 0.0890\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.08750\n",
            "Epoch 342/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0788 - val_loss: 0.0890\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.08750\n",
            "Epoch 343/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0785 - val_loss: 0.0878\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.08750\n",
            "Epoch 344/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0786 - val_loss: 0.0922\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.08750\n",
            "Epoch 345/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0791 - val_loss: 0.0892\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.08750\n",
            "Epoch 346/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0788 - val_loss: 0.0898\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.08750\n",
            "Epoch 347/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0794 - val_loss: 0.0873\n",
            "\n",
            "Epoch 00347: val_loss improved from 0.08750 to 0.08735, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 348/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0784 - val_loss: 0.0891\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.08735\n",
            "Epoch 349/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0770 - val_loss: 0.0874\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.08735\n",
            "Epoch 350/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0782 - val_loss: 0.0893\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.08735\n",
            "Epoch 351/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0782 - val_loss: 0.0889\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.08735\n",
            "Epoch 352/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0789 - val_loss: 0.0872\n",
            "\n",
            "Epoch 00352: val_loss improved from 0.08735 to 0.08716, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 353/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0765 - val_loss: 0.0894\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.08716\n",
            "Epoch 354/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0789 - val_loss: 0.0885\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.08716\n",
            "Epoch 355/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0792 - val_loss: 0.0887\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.08716\n",
            "Epoch 356/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0782 - val_loss: 0.0898\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.08716\n",
            "Epoch 357/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0780 - val_loss: 0.0891\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.08716\n",
            "Epoch 358/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0778 - val_loss: 0.0880\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.08716\n",
            "Epoch 359/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0790 - val_loss: 0.0883\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.08716\n",
            "Epoch 360/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0780 - val_loss: 0.0887\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.08716\n",
            "Epoch 361/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0780 - val_loss: 0.0942\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.08716\n",
            "Epoch 362/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0775 - val_loss: 0.0873\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.08716\n",
            "Epoch 363/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0762 - val_loss: 0.0878\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.08716\n",
            "Epoch 364/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0775 - val_loss: 0.0895\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.08716\n",
            "Epoch 365/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0779 - val_loss: 0.0893\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.08716\n",
            "Epoch 366/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0783 - val_loss: 0.0934\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.08716\n",
            "Epoch 367/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0772 - val_loss: 0.0887\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.08716\n",
            "Epoch 368/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0784 - val_loss: 0.0880\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.08716\n",
            "Epoch 369/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0776 - val_loss: 0.0875\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.08716\n",
            "Epoch 370/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0777 - val_loss: 0.0893\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.08716\n",
            "Epoch 371/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0777 - val_loss: 0.0884\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.08716\n",
            "Epoch 372/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0783 - val_loss: 0.0882\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.08716\n",
            "Epoch 373/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0772 - val_loss: 0.0874\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.08716\n",
            "Epoch 374/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0772 - val_loss: 0.0876\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.08716\n",
            "Epoch 375/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0771 - val_loss: 0.0878\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.08716\n",
            "Epoch 376/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0768 - val_loss: 0.0879\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.08716\n",
            "Epoch 377/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0754 - val_loss: 0.0879\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.08716\n",
            "Epoch 378/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0766 - val_loss: 0.0879\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.08716\n",
            "Epoch 379/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0774 - val_loss: 0.0866\n",
            "\n",
            "Epoch 00379: val_loss improved from 0.08716 to 0.08659, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 380/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0758 - val_loss: 0.0873\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.08659\n",
            "Epoch 381/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0771 - val_loss: 0.0872\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.08659\n",
            "Epoch 382/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0758 - val_loss: 0.0871\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.08659\n",
            "Epoch 383/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0764 - val_loss: 0.0898\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.08659\n",
            "Epoch 384/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0760 - val_loss: 0.0907\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.08659\n",
            "Epoch 385/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0779 - val_loss: 0.0875\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.08659\n",
            "Epoch 386/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0765 - val_loss: 0.0873\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.08659\n",
            "Epoch 387/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0763 - val_loss: 0.0867\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.08659\n",
            "Epoch 388/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0744 - val_loss: 0.0870\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.08659\n",
            "Epoch 389/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0769 - val_loss: 0.0883\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.08659\n",
            "Epoch 390/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0763 - val_loss: 0.0873\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.08659\n",
            "Epoch 391/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0763 - val_loss: 0.0883\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.08659\n",
            "Epoch 392/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0769 - val_loss: 0.0888\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.08659\n",
            "Epoch 393/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0768 - val_loss: 0.0875\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.08659\n",
            "Epoch 394/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0768 - val_loss: 0.0874\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.08659\n",
            "Epoch 395/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0755 - val_loss: 0.0874\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.08659\n",
            "Epoch 396/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0765 - val_loss: 0.0891\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.08659\n",
            "Epoch 397/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0753 - val_loss: 0.0856\n",
            "\n",
            "Epoch 00397: val_loss improved from 0.08659 to 0.08558, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 398/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0749 - val_loss: 0.0859\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.08558\n",
            "Epoch 399/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0753 - val_loss: 0.0908\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.08558\n",
            "Epoch 400/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0749 - val_loss: 0.0858\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.08558\n",
            "Epoch 401/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0741 - val_loss: 0.0883\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.08558\n",
            "Epoch 402/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0747 - val_loss: 0.0901\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.08558\n",
            "Epoch 403/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0776 - val_loss: 0.0858\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.08558\n",
            "Epoch 404/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0754 - val_loss: 0.0864\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.08558\n",
            "Epoch 405/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0753 - val_loss: 0.0871\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.08558\n",
            "Epoch 406/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0748 - val_loss: 0.0911\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.08558\n",
            "Epoch 407/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0756 - val_loss: 0.0860\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.08558\n",
            "Epoch 408/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0756 - val_loss: 0.0857\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.08558\n",
            "Epoch 409/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0742 - val_loss: 0.0891\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.08558\n",
            "Epoch 410/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0762 - val_loss: 0.0861\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.08558\n",
            "Epoch 411/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0753 - val_loss: 0.0856\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.08558\n",
            "Epoch 412/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0745 - val_loss: 0.0898\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.08558\n",
            "Epoch 413/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0748 - val_loss: 0.0861\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.08558\n",
            "Epoch 414/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0733 - val_loss: 0.0864\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.08558\n",
            "Epoch 415/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0756 - val_loss: 0.0859\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.08558\n",
            "Epoch 416/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0749 - val_loss: 0.0861\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.08558\n",
            "Epoch 417/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0746 - val_loss: 0.0894\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.08558\n",
            "Epoch 418/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0752 - val_loss: 0.0875\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.08558\n",
            "Epoch 419/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0754 - val_loss: 0.0913\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.08558\n",
            "Epoch 420/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0760 - val_loss: 0.0854\n",
            "\n",
            "Epoch 00420: val_loss improved from 0.08558 to 0.08543, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 421/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0735 - val_loss: 0.0867\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.08543\n",
            "Epoch 422/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0752 - val_loss: 0.0860\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.08543\n",
            "Epoch 423/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0746 - val_loss: 0.0895\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.08543\n",
            "Epoch 424/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0742 - val_loss: 0.0853\n",
            "\n",
            "Epoch 00424: val_loss improved from 0.08543 to 0.08527, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 425/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0742 - val_loss: 0.0874\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.08527\n",
            "Epoch 426/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0738 - val_loss: 0.0855\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.08527\n",
            "Epoch 427/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0730 - val_loss: 0.0857\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.08527\n",
            "Epoch 428/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0741 - val_loss: 0.0851\n",
            "\n",
            "Epoch 00428: val_loss improved from 0.08527 to 0.08513, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 429/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0746 - val_loss: 0.0855\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.08513\n",
            "Epoch 430/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0738 - val_loss: 0.0855\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.08513\n",
            "Epoch 431/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0744 - val_loss: 0.0883\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.08513\n",
            "Epoch 432/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0732 - val_loss: 0.0865\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.08513\n",
            "Epoch 433/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0747 - val_loss: 0.0861\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.08513\n",
            "Epoch 434/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0724 - val_loss: 0.0858\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.08513\n",
            "Epoch 435/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0726 - val_loss: 0.0878\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.08513\n",
            "Epoch 436/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0748 - val_loss: 0.0859\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.08513\n",
            "Epoch 437/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0747 - val_loss: 0.0855\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.08513\n",
            "Epoch 438/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0717 - val_loss: 0.0869\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.08513\n",
            "Epoch 439/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0748 - val_loss: 0.0857\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.08513\n",
            "Epoch 440/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0715 - val_loss: 0.0853\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.08513\n",
            "Epoch 441/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0721 - val_loss: 0.0864\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.08513\n",
            "Epoch 442/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0750 - val_loss: 0.0871\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.08513\n",
            "Epoch 443/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0730 - val_loss: 0.0859\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.08513\n",
            "Epoch 444/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0738 - val_loss: 0.0886\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.08513\n",
            "Epoch 445/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0724 - val_loss: 0.0864\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.08513\n",
            "Epoch 446/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0747 - val_loss: 0.0846\n",
            "\n",
            "Epoch 00446: val_loss improved from 0.08513 to 0.08463, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 447/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0726 - val_loss: 0.0851\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.08463\n",
            "Epoch 448/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0738 - val_loss: 0.0856\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.08463\n",
            "Epoch 449/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0725 - val_loss: 0.0852\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.08463\n",
            "Epoch 450/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0719 - val_loss: 0.0861\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.08463\n",
            "Epoch 451/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0745 - val_loss: 0.0849\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.08463\n",
            "Epoch 452/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0728 - val_loss: 0.0858\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.08463\n",
            "Epoch 453/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0740 - val_loss: 0.0849\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.08463\n",
            "Epoch 454/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0723 - val_loss: 0.0851\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.08463\n",
            "Epoch 455/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0731 - val_loss: 0.0855\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.08463\n",
            "Epoch 456/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0734 - val_loss: 0.0859\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.08463\n",
            "Epoch 457/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0730 - val_loss: 0.0849\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.08463\n",
            "Epoch 458/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0720 - val_loss: 0.0859\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.08463\n",
            "Epoch 459/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0728 - val_loss: 0.0846\n",
            "\n",
            "Epoch 00459: val_loss improved from 0.08463 to 0.08458, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 460/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0710 - val_loss: 0.0861\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.08458\n",
            "Epoch 461/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0729 - val_loss: 0.0844\n",
            "\n",
            "Epoch 00461: val_loss improved from 0.08458 to 0.08445, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 462/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0723 - val_loss: 0.0845\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.08445\n",
            "Epoch 463/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0711 - val_loss: 0.0863\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.08445\n",
            "Epoch 464/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0731 - val_loss: 0.0848\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.08445\n",
            "Epoch 465/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0726 - val_loss: 0.0854\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.08445\n",
            "Epoch 466/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0723 - val_loss: 0.0857\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.08445\n",
            "Epoch 467/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0719 - val_loss: 0.0859\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.08445\n",
            "Epoch 468/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0722 - val_loss: 0.0847\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.08445\n",
            "Epoch 469/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0726 - val_loss: 0.0846\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.08445\n",
            "Epoch 470/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0720 - val_loss: 0.0845\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.08445\n",
            "Epoch 471/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0705 - val_loss: 0.0855\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.08445\n",
            "Epoch 472/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0730 - val_loss: 0.0856\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.08445\n",
            "Epoch 473/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0730 - val_loss: 0.0850\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.08445\n",
            "Epoch 474/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0720 - val_loss: 0.0854\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.08445\n",
            "Epoch 475/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0710 - val_loss: 0.0838\n",
            "\n",
            "Epoch 00475: val_loss improved from 0.08445 to 0.08385, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 476/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0718 - val_loss: 0.0849\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.08385\n",
            "Epoch 477/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0709 - val_loss: 0.0850\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.08385\n",
            "Epoch 478/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0711 - val_loss: 0.0879\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.08385\n",
            "Epoch 479/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0730 - val_loss: 0.0857\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.08385\n",
            "Epoch 480/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0718 - val_loss: 0.0886\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.08385\n",
            "Epoch 481/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0723 - val_loss: 0.0853\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.08385\n",
            "Epoch 482/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0717 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00482: val_loss improved from 0.08385 to 0.08336, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 483/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0704 - val_loss: 0.0849\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.08336\n",
            "Epoch 484/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0709 - val_loss: 0.0860\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.08336\n",
            "Epoch 485/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0711 - val_loss: 0.0858\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.08336\n",
            "Epoch 486/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0704 - val_loss: 0.0840\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.08336\n",
            "Epoch 487/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0711 - val_loss: 0.0836\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.08336\n",
            "Epoch 488/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0691 - val_loss: 0.0835\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.08336\n",
            "Epoch 489/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0716 - val_loss: 0.0874\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.08336\n",
            "Epoch 490/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0720 - val_loss: 0.0861\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.08336\n",
            "Epoch 491/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0716 - val_loss: 0.0851\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.08336\n",
            "Epoch 492/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0714 - val_loss: 0.0837\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.08336\n",
            "Epoch 493/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0703 - val_loss: 0.0862\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.08336\n",
            "Epoch 494/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0717 - val_loss: 0.0837\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.08336\n",
            "Epoch 495/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0697 - val_loss: 0.0869\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.08336\n",
            "Epoch 496/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0717 - val_loss: 0.0853\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.08336\n",
            "Epoch 497/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0710 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.08336\n",
            "Epoch 498/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0696 - val_loss: 0.0852\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.08336\n",
            "Epoch 499/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0712 - val_loss: 0.0864\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.08336\n",
            "Epoch 500/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0718 - val_loss: 0.0844\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.08336\n",
            "Epoch 501/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0712 - val_loss: 0.0850\n",
            "\n",
            "Epoch 00501: val_loss did not improve from 0.08336\n",
            "Epoch 502/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0707 - val_loss: 0.0837\n",
            "\n",
            "Epoch 00502: val_loss did not improve from 0.08336\n",
            "Epoch 503/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0710 - val_loss: 0.0861\n",
            "\n",
            "Epoch 00503: val_loss did not improve from 0.08336\n",
            "Epoch 504/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0704 - val_loss: 0.0835\n",
            "\n",
            "Epoch 00504: val_loss did not improve from 0.08336\n",
            "Epoch 505/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0708 - val_loss: 0.0836\n",
            "\n",
            "Epoch 00505: val_loss did not improve from 0.08336\n",
            "Epoch 506/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0691 - val_loss: 0.0849\n",
            "\n",
            "Epoch 00506: val_loss did not improve from 0.08336\n",
            "Epoch 507/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0714 - val_loss: 0.0854\n",
            "\n",
            "Epoch 00507: val_loss did not improve from 0.08336\n",
            "Epoch 508/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0698 - val_loss: 0.0847\n",
            "\n",
            "Epoch 00508: val_loss did not improve from 0.08336\n",
            "Epoch 509/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0709 - val_loss: 0.0832\n",
            "\n",
            "Epoch 00509: val_loss improved from 0.08336 to 0.08323, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 510/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0695 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00510: val_loss improved from 0.08323 to 0.08292, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 511/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0687 - val_loss: 0.0836\n",
            "\n",
            "Epoch 00511: val_loss did not improve from 0.08292\n",
            "Epoch 512/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0707 - val_loss: 0.0850\n",
            "\n",
            "Epoch 00512: val_loss did not improve from 0.08292\n",
            "Epoch 513/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0704 - val_loss: 0.0827\n",
            "\n",
            "Epoch 00513: val_loss improved from 0.08292 to 0.08274, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 514/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0680 - val_loss: 0.0835\n",
            "\n",
            "Epoch 00514: val_loss did not improve from 0.08274\n",
            "Epoch 515/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0706 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00515: val_loss did not improve from 0.08274\n",
            "Epoch 516/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0689 - val_loss: 0.0862\n",
            "\n",
            "Epoch 00516: val_loss did not improve from 0.08274\n",
            "Epoch 517/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0702 - val_loss: 0.0840\n",
            "\n",
            "Epoch 00517: val_loss did not improve from 0.08274\n",
            "Epoch 518/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0697 - val_loss: 0.0839\n",
            "\n",
            "Epoch 00518: val_loss did not improve from 0.08274\n",
            "Epoch 519/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0701 - val_loss: 0.0853\n",
            "\n",
            "Epoch 00519: val_loss did not improve from 0.08274\n",
            "Epoch 520/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0712 - val_loss: 0.0840\n",
            "\n",
            "Epoch 00520: val_loss did not improve from 0.08274\n",
            "Epoch 521/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0700 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00521: val_loss did not improve from 0.08274\n",
            "Epoch 522/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0697 - val_loss: 0.0882\n",
            "\n",
            "Epoch 00522: val_loss did not improve from 0.08274\n",
            "Epoch 523/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0712 - val_loss: 0.0827\n",
            "\n",
            "Epoch 00523: val_loss improved from 0.08274 to 0.08272, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 524/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0696 - val_loss: 0.0847\n",
            "\n",
            "Epoch 00524: val_loss did not improve from 0.08272\n",
            "Epoch 525/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0691 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00525: val_loss did not improve from 0.08272\n",
            "Epoch 526/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0694 - val_loss: 0.0840\n",
            "\n",
            "Epoch 00526: val_loss did not improve from 0.08272\n",
            "Epoch 527/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0691 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00527: val_loss did not improve from 0.08272\n",
            "Epoch 528/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0687 - val_loss: 0.0866\n",
            "\n",
            "Epoch 00528: val_loss did not improve from 0.08272\n",
            "Epoch 529/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0703 - val_loss: 0.0844\n",
            "\n",
            "Epoch 00529: val_loss did not improve from 0.08272\n",
            "Epoch 530/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0699 - val_loss: 0.0886\n",
            "\n",
            "Epoch 00530: val_loss did not improve from 0.08272\n",
            "Epoch 531/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0700 - val_loss: 0.0844\n",
            "\n",
            "Epoch 00531: val_loss did not improve from 0.08272\n",
            "Epoch 532/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0697 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00532: val_loss did not improve from 0.08272\n",
            "Epoch 533/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0682 - val_loss: 0.0839\n",
            "\n",
            "Epoch 00533: val_loss did not improve from 0.08272\n",
            "Epoch 534/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0695 - val_loss: 0.0847\n",
            "\n",
            "Epoch 00534: val_loss did not improve from 0.08272\n",
            "Epoch 535/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0680 - val_loss: 0.0836\n",
            "\n",
            "Epoch 00535: val_loss did not improve from 0.08272\n",
            "Epoch 536/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0697 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00536: val_loss did not improve from 0.08272\n",
            "Epoch 537/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0674 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00537: val_loss improved from 0.08272 to 0.08248, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 538/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0684 - val_loss: 0.0845\n",
            "\n",
            "Epoch 00538: val_loss did not improve from 0.08248\n",
            "Epoch 539/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0693 - val_loss: 0.0826\n",
            "\n",
            "Epoch 00539: val_loss did not improve from 0.08248\n",
            "Epoch 540/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0677 - val_loss: 0.0852\n",
            "\n",
            "Epoch 00540: val_loss did not improve from 0.08248\n",
            "Epoch 541/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0697 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00541: val_loss did not improve from 0.08248\n",
            "Epoch 542/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0674 - val_loss: 0.0837\n",
            "\n",
            "Epoch 00542: val_loss did not improve from 0.08248\n",
            "Epoch 543/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0693 - val_loss: 0.0848\n",
            "\n",
            "Epoch 00543: val_loss did not improve from 0.08248\n",
            "Epoch 544/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0695 - val_loss: 0.0832\n",
            "\n",
            "Epoch 00544: val_loss did not improve from 0.08248\n",
            "Epoch 545/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0682 - val_loss: 0.0846\n",
            "\n",
            "Epoch 00545: val_loss did not improve from 0.08248\n",
            "Epoch 546/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0705 - val_loss: 0.0830\n",
            "\n",
            "Epoch 00546: val_loss did not improve from 0.08248\n",
            "Epoch 547/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0683 - val_loss: 0.0848\n",
            "\n",
            "Epoch 00547: val_loss did not improve from 0.08248\n",
            "Epoch 548/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0675 - val_loss: 0.0828\n",
            "\n",
            "Epoch 00548: val_loss did not improve from 0.08248\n",
            "Epoch 549/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0691 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00549: val_loss did not improve from 0.08248\n",
            "Epoch 550/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0669 - val_loss: 0.0840\n",
            "\n",
            "Epoch 00550: val_loss did not improve from 0.08248\n",
            "Epoch 551/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0686 - val_loss: 0.0832\n",
            "\n",
            "Epoch 00551: val_loss did not improve from 0.08248\n",
            "Epoch 552/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0688 - val_loss: 0.0828\n",
            "\n",
            "Epoch 00552: val_loss did not improve from 0.08248\n",
            "Epoch 553/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0688 - val_loss: 0.0849\n",
            "\n",
            "Epoch 00553: val_loss did not improve from 0.08248\n",
            "Epoch 554/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0691 - val_loss: 0.0847\n",
            "\n",
            "Epoch 00554: val_loss did not improve from 0.08248\n",
            "Epoch 555/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0676 - val_loss: 0.0849\n",
            "\n",
            "Epoch 00555: val_loss did not improve from 0.08248\n",
            "Epoch 556/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0696 - val_loss: 0.0838\n",
            "\n",
            "Epoch 00556: val_loss did not improve from 0.08248\n",
            "Epoch 557/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0685 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00557: val_loss did not improve from 0.08248\n",
            "Epoch 558/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0681 - val_loss: 0.0863\n",
            "\n",
            "Epoch 00558: val_loss did not improve from 0.08248\n",
            "Epoch 559/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0689 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00559: val_loss improved from 0.08248 to 0.08218, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 560/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0669 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00560: val_loss did not improve from 0.08218\n",
            "Epoch 561/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0667 - val_loss: 0.0840\n",
            "\n",
            "Epoch 00561: val_loss did not improve from 0.08218\n",
            "Epoch 562/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0685 - val_loss: 0.0830\n",
            "\n",
            "Epoch 00562: val_loss did not improve from 0.08218\n",
            "Epoch 563/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0683 - val_loss: 0.0823\n",
            "\n",
            "Epoch 00563: val_loss did not improve from 0.08218\n",
            "Epoch 564/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0666 - val_loss: 0.0823\n",
            "\n",
            "Epoch 00564: val_loss did not improve from 0.08218\n",
            "Epoch 565/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0679 - val_loss: 0.0833\n",
            "\n",
            "Epoch 00565: val_loss did not improve from 0.08218\n",
            "Epoch 566/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0682 - val_loss: 0.0888\n",
            "\n",
            "Epoch 00566: val_loss did not improve from 0.08218\n",
            "Epoch 567/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0679 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00567: val_loss improved from 0.08218 to 0.08211, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 568/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0684 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00568: val_loss did not improve from 0.08211\n",
            "Epoch 569/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0678 - val_loss: 0.0838\n",
            "\n",
            "Epoch 00569: val_loss did not improve from 0.08211\n",
            "Epoch 570/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0677 - val_loss: 0.0850\n",
            "\n",
            "Epoch 00570: val_loss did not improve from 0.08211\n",
            "Epoch 571/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0686 - val_loss: 0.0835\n",
            "\n",
            "Epoch 00571: val_loss did not improve from 0.08211\n",
            "Epoch 572/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0669 - val_loss: 0.0839\n",
            "\n",
            "Epoch 00572: val_loss did not improve from 0.08211\n",
            "Epoch 573/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0679 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00573: val_loss did not improve from 0.08211\n",
            "Epoch 574/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0680 - val_loss: 0.0835\n",
            "\n",
            "Epoch 00574: val_loss did not improve from 0.08211\n",
            "Epoch 575/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0664 - val_loss: 0.0830\n",
            "\n",
            "Epoch 00575: val_loss did not improve from 0.08211\n",
            "Epoch 576/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0683 - val_loss: 0.0839\n",
            "\n",
            "Epoch 00576: val_loss did not improve from 0.08211\n",
            "Epoch 577/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0675 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00577: val_loss did not improve from 0.08211\n",
            "Epoch 578/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0670 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00578: val_loss did not improve from 0.08211\n",
            "Epoch 579/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0661 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00579: val_loss improved from 0.08211 to 0.08175, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 580/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0673 - val_loss: 0.0846\n",
            "\n",
            "Epoch 00580: val_loss did not improve from 0.08175\n",
            "Epoch 581/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0682 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00581: val_loss did not improve from 0.08175\n",
            "Epoch 582/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0680 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00582: val_loss did not improve from 0.08175\n",
            "Epoch 583/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0669 - val_loss: 0.0820\n",
            "\n",
            "Epoch 00583: val_loss did not improve from 0.08175\n",
            "Epoch 584/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0666 - val_loss: 0.0846\n",
            "\n",
            "Epoch 00584: val_loss did not improve from 0.08175\n",
            "Epoch 585/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0669 - val_loss: 0.0827\n",
            "\n",
            "Epoch 00585: val_loss did not improve from 0.08175\n",
            "Epoch 586/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0676 - val_loss: 0.0832\n",
            "\n",
            "Epoch 00586: val_loss did not improve from 0.08175\n",
            "Epoch 587/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0673 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00587: val_loss did not improve from 0.08175\n",
            "Epoch 588/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0672 - val_loss: 0.0851\n",
            "\n",
            "Epoch 00588: val_loss did not improve from 0.08175\n",
            "Epoch 589/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0675 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00589: val_loss did not improve from 0.08175\n",
            "Epoch 590/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0667 - val_loss: 0.0840\n",
            "\n",
            "Epoch 00590: val_loss did not improve from 0.08175\n",
            "Epoch 591/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0654 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00591: val_loss improved from 0.08175 to 0.08173, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 592/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0670 - val_loss: 0.0820\n",
            "\n",
            "Epoch 00592: val_loss did not improve from 0.08173\n",
            "Epoch 593/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0651 - val_loss: 0.0819\n",
            "\n",
            "Epoch 00593: val_loss did not improve from 0.08173\n",
            "Epoch 594/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0664 - val_loss: 0.0840\n",
            "\n",
            "Epoch 00594: val_loss did not improve from 0.08173\n",
            "Epoch 595/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0673 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00595: val_loss did not improve from 0.08173\n",
            "Epoch 596/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0664 - val_loss: 0.0839\n",
            "\n",
            "Epoch 00596: val_loss did not improve from 0.08173\n",
            "Epoch 597/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0673 - val_loss: 0.0837\n",
            "\n",
            "Epoch 00597: val_loss did not improve from 0.08173\n",
            "Epoch 598/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0676 - val_loss: 0.0845\n",
            "\n",
            "Epoch 00598: val_loss did not improve from 0.08173\n",
            "Epoch 599/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0668 - val_loss: 0.0830\n",
            "\n",
            "Epoch 00599: val_loss did not improve from 0.08173\n",
            "Epoch 600/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0663 - val_loss: 0.0842\n",
            "\n",
            "Epoch 00600: val_loss did not improve from 0.08173\n",
            "Epoch 601/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0657 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00601: val_loss improved from 0.08173 to 0.08170, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 602/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0656 - val_loss: 0.0828\n",
            "\n",
            "Epoch 00602: val_loss did not improve from 0.08170\n",
            "Epoch 603/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0661 - val_loss: 0.0839\n",
            "\n",
            "Epoch 00603: val_loss did not improve from 0.08170\n",
            "Epoch 604/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0673 - val_loss: 0.0823\n",
            "\n",
            "Epoch 00604: val_loss did not improve from 0.08170\n",
            "Epoch 605/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0647 - val_loss: 0.0842\n",
            "\n",
            "Epoch 00605: val_loss did not improve from 0.08170\n",
            "Epoch 606/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0663 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00606: val_loss did not improve from 0.08170\n",
            "Epoch 607/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0645 - val_loss: 0.0868\n",
            "\n",
            "Epoch 00607: val_loss did not improve from 0.08170\n",
            "Epoch 608/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0665 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00608: val_loss did not improve from 0.08170\n",
            "Epoch 609/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0667 - val_loss: 0.0867\n",
            "\n",
            "Epoch 00609: val_loss did not improve from 0.08170\n",
            "Epoch 610/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0652 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00610: val_loss did not improve from 0.08170\n",
            "Epoch 611/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0658 - val_loss: 0.0878\n",
            "\n",
            "Epoch 00611: val_loss did not improve from 0.08170\n",
            "Epoch 612/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0664 - val_loss: 0.0823\n",
            "\n",
            "Epoch 00612: val_loss did not improve from 0.08170\n",
            "Epoch 613/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0657 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00613: val_loss did not improve from 0.08170\n",
            "Epoch 614/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0660 - val_loss: 0.0840\n",
            "\n",
            "Epoch 00614: val_loss did not improve from 0.08170\n",
            "Epoch 615/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0664 - val_loss: 0.0827\n",
            "\n",
            "Epoch 00615: val_loss did not improve from 0.08170\n",
            "Epoch 616/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0678 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00616: val_loss did not improve from 0.08170\n",
            "Epoch 617/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0647 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00617: val_loss improved from 0.08170 to 0.08132, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 618/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0652 - val_loss: 0.0820\n",
            "\n",
            "Epoch 00618: val_loss did not improve from 0.08132\n",
            "Epoch 619/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0652 - val_loss: 0.0820\n",
            "\n",
            "Epoch 00619: val_loss did not improve from 0.08132\n",
            "Epoch 620/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0664 - val_loss: 0.0845\n",
            "\n",
            "Epoch 00620: val_loss did not improve from 0.08132\n",
            "Epoch 621/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0669 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00621: val_loss did not improve from 0.08132\n",
            "Epoch 622/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0652 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00622: val_loss did not improve from 0.08132\n",
            "Epoch 623/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0658 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00623: val_loss improved from 0.08132 to 0.08131, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 624/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0652 - val_loss: 0.0840\n",
            "\n",
            "Epoch 00624: val_loss did not improve from 0.08131\n",
            "Epoch 625/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0653 - val_loss: 0.0833\n",
            "\n",
            "Epoch 00625: val_loss did not improve from 0.08131\n",
            "Epoch 626/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0666 - val_loss: 0.0856\n",
            "\n",
            "Epoch 00626: val_loss did not improve from 0.08131\n",
            "Epoch 627/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0653 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00627: val_loss did not improve from 0.08131\n",
            "Epoch 628/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0658 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00628: val_loss did not improve from 0.08131\n",
            "Epoch 629/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0655 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00629: val_loss did not improve from 0.08131\n",
            "Epoch 630/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0651 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00630: val_loss did not improve from 0.08131\n",
            "Epoch 631/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0640 - val_loss: 0.0820\n",
            "\n",
            "Epoch 00631: val_loss did not improve from 0.08131\n",
            "Epoch 632/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0649 - val_loss: 0.0830\n",
            "\n",
            "Epoch 00632: val_loss did not improve from 0.08131\n",
            "Epoch 633/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0655 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00633: val_loss did not improve from 0.08131\n",
            "Epoch 634/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0640 - val_loss: 0.0836\n",
            "\n",
            "Epoch 00634: val_loss did not improve from 0.08131\n",
            "Epoch 635/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0668 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00635: val_loss did not improve from 0.08131\n",
            "Epoch 636/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0633 - val_loss: 0.0828\n",
            "\n",
            "Epoch 00636: val_loss did not improve from 0.08131\n",
            "Epoch 637/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0652 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00637: val_loss did not improve from 0.08131\n",
            "Epoch 638/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0632 - val_loss: 0.0837\n",
            "\n",
            "Epoch 00638: val_loss did not improve from 0.08131\n",
            "Epoch 639/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0665 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00639: val_loss did not improve from 0.08131\n",
            "Epoch 640/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0634 - val_loss: 0.0816\n",
            "\n",
            "Epoch 00640: val_loss did not improve from 0.08131\n",
            "Epoch 641/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0652 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00641: val_loss did not improve from 0.08131\n",
            "Epoch 642/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0651 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00642: val_loss improved from 0.08131 to 0.08129, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 643/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0651 - val_loss: 0.0851\n",
            "\n",
            "Epoch 00643: val_loss did not improve from 0.08129\n",
            "Epoch 644/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0656 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00644: val_loss improved from 0.08129 to 0.08111, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 645/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0638 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00645: val_loss did not improve from 0.08111\n",
            "Epoch 646/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0657 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00646: val_loss did not improve from 0.08111\n",
            "Epoch 647/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0631 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00647: val_loss did not improve from 0.08111\n",
            "Epoch 648/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0652 - val_loss: 0.0820\n",
            "\n",
            "Epoch 00648: val_loss did not improve from 0.08111\n",
            "Epoch 649/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0647 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00649: val_loss did not improve from 0.08111\n",
            "Epoch 650/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0645 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00650: val_loss did not improve from 0.08111\n",
            "Epoch 651/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0647 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00651: val_loss did not improve from 0.08111\n",
            "Epoch 652/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0660 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00652: val_loss did not improve from 0.08111\n",
            "Epoch 653/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0649 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00653: val_loss did not improve from 0.08111\n",
            "Epoch 654/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0650 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00654: val_loss did not improve from 0.08111\n",
            "Epoch 655/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0642 - val_loss: 0.0861\n",
            "\n",
            "Epoch 00655: val_loss did not improve from 0.08111\n",
            "Epoch 656/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0631 - val_loss: 0.0827\n",
            "\n",
            "Epoch 00656: val_loss did not improve from 0.08111\n",
            "Epoch 657/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0644 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00657: val_loss did not improve from 0.08111\n",
            "Epoch 658/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0652 - val_loss: 0.0823\n",
            "\n",
            "Epoch 00658: val_loss did not improve from 0.08111\n",
            "Epoch 659/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0637 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00659: val_loss did not improve from 0.08111\n",
            "Epoch 660/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0634 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00660: val_loss improved from 0.08111 to 0.08108, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 661/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0640 - val_loss: 0.0833\n",
            "\n",
            "Epoch 00661: val_loss did not improve from 0.08108\n",
            "Epoch 662/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0645 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00662: val_loss did not improve from 0.08108\n",
            "Epoch 663/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0638 - val_loss: 0.0830\n",
            "\n",
            "Epoch 00663: val_loss did not improve from 0.08108\n",
            "Epoch 664/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0635 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00664: val_loss did not improve from 0.08108\n",
            "Epoch 665/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0636 - val_loss: 0.0843\n",
            "\n",
            "Epoch 00665: val_loss did not improve from 0.08108\n",
            "Epoch 666/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0638 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00666: val_loss did not improve from 0.08108\n",
            "Epoch 667/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0640 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00667: val_loss did not improve from 0.08108\n",
            "Epoch 668/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0638 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00668: val_loss did not improve from 0.08108\n",
            "Epoch 669/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0635 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00669: val_loss did not improve from 0.08108\n",
            "Epoch 670/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0642 - val_loss: 0.0820\n",
            "\n",
            "Epoch 00670: val_loss did not improve from 0.08108\n",
            "Epoch 671/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0634 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00671: val_loss did not improve from 0.08108\n",
            "Epoch 672/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0630 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00672: val_loss did not improve from 0.08108\n",
            "Epoch 673/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0632 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00673: val_loss did not improve from 0.08108\n",
            "Epoch 674/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0643 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00674: val_loss did not improve from 0.08108\n",
            "Epoch 675/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0627 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00675: val_loss did not improve from 0.08108\n",
            "Epoch 676/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0637 - val_loss: 0.0862\n",
            "\n",
            "Epoch 00676: val_loss did not improve from 0.08108\n",
            "Epoch 677/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0649 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00677: val_loss did not improve from 0.08108\n",
            "Epoch 678/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0633 - val_loss: 0.0835\n",
            "\n",
            "Epoch 00678: val_loss did not improve from 0.08108\n",
            "Epoch 679/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0636 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00679: val_loss improved from 0.08108 to 0.08106, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 680/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0632 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00680: val_loss improved from 0.08106 to 0.08098, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 681/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0625 - val_loss: 0.0828\n",
            "\n",
            "Epoch 00681: val_loss did not improve from 0.08098\n",
            "Epoch 682/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0630 - val_loss: 0.0809\n",
            "\n",
            "Epoch 00682: val_loss improved from 0.08098 to 0.08088, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 683/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0628 - val_loss: 0.0820\n",
            "\n",
            "Epoch 00683: val_loss did not improve from 0.08088\n",
            "Epoch 684/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0621 - val_loss: 0.0826\n",
            "\n",
            "Epoch 00684: val_loss did not improve from 0.08088\n",
            "Epoch 685/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0633 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00685: val_loss improved from 0.08088 to 0.08084, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 686/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0612 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00686: val_loss did not improve from 0.08084\n",
            "Epoch 687/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0622 - val_loss: 0.0847\n",
            "\n",
            "Epoch 00687: val_loss did not improve from 0.08084\n",
            "Epoch 688/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0647 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00688: val_loss did not improve from 0.08084\n",
            "Epoch 689/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0635 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00689: val_loss improved from 0.08084 to 0.08071, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 690/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0615 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00690: val_loss did not improve from 0.08071\n",
            "Epoch 691/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0634 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00691: val_loss did not improve from 0.08071\n",
            "Epoch 692/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0627 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00692: val_loss improved from 0.08071 to 0.08067, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 693/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0616 - val_loss: 0.0833\n",
            "\n",
            "Epoch 00693: val_loss did not improve from 0.08067\n",
            "Epoch 694/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0643 - val_loss: 0.0815\n",
            "\n",
            "Epoch 00694: val_loss did not improve from 0.08067\n",
            "Epoch 695/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0629 - val_loss: 0.0835\n",
            "\n",
            "Epoch 00695: val_loss did not improve from 0.08067\n",
            "Epoch 696/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0631 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00696: val_loss did not improve from 0.08067\n",
            "Epoch 697/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0636 - val_loss: 0.0837\n",
            "\n",
            "Epoch 00697: val_loss did not improve from 0.08067\n",
            "Epoch 698/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0622 - val_loss: 0.0809\n",
            "\n",
            "Epoch 00698: val_loss did not improve from 0.08067\n",
            "Epoch 699/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0639 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00699: val_loss did not improve from 0.08067\n",
            "Epoch 700/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0618 - val_loss: 0.0823\n",
            "\n",
            "Epoch 00700: val_loss did not improve from 0.08067\n",
            "Epoch 701/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0628 - val_loss: 0.0842\n",
            "\n",
            "Epoch 00701: val_loss did not improve from 0.08067\n",
            "Epoch 702/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0632 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00702: val_loss did not improve from 0.08067\n",
            "Epoch 703/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0630 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00703: val_loss did not improve from 0.08067\n",
            "Epoch 704/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0623 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00704: val_loss did not improve from 0.08067\n",
            "Epoch 705/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0622 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00705: val_loss did not improve from 0.08067\n",
            "Epoch 706/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0630 - val_loss: 0.0826\n",
            "\n",
            "Epoch 00706: val_loss did not improve from 0.08067\n",
            "Epoch 707/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0631 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00707: val_loss did not improve from 0.08067\n",
            "Epoch 708/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0617 - val_loss: 0.0806\n",
            "\n",
            "Epoch 00708: val_loss improved from 0.08067 to 0.08062, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 709/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0620 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00709: val_loss did not improve from 0.08062\n",
            "Epoch 710/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0624 - val_loss: 0.0819\n",
            "\n",
            "Epoch 00710: val_loss did not improve from 0.08062\n",
            "Epoch 711/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0617 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00711: val_loss did not improve from 0.08062\n",
            "Epoch 712/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0619 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00712: val_loss did not improve from 0.08062\n",
            "Epoch 713/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0619 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00713: val_loss did not improve from 0.08062\n",
            "Epoch 714/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0620 - val_loss: 0.0809\n",
            "\n",
            "Epoch 00714: val_loss did not improve from 0.08062\n",
            "Epoch 715/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0607 - val_loss: 0.0809\n",
            "\n",
            "Epoch 00715: val_loss did not improve from 0.08062\n",
            "Epoch 716/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0610 - val_loss: 0.0826\n",
            "\n",
            "Epoch 00716: val_loss did not improve from 0.08062\n",
            "Epoch 717/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0626 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00717: val_loss did not improve from 0.08062\n",
            "Epoch 718/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0618 - val_loss: 0.0827\n",
            "\n",
            "Epoch 00718: val_loss did not improve from 0.08062\n",
            "Epoch 719/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0621 - val_loss: 0.0819\n",
            "\n",
            "Epoch 00719: val_loss did not improve from 0.08062\n",
            "Epoch 720/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0630 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00720: val_loss did not improve from 0.08062\n",
            "Epoch 721/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0615 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00721: val_loss did not improve from 0.08062\n",
            "Epoch 722/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0603 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00722: val_loss did not improve from 0.08062\n",
            "Epoch 723/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0614 - val_loss: 0.0819\n",
            "\n",
            "Epoch 00723: val_loss did not improve from 0.08062\n",
            "Epoch 724/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0628 - val_loss: 0.0866\n",
            "\n",
            "Epoch 00724: val_loss did not improve from 0.08062\n",
            "Epoch 725/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0616 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00725: val_loss did not improve from 0.08062\n",
            "Epoch 726/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0613 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00726: val_loss did not improve from 0.08062\n",
            "Epoch 727/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0621 - val_loss: 0.0809\n",
            "\n",
            "Epoch 00727: val_loss did not improve from 0.08062\n",
            "Epoch 728/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0627 - val_loss: 0.0816\n",
            "\n",
            "Epoch 00728: val_loss did not improve from 0.08062\n",
            "Epoch 729/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0606 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00729: val_loss did not improve from 0.08062\n",
            "Epoch 730/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0602 - val_loss: 0.0809\n",
            "\n",
            "Epoch 00730: val_loss did not improve from 0.08062\n",
            "Epoch 731/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0613 - val_loss: 0.0839\n",
            "\n",
            "Epoch 00731: val_loss did not improve from 0.08062\n",
            "Epoch 732/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0612 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00732: val_loss did not improve from 0.08062\n",
            "Epoch 733/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0606 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00733: val_loss did not improve from 0.08062\n",
            "Epoch 734/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0613 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00734: val_loss improved from 0.08062 to 0.08040, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 735/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0610 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00735: val_loss did not improve from 0.08040\n",
            "Epoch 736/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0599 - val_loss: 0.0815\n",
            "\n",
            "Epoch 00736: val_loss did not improve from 0.08040\n",
            "Epoch 737/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0616 - val_loss: 0.0844\n",
            "\n",
            "Epoch 00737: val_loss did not improve from 0.08040\n",
            "Epoch 738/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0609 - val_loss: 0.0805\n",
            "\n",
            "Epoch 00738: val_loss did not improve from 0.08040\n",
            "Epoch 739/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0613 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00739: val_loss did not improve from 0.08040\n",
            "Epoch 740/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0613 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00740: val_loss improved from 0.08040 to 0.08030, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 741/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0611 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00741: val_loss did not improve from 0.08030\n",
            "Epoch 742/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0602 - val_loss: 0.0819\n",
            "\n",
            "Epoch 00742: val_loss did not improve from 0.08030\n",
            "Epoch 743/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0600 - val_loss: 0.0850\n",
            "\n",
            "Epoch 00743: val_loss did not improve from 0.08030\n",
            "Epoch 744/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0612 - val_loss: 0.0809\n",
            "\n",
            "Epoch 00744: val_loss did not improve from 0.08030\n",
            "Epoch 745/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0596 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00745: val_loss improved from 0.08030 to 0.08004, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 746/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0610 - val_loss: 0.0827\n",
            "\n",
            "Epoch 00746: val_loss did not improve from 0.08004\n",
            "Epoch 747/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0613 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00747: val_loss did not improve from 0.08004\n",
            "Epoch 748/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0601 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00748: val_loss did not improve from 0.08004\n",
            "Epoch 749/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0607 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00749: val_loss did not improve from 0.08004\n",
            "Epoch 750/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0613 - val_loss: 0.0840\n",
            "\n",
            "Epoch 00750: val_loss did not improve from 0.08004\n",
            "Epoch 751/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0607 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00751: val_loss did not improve from 0.08004\n",
            "Epoch 752/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0617 - val_loss: 0.0828\n",
            "\n",
            "Epoch 00752: val_loss did not improve from 0.08004\n",
            "Epoch 753/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0611 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00753: val_loss did not improve from 0.08004\n",
            "Epoch 754/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0606 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00754: val_loss did not improve from 0.08004\n",
            "Epoch 755/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0604 - val_loss: 0.0809\n",
            "\n",
            "Epoch 00755: val_loss did not improve from 0.08004\n",
            "Epoch 756/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0610 - val_loss: 0.0837\n",
            "\n",
            "Epoch 00756: val_loss did not improve from 0.08004\n",
            "Epoch 757/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0599 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00757: val_loss did not improve from 0.08004\n",
            "Epoch 758/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0588 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00758: val_loss did not improve from 0.08004\n",
            "Epoch 759/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0609 - val_loss: 0.0805\n",
            "\n",
            "Epoch 00759: val_loss did not improve from 0.08004\n",
            "Epoch 760/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0601 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00760: val_loss did not improve from 0.08004\n",
            "Epoch 761/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0610 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00761: val_loss did not improve from 0.08004\n",
            "Epoch 762/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0606 - val_loss: 0.0809\n",
            "\n",
            "Epoch 00762: val_loss did not improve from 0.08004\n",
            "Epoch 763/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0597 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00763: val_loss did not improve from 0.08004\n",
            "Epoch 764/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0601 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00764: val_loss did not improve from 0.08004\n",
            "Epoch 765/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0594 - val_loss: 0.0823\n",
            "\n",
            "Epoch 00765: val_loss did not improve from 0.08004\n",
            "Epoch 766/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0606 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00766: val_loss did not improve from 0.08004\n",
            "Epoch 767/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0600 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00767: val_loss did not improve from 0.08004\n",
            "Epoch 768/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0596 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00768: val_loss did not improve from 0.08004\n",
            "Epoch 769/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0601 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00769: val_loss did not improve from 0.08004\n",
            "Epoch 770/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0601 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00770: val_loss did not improve from 0.08004\n",
            "Epoch 771/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0607 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00771: val_loss did not improve from 0.08004\n",
            "Epoch 772/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0603 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00772: val_loss did not improve from 0.08004\n",
            "Epoch 773/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0587 - val_loss: 0.0806\n",
            "\n",
            "Epoch 00773: val_loss did not improve from 0.08004\n",
            "Epoch 774/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0598 - val_loss: 0.0816\n",
            "\n",
            "Epoch 00774: val_loss did not improve from 0.08004\n",
            "Epoch 775/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0599 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00775: val_loss did not improve from 0.08004\n",
            "Epoch 776/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0597 - val_loss: 0.0826\n",
            "\n",
            "Epoch 00776: val_loss did not improve from 0.08004\n",
            "Epoch 777/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0591 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00777: val_loss did not improve from 0.08004\n",
            "Epoch 778/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0587 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00778: val_loss did not improve from 0.08004\n",
            "Epoch 779/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0577 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00779: val_loss improved from 0.08004 to 0.08003, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 780/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0590 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00780: val_loss did not improve from 0.08003\n",
            "Epoch 781/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0610 - val_loss: 0.0819\n",
            "\n",
            "Epoch 00781: val_loss did not improve from 0.08003\n",
            "Epoch 782/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0598 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00782: val_loss did not improve from 0.08003\n",
            "Epoch 783/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0598 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00783: val_loss did not improve from 0.08003\n",
            "Epoch 784/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0589 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00784: val_loss improved from 0.08003 to 0.07995, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 785/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0586 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00785: val_loss did not improve from 0.07995\n",
            "Epoch 786/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0606 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00786: val_loss did not improve from 0.07995\n",
            "Epoch 787/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0605 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00787: val_loss did not improve from 0.07995\n",
            "Epoch 788/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0592 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00788: val_loss did not improve from 0.07995\n",
            "Epoch 789/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0594 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00789: val_loss did not improve from 0.07995\n",
            "Epoch 790/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0601 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00790: val_loss did not improve from 0.07995\n",
            "Epoch 791/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0594 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00791: val_loss did not improve from 0.07995\n",
            "Epoch 792/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0583 - val_loss: 0.0798\n",
            "\n",
            "Epoch 00792: val_loss improved from 0.07995 to 0.07985, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 793/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0585 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00793: val_loss did not improve from 0.07985\n",
            "Epoch 794/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0602 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00794: val_loss did not improve from 0.07985\n",
            "Epoch 795/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0588 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00795: val_loss did not improve from 0.07985\n",
            "Epoch 796/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0589 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00796: val_loss did not improve from 0.07985\n",
            "Epoch 797/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0589 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00797: val_loss did not improve from 0.07985\n",
            "Epoch 798/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0576 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00798: val_loss did not improve from 0.07985\n",
            "Epoch 799/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0598 - val_loss: 0.0816\n",
            "\n",
            "Epoch 00799: val_loss did not improve from 0.07985\n",
            "Epoch 800/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0591 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00800: val_loss did not improve from 0.07985\n",
            "Epoch 801/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0587 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00801: val_loss did not improve from 0.07985\n",
            "Epoch 802/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0593 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00802: val_loss did not improve from 0.07985\n",
            "Epoch 803/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0583 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00803: val_loss did not improve from 0.07985\n",
            "Epoch 804/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0591 - val_loss: 0.0825\n",
            "\n",
            "Epoch 00804: val_loss did not improve from 0.07985\n",
            "Epoch 805/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0593 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00805: val_loss did not improve from 0.07985\n",
            "Epoch 806/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0582 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00806: val_loss did not improve from 0.07985\n",
            "Epoch 807/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0592 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00807: val_loss did not improve from 0.07985\n",
            "Epoch 808/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0577 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00808: val_loss did not improve from 0.07985\n",
            "Epoch 809/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0582 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00809: val_loss did not improve from 0.07985\n",
            "Epoch 810/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0578 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00810: val_loss did not improve from 0.07985\n",
            "Epoch 811/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0587 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00811: val_loss did not improve from 0.07985\n",
            "Epoch 812/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0583 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00812: val_loss did not improve from 0.07985\n",
            "Epoch 813/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0585 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00813: val_loss did not improve from 0.07985\n",
            "Epoch 814/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0586 - val_loss: 0.0806\n",
            "\n",
            "Epoch 00814: val_loss did not improve from 0.07985\n",
            "Epoch 815/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0585 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00815: val_loss did not improve from 0.07985\n",
            "Epoch 816/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0590 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00816: val_loss did not improve from 0.07985\n",
            "Epoch 817/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0587 - val_loss: 0.0805\n",
            "\n",
            "Epoch 00817: val_loss did not improve from 0.07985\n",
            "Epoch 818/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0566 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00818: val_loss did not improve from 0.07985\n",
            "Epoch 819/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0581 - val_loss: 0.0815\n",
            "\n",
            "Epoch 00819: val_loss did not improve from 0.07985\n",
            "Epoch 820/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0597 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00820: val_loss did not improve from 0.07985\n",
            "Epoch 821/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0582 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00821: val_loss did not improve from 0.07985\n",
            "Epoch 822/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0585 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00822: val_loss did not improve from 0.07985\n",
            "Epoch 823/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0579 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00823: val_loss did not improve from 0.07985\n",
            "Epoch 824/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0584 - val_loss: 0.0797\n",
            "\n",
            "Epoch 00824: val_loss improved from 0.07985 to 0.07971, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 825/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0576 - val_loss: 0.0855\n",
            "\n",
            "Epoch 00825: val_loss did not improve from 0.07971\n",
            "Epoch 826/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0589 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00826: val_loss did not improve from 0.07971\n",
            "Epoch 827/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0570 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00827: val_loss did not improve from 0.07971\n",
            "Epoch 828/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0578 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00828: val_loss did not improve from 0.07971\n",
            "Epoch 829/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0566 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00829: val_loss did not improve from 0.07971\n",
            "Epoch 830/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0579 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00830: val_loss did not improve from 0.07971\n",
            "Epoch 831/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0582 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00831: val_loss did not improve from 0.07971\n",
            "Epoch 832/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0587 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00832: val_loss did not improve from 0.07971\n",
            "Epoch 833/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0560 - val_loss: 0.0799\n",
            "\n",
            "Epoch 00833: val_loss did not improve from 0.07971\n",
            "Epoch 834/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0576 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00834: val_loss did not improve from 0.07971\n",
            "Epoch 835/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0579 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00835: val_loss did not improve from 0.07971\n",
            "Epoch 836/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0585 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00836: val_loss did not improve from 0.07971\n",
            "Epoch 837/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0583 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00837: val_loss did not improve from 0.07971\n",
            "Epoch 838/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0579 - val_loss: 0.0797\n",
            "\n",
            "Epoch 00838: val_loss did not improve from 0.07971\n",
            "Epoch 839/1000\n",
            "13/13 [==============================] - 0s 39ms/step - loss: 0.0567 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00839: val_loss did not improve from 0.07971\n",
            "Epoch 840/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0577 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00840: val_loss did not improve from 0.07971\n",
            "Epoch 841/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0577 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00841: val_loss did not improve from 0.07971\n",
            "Epoch 842/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0566 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00842: val_loss did not improve from 0.07971\n",
            "Epoch 843/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0574 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00843: val_loss did not improve from 0.07971\n",
            "Epoch 844/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0583 - val_loss: 0.0832\n",
            "\n",
            "Epoch 00844: val_loss did not improve from 0.07971\n",
            "Epoch 845/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0573 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00845: val_loss did not improve from 0.07971\n",
            "Epoch 846/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0571 - val_loss: 0.0809\n",
            "\n",
            "Epoch 00846: val_loss did not improve from 0.07971\n",
            "Epoch 847/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0573 - val_loss: 0.0842\n",
            "\n",
            "Epoch 00847: val_loss did not improve from 0.07971\n",
            "Epoch 848/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0576 - val_loss: 0.0839\n",
            "\n",
            "Epoch 00848: val_loss did not improve from 0.07971\n",
            "Epoch 849/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0579 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00849: val_loss did not improve from 0.07971\n",
            "Epoch 850/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0576 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00850: val_loss did not improve from 0.07971\n",
            "Epoch 851/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0574 - val_loss: 0.0799\n",
            "\n",
            "Epoch 00851: val_loss did not improve from 0.07971\n",
            "Epoch 852/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0556 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00852: val_loss did not improve from 0.07971\n",
            "Epoch 853/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0565 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00853: val_loss did not improve from 0.07971\n",
            "Epoch 854/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0570 - val_loss: 0.0820\n",
            "\n",
            "Epoch 00854: val_loss did not improve from 0.07971\n",
            "Epoch 855/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0569 - val_loss: 0.0805\n",
            "\n",
            "Epoch 00855: val_loss did not improve from 0.07971\n",
            "Epoch 856/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0574 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00856: val_loss did not improve from 0.07971\n",
            "Epoch 857/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0575 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00857: val_loss did not improve from 0.07971\n",
            "Epoch 858/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0573 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00858: val_loss did not improve from 0.07971\n",
            "Epoch 859/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0570 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00859: val_loss did not improve from 0.07971\n",
            "Epoch 860/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0562 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00860: val_loss did not improve from 0.07971\n",
            "Epoch 861/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0562 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00861: val_loss did not improve from 0.07971\n",
            "Epoch 862/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0557 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00862: val_loss did not improve from 0.07971\n",
            "Epoch 863/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0566 - val_loss: 0.0835\n",
            "\n",
            "Epoch 00863: val_loss did not improve from 0.07971\n",
            "Epoch 864/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0564 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00864: val_loss did not improve from 0.07971\n",
            "Epoch 865/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0569 - val_loss: 0.0819\n",
            "\n",
            "Epoch 00865: val_loss did not improve from 0.07971\n",
            "Epoch 866/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0580 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00866: val_loss did not improve from 0.07971\n",
            "Epoch 867/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0569 - val_loss: 0.0816\n",
            "\n",
            "Epoch 00867: val_loss did not improve from 0.07971\n",
            "Epoch 868/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0578 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00868: val_loss did not improve from 0.07971\n",
            "Epoch 869/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0555 - val_loss: 0.0816\n",
            "\n",
            "Epoch 00869: val_loss did not improve from 0.07971\n",
            "Epoch 870/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0577 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00870: val_loss did not improve from 0.07971\n",
            "Epoch 871/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0558 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00871: val_loss did not improve from 0.07971\n",
            "Epoch 872/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0566 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00872: val_loss did not improve from 0.07971\n",
            "Epoch 873/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0579 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00873: val_loss did not improve from 0.07971\n",
            "Epoch 874/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0561 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00874: val_loss did not improve from 0.07971\n",
            "Epoch 875/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0561 - val_loss: 0.0806\n",
            "\n",
            "Epoch 00875: val_loss did not improve from 0.07971\n",
            "Epoch 876/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0563 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00876: val_loss did not improve from 0.07971\n",
            "Epoch 877/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0565 - val_loss: 0.0836\n",
            "\n",
            "Epoch 00877: val_loss did not improve from 0.07971\n",
            "Epoch 878/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0575 - val_loss: 0.0805\n",
            "\n",
            "Epoch 00878: val_loss did not improve from 0.07971\n",
            "Epoch 879/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0563 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00879: val_loss did not improve from 0.07971\n",
            "Epoch 880/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0555 - val_loss: 0.0798\n",
            "\n",
            "Epoch 00880: val_loss did not improve from 0.07971\n",
            "Epoch 881/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0561 - val_loss: 0.0826\n",
            "\n",
            "Epoch 00881: val_loss did not improve from 0.07971\n",
            "Epoch 882/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0563 - val_loss: 0.0830\n",
            "\n",
            "Epoch 00882: val_loss did not improve from 0.07971\n",
            "Epoch 883/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0558 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00883: val_loss did not improve from 0.07971\n",
            "Epoch 884/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0559 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00884: val_loss did not improve from 0.07971\n",
            "Epoch 885/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0549 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00885: val_loss did not improve from 0.07971\n",
            "Epoch 886/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0561 - val_loss: 0.0819\n",
            "\n",
            "Epoch 00886: val_loss did not improve from 0.07971\n",
            "Epoch 887/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0568 - val_loss: 0.0805\n",
            "\n",
            "Epoch 00887: val_loss did not improve from 0.07971\n",
            "Epoch 888/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0547 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00888: val_loss did not improve from 0.07971\n",
            "Epoch 889/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0563 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00889: val_loss did not improve from 0.07971\n",
            "Epoch 890/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0558 - val_loss: 0.0819\n",
            "\n",
            "Epoch 00890: val_loss did not improve from 0.07971\n",
            "Epoch 891/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0567 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00891: val_loss did not improve from 0.07971\n",
            "Epoch 892/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0552 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00892: val_loss did not improve from 0.07971\n",
            "Epoch 893/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0551 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00893: val_loss did not improve from 0.07971\n",
            "Epoch 894/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0556 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00894: val_loss did not improve from 0.07971\n",
            "Epoch 895/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0569 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00895: val_loss did not improve from 0.07971\n",
            "Epoch 896/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0569 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00896: val_loss did not improve from 0.07971\n",
            "Epoch 897/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0557 - val_loss: 0.0826\n",
            "\n",
            "Epoch 00897: val_loss did not improve from 0.07971\n",
            "Epoch 898/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0554 - val_loss: 0.0798\n",
            "\n",
            "Epoch 00898: val_loss did not improve from 0.07971\n",
            "Epoch 899/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0556 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00899: val_loss did not improve from 0.07971\n",
            "Epoch 900/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0559 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00900: val_loss did not improve from 0.07971\n",
            "Epoch 901/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0557 - val_loss: 0.0799\n",
            "\n",
            "Epoch 00901: val_loss did not improve from 0.07971\n",
            "Epoch 902/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0542 - val_loss: 0.0815\n",
            "\n",
            "Epoch 00902: val_loss did not improve from 0.07971\n",
            "Epoch 903/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0562 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00903: val_loss did not improve from 0.07971\n",
            "Epoch 904/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0558 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00904: val_loss did not improve from 0.07971\n",
            "Epoch 905/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0535 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00905: val_loss did not improve from 0.07971\n",
            "Epoch 906/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0545 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00906: val_loss did not improve from 0.07971\n",
            "Epoch 907/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0560 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00907: val_loss did not improve from 0.07971\n",
            "Epoch 908/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0555 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00908: val_loss did not improve from 0.07971\n",
            "Epoch 909/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0562 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00909: val_loss did not improve from 0.07971\n",
            "Epoch 910/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0553 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00910: val_loss did not improve from 0.07971\n",
            "Epoch 911/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0554 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00911: val_loss did not improve from 0.07971\n",
            "Epoch 912/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0548 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00912: val_loss did not improve from 0.07971\n",
            "Epoch 913/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0537 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00913: val_loss did not improve from 0.07971\n",
            "Epoch 914/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0548 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00914: val_loss did not improve from 0.07971\n",
            "Epoch 915/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0554 - val_loss: 0.0832\n",
            "\n",
            "Epoch 00915: val_loss did not improve from 0.07971\n",
            "Epoch 916/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0542 - val_loss: 0.0798\n",
            "\n",
            "Epoch 00916: val_loss did not improve from 0.07971\n",
            "Epoch 917/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0543 - val_loss: 0.0867\n",
            "\n",
            "Epoch 00917: val_loss did not improve from 0.07971\n",
            "Epoch 918/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0568 - val_loss: 0.0805\n",
            "\n",
            "Epoch 00918: val_loss did not improve from 0.07971\n",
            "Epoch 919/1000\n",
            "13/13 [==============================] - 1s 41ms/step - loss: 0.0546 - val_loss: 0.0823\n",
            "\n",
            "Epoch 00919: val_loss did not improve from 0.07971\n",
            "Epoch 920/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0562 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00920: val_loss did not improve from 0.07971\n",
            "Epoch 921/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0542 - val_loss: 0.0799\n",
            "\n",
            "Epoch 00921: val_loss did not improve from 0.07971\n",
            "Epoch 922/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0551 - val_loss: 0.0815\n",
            "\n",
            "Epoch 00922: val_loss did not improve from 0.07971\n",
            "Epoch 923/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0553 - val_loss: 0.0798\n",
            "\n",
            "Epoch 00923: val_loss did not improve from 0.07971\n",
            "Epoch 924/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0535 - val_loss: 0.0823\n",
            "\n",
            "Epoch 00924: val_loss did not improve from 0.07971\n",
            "Epoch 925/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0551 - val_loss: 0.0805\n",
            "\n",
            "Epoch 00925: val_loss did not improve from 0.07971\n",
            "Epoch 926/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0558 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00926: val_loss did not improve from 0.07971\n",
            "Epoch 927/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0548 - val_loss: 0.0798\n",
            "\n",
            "Epoch 00927: val_loss did not improve from 0.07971\n",
            "Epoch 928/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0544 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00928: val_loss did not improve from 0.07971\n",
            "Epoch 929/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0541 - val_loss: 0.0799\n",
            "\n",
            "Epoch 00929: val_loss did not improve from 0.07971\n",
            "Epoch 930/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0530 - val_loss: 0.0828\n",
            "\n",
            "Epoch 00930: val_loss did not improve from 0.07971\n",
            "Epoch 931/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0555 - val_loss: 0.0805\n",
            "\n",
            "Epoch 00931: val_loss did not improve from 0.07971\n",
            "Epoch 932/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0550 - val_loss: 0.0820\n",
            "\n",
            "Epoch 00932: val_loss did not improve from 0.07971\n",
            "Epoch 933/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0556 - val_loss: 0.0805\n",
            "\n",
            "Epoch 00933: val_loss did not improve from 0.07971\n",
            "Epoch 934/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0551 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00934: val_loss did not improve from 0.07971\n",
            "Epoch 935/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0539 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00935: val_loss did not improve from 0.07971\n",
            "Epoch 936/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0552 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00936: val_loss did not improve from 0.07971\n",
            "Epoch 937/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0547 - val_loss: 0.0799\n",
            "\n",
            "Epoch 00937: val_loss did not improve from 0.07971\n",
            "Epoch 938/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0534 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00938: val_loss did not improve from 0.07971\n",
            "Epoch 939/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0551 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00939: val_loss did not improve from 0.07971\n",
            "Epoch 940/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0543 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00940: val_loss did not improve from 0.07971\n",
            "Epoch 941/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0538 - val_loss: 0.0796\n",
            "\n",
            "Epoch 00941: val_loss improved from 0.07971 to 0.07955, saving model to autoencoder_celeba.hdf5\n",
            "Epoch 942/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0548 - val_loss: 0.0827\n",
            "\n",
            "Epoch 00942: val_loss did not improve from 0.07955\n",
            "Epoch 943/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0533 - val_loss: 0.0797\n",
            "\n",
            "Epoch 00943: val_loss did not improve from 0.07955\n",
            "Epoch 944/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0529 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00944: val_loss did not improve from 0.07955\n",
            "Epoch 945/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0548 - val_loss: 0.0807\n",
            "\n",
            "Epoch 00945: val_loss did not improve from 0.07955\n",
            "Epoch 946/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0548 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00946: val_loss did not improve from 0.07955\n",
            "Epoch 947/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0540 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00947: val_loss did not improve from 0.07955\n",
            "Epoch 948/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0550 - val_loss: 0.0798\n",
            "\n",
            "Epoch 00948: val_loss did not improve from 0.07955\n",
            "Epoch 949/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0539 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00949: val_loss did not improve from 0.07955\n",
            "Epoch 950/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0537 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00950: val_loss did not improve from 0.07955\n",
            "Epoch 951/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0534 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00951: val_loss did not improve from 0.07955\n",
            "Epoch 952/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0557 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00952: val_loss did not improve from 0.07955\n",
            "Epoch 953/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0538 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00953: val_loss did not improve from 0.07955\n",
            "Epoch 954/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0538 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00954: val_loss did not improve from 0.07955\n",
            "Epoch 955/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0548 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00955: val_loss did not improve from 0.07955\n",
            "Epoch 956/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0538 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00956: val_loss did not improve from 0.07955\n",
            "Epoch 957/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0540 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00957: val_loss did not improve from 0.07955\n",
            "Epoch 958/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0531 - val_loss: 0.0809\n",
            "\n",
            "Epoch 00958: val_loss did not improve from 0.07955\n",
            "Epoch 959/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0534 - val_loss: 0.0839\n",
            "\n",
            "Epoch 00959: val_loss did not improve from 0.07955\n",
            "Epoch 960/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0538 - val_loss: 0.0816\n",
            "\n",
            "Epoch 00960: val_loss did not improve from 0.07955\n",
            "Epoch 961/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0545 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00961: val_loss did not improve from 0.07955\n",
            "Epoch 962/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0540 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00962: val_loss did not improve from 0.07955\n",
            "Epoch 963/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0529 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00963: val_loss did not improve from 0.07955\n",
            "Epoch 964/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0533 - val_loss: 0.0798\n",
            "\n",
            "Epoch 00964: val_loss did not improve from 0.07955\n",
            "Epoch 965/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0537 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00965: val_loss did not improve from 0.07955\n",
            "Epoch 966/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0525 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00966: val_loss did not improve from 0.07955\n",
            "Epoch 967/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0546 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00967: val_loss did not improve from 0.07955\n",
            "Epoch 968/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0531 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00968: val_loss did not improve from 0.07955\n",
            "Epoch 969/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0543 - val_loss: 0.0810\n",
            "\n",
            "Epoch 00969: val_loss did not improve from 0.07955\n",
            "Epoch 970/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0529 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00970: val_loss did not improve from 0.07955\n",
            "Epoch 971/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0544 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00971: val_loss did not improve from 0.07955\n",
            "Epoch 972/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0522 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00972: val_loss did not improve from 0.07955\n",
            "Epoch 973/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0529 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00973: val_loss did not improve from 0.07955\n",
            "Epoch 974/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0529 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00974: val_loss did not improve from 0.07955\n",
            "Epoch 975/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0525 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00975: val_loss did not improve from 0.07955\n",
            "Epoch 976/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0537 - val_loss: 0.0806\n",
            "\n",
            "Epoch 00976: val_loss did not improve from 0.07955\n",
            "Epoch 977/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0522 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00977: val_loss did not improve from 0.07955\n",
            "Epoch 978/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0538 - val_loss: 0.0801\n",
            "\n",
            "Epoch 00978: val_loss did not improve from 0.07955\n",
            "Epoch 979/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0538 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00979: val_loss did not improve from 0.07955\n",
            "Epoch 980/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0524 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00980: val_loss did not improve from 0.07955\n",
            "Epoch 981/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0534 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00981: val_loss did not improve from 0.07955\n",
            "Epoch 982/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0516 - val_loss: 0.0798\n",
            "\n",
            "Epoch 00982: val_loss did not improve from 0.07955\n",
            "Epoch 983/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0523 - val_loss: 0.0798\n",
            "\n",
            "Epoch 00983: val_loss did not improve from 0.07955\n",
            "Epoch 984/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0513 - val_loss: 0.0806\n",
            "\n",
            "Epoch 00984: val_loss did not improve from 0.07955\n",
            "Epoch 985/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0539 - val_loss: 0.0824\n",
            "\n",
            "Epoch 00985: val_loss did not improve from 0.07955\n",
            "Epoch 986/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0533 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00986: val_loss did not improve from 0.07955\n",
            "Epoch 987/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0547 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00987: val_loss did not improve from 0.07955\n",
            "Epoch 988/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0536 - val_loss: 0.0834\n",
            "\n",
            "Epoch 00988: val_loss did not improve from 0.07955\n",
            "Epoch 989/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0542 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00989: val_loss did not improve from 0.07955\n",
            "Epoch 990/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0542 - val_loss: 0.0808\n",
            "\n",
            "Epoch 00990: val_loss did not improve from 0.07955\n",
            "Epoch 991/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0513 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00991: val_loss did not improve from 0.07955\n",
            "Epoch 992/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0522 - val_loss: 0.0830\n",
            "\n",
            "Epoch 00992: val_loss did not improve from 0.07955\n",
            "Epoch 993/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0525 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00993: val_loss did not improve from 0.07955\n",
            "Epoch 994/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0536 - val_loss: 0.0814\n",
            "\n",
            "Epoch 00994: val_loss did not improve from 0.07955\n",
            "Epoch 995/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0535 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00995: val_loss did not improve from 0.07955\n",
            "Epoch 996/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0531 - val_loss: 0.0802\n",
            "\n",
            "Epoch 00996: val_loss did not improve from 0.07955\n",
            "Epoch 997/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0527 - val_loss: 0.0798\n",
            "\n",
            "Epoch 00997: val_loss did not improve from 0.07955\n",
            "Epoch 998/1000\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.0528 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00998: val_loss did not improve from 0.07955\n",
            "Epoch 999/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0514 - val_loss: 0.0812\n",
            "\n",
            "Epoch 00999: val_loss did not improve from 0.07955\n",
            "Epoch 1000/1000\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.0531 - val_loss: 0.0807\n",
            "\n",
            "Epoch 01000: val_loss did not improve from 0.07955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "o9yfOk06l5jY",
        "outputId": "c46aa7cb-4aed-4f48-9fe3-f372f0722372"
      },
      "source": [
        "%matplotlib inline\n",
        "plt.figure()\n",
        "plt.imshow(X_test[100])\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwM-th9otK5p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "5da9aa63-5467-4bbd-e141-c3c0ffda456d"
      },
      "source": [
        "output_image = autoencoder.predict(np.array([X_test[100]]))\n",
        "plt.figure()\n",
        "plt.imshow(output_image[0])\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29W6xl2XUdNuZ+n8c99956dLHIpkTJIqgISEQ5DVmCBIOWIoNRDOtHECwbARMQ6I8ogYwoMKUECOwgBqQfy/oIBDQixfxQLMkPmQRh2GYYEoaBgBIVUTYfpkkxIthkd1d31X2d136ufJxTd4451VV12V11qqWzBlCofe7eZ+2119r77DnXmHNMCSEgIiLizz6Sp92BiIiI3SA+7BERe4L4sEdE7Aniwx4RsSeID3tExJ4gPuwREXuCN/Wwi8j7ReRLIvIVEfn5x9WpiIiIxw95ozy7iKQA/gOAHwPwIoDfA/DTIYQvPL7uRUREPC5kb+K73w/gKyGErwKAiPwmgJ8A8MCHvcizUFUFAKB3PzJhoG23T2h7oH1hcG3IgAeB2xc6LhFxR+pn8T+ECe2j4/zPZRKoTdd8yp/FfjOR5HX3iWskoTMO4oyzwN+jP7t+DD214ds3103X6eeMr/wh7ww7Ta4Nmpjg9w003jT2SeLHg/tozy3cJn8vSc1xeaafs9y2L9B9SWq/l6YJHSf0dzfgwvsK23+a64Ta+5MXI6+/TYe+cucuzs7n/qYG8OYe9ncA+Dp9fhHAX3jYF6qqwHP/6XsAAIu2M/vqpU5637RmXzLoviXdOc2qNseFdK0fxLbfrnQQE5lfbk9HduAH5JfbZW1/PGRMN4TocU1vJ2Xcl9qnwrZxyD8YZW+/V2pfJNF9uZumItU214ntP3q97oz61ef2sMVCx3jV2TZGQfclg36x69yYDs3l9uB+Z/nGXzV07w12blfNhfYRtpG6obkodOzH09IcN6Lx6dxcpNBxlLG2l5ZTc9zNG0eX2zfeZgcrT65dbh8c2e9NpyM9TnQcjw/cfVXp59nB282+caF9nh6N9Tt1Y44LKd0Hle1jaDf393/zc38XD8KbedivBBF5HsDzAFCW+SOOjoiIeFJ4Mw/7NwC8kz4/u/2bQQjhBQAvAMBoOgp31pu3w3Fv32qrgt54y7XZt2i1m8mwuNzOrEWF81rf9KVfe2SzZ2ATfGwOq+iN1LnXVb7WNnt6u+ajA3Ncv9Y2Jrl90wxkBuZhZfuI2eXWdKrnGlX2Wlr6wZep3Xew0p0NdT+FHazDQsf43E4FwlrHe5Tptu9tt9S39LqZm3110O9NS7UIVo3tR0Vv7LltAgWNXR+0k936why3nNzQD+6+CmTp5EEtgnxl57Zd6HicnNk5u3ag++qV7X+/0mNvTHXfQmw/ylLvzTBfmH314eRye6AxPcDIHNdMdAay3s57n2378boG/AZvZjX+9wC8W0S+Q0QKAH8NwEffRHsRERFPEG/4zR5C6ETkvwXwLwGkAH49hPD5x9aziIiIx4o35bOHEP45gH/+mPoSERHxBPHEF+gYEgKyYeO7nHXWuVgszy63B+fXMaXGfke9dl5kRjROa/2uolVfKDtUHzvt7epw32obMrILigktRktJ5+qtDzaa6sprmdqV45xYguuHdsW2zPVzQn76yE1TdVPbTMT2fyiu67mGpfaxsePdTA71uM46y9WBjl3X6/eKwXp961T90Cq41WehlfoL9V/L1K4w1yvth0yXdl+vx/a0fpK1djyapX4vGdnrrAry0zttYxjbdaFVr/uKuW2/rnSM+7m9r2YjvQdfpq89k9p1nFLU/14FO95lV11uc+ut2OPYT09bT+1t+xEeTD/HcNmIiD1BfNgjIvYEOzXjhwCs6o2hUnXWnGta/d0JjTVFKB4By0bNpt4FeaStmmZNOjH7Subp1mSCd/ZcCVndycKaekOhZn2BiratSZURzeViHzA90O/NXNzBhD4L9WM6tibhhCK8xFFqCVGJSa7nateWChLo2C2byuxrOzVbW+g8TRo7ZyvqftfaYJMWFPCUq5uTdkfmuOmF7hsaFxCz0NuzpmCkeTg3x5nYpJV1m9bD3cvtBZnSpdgxzaj9bGTdsnShbR7O7L71Qvs8IaaMA4IAoCv0fixHLhis1z6mndKI9WCpt4yCjvLEjlV9/+NDIhnjmz0iYk8QH/aIiD1BfNgjIvYEO6fekm0444ULic3JxxsyS+OckqvYM4XmEmHYX62cL95NiP6hJJb1xPq8s5RCNCvbxrhVH2pKCRY4sOsDtzL1/46m1petbmpI7FFi/csZhU1miZ57Ora+G0V9YuSovclKp3Seq18umb2WBSUUZXNL30mvlE+31vbXqzNz3I3sth43c0ky5zq//Ur949t2eQB3KSR25jLKXntZx3ix1j5Ox3a8awqRXTb2vgIl8iDVNkJtfe9VSmG11czsWxM1maXWKc6oL/mJ3o/FoT3ubE3n6+w9UST6zs2Issyu2fHgDEefEJdNtmshLpuPEd/sERF7gviwR0TsCXZLvQ0D1tuMn9Ba07Ehs7htrHneUJoXU3Rlbn+r0oHMqNK6An2p9mNCOeATJ4AhjZpz47G1OctjymeHtnHofjKnlPM8u2EpnlvVsX7vpjUXJxSVNyLqsMwtbTYdqWndpzZr76DUKW0zHY+utuZdx9lxY5eZRzTRnCz3JY7NcTlFkzVizeemJAqQ/I4ht8cdHeu5VnPrCsyKk8vtk4WOwb1ze39cUCfT3kZV3qWstzLX74mb9/npPf3Qu8y2Y3aBrAtxTMfWRM2m7T1zXFi/7XI7mdk+XiNaeKCIyPW5dd8OpjoGNexYSbfNBIwRdBEREfFhj4jYE+zUjEcIQLMxRerW/s4EWpXNEicLlJF4AK02FsGuRPeUPJIM1gRPWzKFSUwhH6w5VJQkf+T03WaZ9iubaPvHExtB97ZDNXcPjq+ZfTeuq+nuV9IPJ9r+qNJ+TMY26uyw0GkrRtbkbGi1OJlrkkmfnZrj+qkmzJysrGmdtep6TCrdFwbrTnQ3KcJtYQUlklzNyYrYlGVt23iNvJxmYhM/JiM1maszNemLzl5LSpFmp7Wdz2sTOnmj7YXK3vp5z26kvZZ6pfN7+tqJ2ZeOKZqRxqftrQk+UJRcNtj5bOgxvDfS/r/twCXknKtrVI6tGxzuRxsODw6hi2/2iIg9QXzYIyL2BPFhj4jYE+zUZw8S0OQb2mHthPtSojDyzFIrbUfZYJn6RSG1NMOI9JJl6jKLaqXKBopO6wbrb48y9aOnTuhxRB9nRK89O7H+2TPXbl5uHxxZ6u14on6jlxu+PlFffyjV96pmlu45IOpwEew4Xs8oaq4g/3ht1wcCOdKzle3HoiM/PdH+J47WakgQI3fX0qc6Fwv6WpHY9YHJuUaWnVTW3zwg4YnqgNZqUremc0piIU7g88VTEqwgSrdwEYU9RT0uWpchuCARkM5FxtGwZiQ0euBeox1Rdt3CZu29VuqYPCN6X12ULkOw0rWEUWcp124bhRd8gQBCfLNHROwJ4sMeEbEn2K0ZH4C225zS5TygJm02caIUKCkBYEkJM+KqYlQkhLCwZs6IovByqsAxqSxFd0D69ZPKmqbPXFMq6/pYKbR3PGPNrRukcXd9ZqPkZkzVHFoTH1O9niNyJ4rS9pEr5Bwk9ve6yPV7KUXaec21lgQrZq4beUcJOZm236zteHQk+ND3ds4SEtjgyMD7838fZ0xntnY+5zPSsbur/U1dlZ280D4OrkJOTvr1rwxK2d2zzBVKctn62lJ7Hd2sYWUFPNKX1cQ/u0XnThy1B6UV69omwvRzrjRELknmKEAhd84VAgrbegoDYgRdRMTeIz7sERF7gviwR0TsCXYcLgvIltZYO9+iIr323tWxGiiTpyO/cTy2NAOXZguOvsuFLrWnOmROGOKZqbY5PnAZa29Tn/3WLfWfjpzffzTTNq9NnA95qOcel7Z9DumtiELrnDZ8xn66E1NIqYppQWIN3pfNJtq+K4+GAxqfi0bPlY+dX86lowtL7Q01iVdwhlawt1xBFeRGXoCT9OHLMZeztnOWXNOJXzlhzZYox7dReHVY2TGtSYddStt+3mmbmROH6OjebBck4imWpswTqufmhEwrUKYb92lir2WgdZZi7eZCNve7+HK6fMwD92whIr8uIndE5HP0t2si8nER+fL2/+OHtREREfH0cRUz/h8AeL/7288D+EQI4d0APrH9HBER8RbGI834EMK/FpF3uT//BID3bbc/DOBTAD50hbbQbm3GWWZ/Z+4R9ZY7eqagcr0N0S6Zo506upyJK4+TUNmhMldabuL6cUACBEfOxJ+WasYfpUqf3DyyNF9B0W/DyLYxFjUD08qaaUy7rOhaytqOx0DMTeZEDLJMzcV+4H44gQpikIbWaZ1xFBoJPvSJvZYJl0XKrBlfU5RboFrM02Cpq2qqY1c7+q4b1KidjfW4/CVLZ36NKMCjm9Z8Xix0TF/OdP6K3kax9eRWlk6/fhF0DLyWe0VRhYsllQJ30Z0luXpp7ng/mt9irHTbOWUtAsABCX+07tFNt6W5HiIb/4YX6G6FEF7abr8M4NYbbCciImJHeNMLdCGEICIP/EERkecBPA8ASVz7j4h4anijD/srInI7hPCSiNwGcOdBB4YQXgDwAgBkWRrCNuptvbImbEpCEWVmfzuWrZp3CZmcXWpN9bxVcyudWrMyEzWjpiQffTCyx40OVWxidN2at88caJ+PSPwgdZVDD8hNKEfWNO1JmCNfukqzM2qH2IO1izY8JrNPpnaVnc27nlyGzFUOzam+lG+/J020Ca0UN5k9cE2sSVHa+awu9FrSsX6vz6yYR06S4plbwZ5SdN0FmfjFdesKPEPJQMOFc1dIIHB5T4Uncic48oq8drktjXXLejKOZeFW+8cUpUi6irK2fWyX6ja0pZOqpujOhMZ0NLIRdEeUPLbILANUbKNMB68xTXij79qPAvjAdvsDAD7yBtuJiIjYEa5Cvf1DAP8PgPeIyIsi8kEAvwjgx0TkywD+s+3niIiItzCushr/0w/Y9aOPuS8RERFPELvNekPQEsCp9X1Crb7PsrOleTgoKCVKIzhnM72mtEjmkvjLA91XkQ95cGB99nKqPs8YLtuMtNCF6KnJ0tU0OiKfunXZd+Rjh8R+bz3Q9zjrzdFmy4F8cUeH5aQxPwzUhlvfCLSmmmRu/aRXP1oy5fnEi3OyDvvSGok1CYSIERC1fvmItP/XtZ2LhMtXUX3orHL68sd6LXmwGWWLQ81Ku00CEq92lv5aLKj9wrbfnGofLxKX9UbltrKcyorNbQTnqyPSkV/aSMGMMi3nRAFO71l68IL0/CW3azA3k+06QCzZHBERER/2iIg9wW4TYRCQbimU1iUUhJWa7rWLaqvIjm+hJk/pNOKyRk2zycgmmUwpUWNC5taotP04pEqwpUtiOS4oEaHQ9jOnv1aRWJ0ULjqNIst8VFsXtJ0puQn54PTjRMegDk4nr9c20oQi3JzG/pLonlFuTfw10aBDRxVG4TJmOh2P3l2L1GquV3RdayfqUAeiIp0WekcJI+VMaajswpVnIo2+zlXvPb6mZv0qUATa2ro/NX1+ubHmcz4iU/3cXmdLEZ2LhY5pVdrrbEkPo5Gl2SeH5H6uyRXIrTtxevLq5faN8dvMvvV60+chuDkixDd7RMSeID7sERF7gviwR0TsCXbrsw9AV298nNpxBAllm5XB+m5rTsJK9EPvNMhTCkP8E5qVpBN+TIIPlaPepsfqP113fn9JvvmUtnMnDJFQuWhXUsxk7RWd9T2vUbZcMyLf2zaBnDTDgxPFbAcdOy6VnObWl01JRCK4bLOc/L52QqGia/tu6HP1UUNnacSMBDdWtD4zOL32gdYLmIICgITWTzjquB9Z2mm5pFLXIzvg16Y6h3dJsKI6ckKdtD6Qzy09mJzpGITSUm9CpbADCY6sGyesQvfca3MraPk2Ctm+SLW/obb3d0mU8b17dh+ubdp/EuGyERERf8oQH/aIiD3BTs34QQT1NpNp2jqahbKrlpk1gTIy3cuaSjtPbCRSv6YywTN7aRUxLdVMyxVfz2yE27TSz7Pr1sRnwYo0p6y3ypZnSlln3OmqVVRuOe3sPjZpc4qmC4kdj3qsZmbuqCb2DDLKUqsbey6SWkfb2GivkjIEE8o4bBI7HkIWbejtfPZUuihtlVaVte1vkqhJ3jk6LCHtt4JcgXnhBEHI0RFxYzXT87291e95E3mZ6zWfH7j7iujS9tya8edzKidO7ufMRUeGXvuROM3Cek7XGZSW87TqBfVDxNKDy2oz3kMs2RwREREf9oiIPcFOzXiBoNhWWm1dIkxLAhDZhZOSJnuR8y0mS2s6zijZJSvtKvWE9MemZGKlI6vzNaNV9qPcroOPaOU7IUHdxEks59R+OzgJ54X2ubM5G0gowWVErou4SrNo9LiVizZkZemGbPrUjXdP4gpdavsYVtpmR6v4WWvN/XVDEsuZc0kaNc+FKvT2iTXjs5bmwglgsNx4oFXmxr2jKoqIbJcuAYrkmIu5mr7TmZvbG9r+oauQulyeaRu9ddly0uGrSVTkxOnMHVFkY+dYjWai13m30zG+NdiyYsteTfxqcAIbq+33ohkfERERH/aIiD1BfNgjIvYET0G8Ytie2PqJRa1+r9MwABbqi+dEO2Xi6DVyY45y2/6IMuQmFLF0PKrccXSuwvpMHUV4VVQCWVorIBhMZV1L1XTQc4/F/tYmVEooJXGJ4GR516m2MfKiFFRGC0I+sKsK1NAaRprZNQfpKcKLxD+D00wvSvUvOxcZx9rrDVFSXuufqaZ17zLFBu3/uKOIQicu0ROtVTjKdXKu/vyYBUwu7LXMcp20i5kTrZyTz75y6yeDXk9FQhbzzq5vDHTvDO6e6FsVthgLRYE6erokOnbV2H0X2/WTPkbQRURExIc9ImJPsPNEGDQbE6y3TA16MlsLl0TQkZ74iKyX3FFXPdE/xcQlCmRqkleVmsH5xJpbBSVOJJkVGcgPbmtzTMs5bbZ8INcgtRRPTn2Uwf7W5iVpkJM1Ji6CrupZZ865ApRg1JJGXFtbqqYkAQVf95NNwTDkDz4up3HsrGma074wV+GJhaPoctL1y5xpWlGCUVZSlV9HNwaiodLGzllG+nrjiZru45ml0KpO3ZpyYfsxJqGS8/zE7KtIm/90pOcaOZqShydprevIcng1JRDlsNeChV53Utg2mq2bE4KfJTrvA/dERET8mUJ82CMi9gTxYY+I2BPsNlw2DZDpxqfoXa031rIIwdIiHWnA10Q1HSTWD2VXbnC1sG6MSeCgou3MhsuOmf45sJSaUBYWyFdLKuuHdkTzDa2lQoy2Y+n8baLiApWtDm59IyUBylA6Gor87aRTX3OcudpjpM+QjOy+Iei4JpxV54Qp+dQ9bCdbEpQIh9rfo9b6lOzDJ4m9Jzh8uCXKLnHhw3lHawKOjpWU6LZU902d/nuVqH9clPbeGVP2WZnb9i9ovAcqR110to893dJ5cLr0RH0KCY4EJx65XOq5c1cSWsJmLaHv34TgpIi8U0Q+KSJfEJHPi8jPbv9+TUQ+LiJf3v5//Ki2IiIinh6uYsZ3AH4uhPA9AH4AwM+IyPcA+HkAnwghvBvAJ7afIyIi3qK4Sq23lwC8tN2+EJEvAngHgJ8A8L7tYR8G8CkAH3poWwMwbLOSstSa6tJyaWBLfaRkKo24fG5hTZlxofsO3L6EqKDDEUVVOe35lKiakcsUy0i8gSXXZi6TK+3VhGtc+WlQ5lzq2EGmH/OZ9qt2gg+Y6Pikje1jR+5FSJX+qVt3nabskhPfyKiMMn0vae25Coqamw+uDsCIdOdIX3BILSU1JkGG3lrW5nwZzdO5yyTsSDijcu4KZ+0lTNseO+1BKgWevGQ53Wqq9+poYSetLym6bkG0p6MYU6JPL5xAYkUlwoqWxDBclGlGpcNa2BJp+Wp7PY8r601E3gXg+wB8GsCt7Q8BALwM4Na30lZERMRuceUFOhGZAvgnAP5mCOGc5X9CCEFEXvcnRUSeB/D8m+1oRETEm8OV3uwikmPzoP9GCOGfbv/8iojc3u6/DeDO6303hPBCCOG5EMJzkNc7IiIiYhd45JtdNq/wXwPwxRDC36NdHwXwAQC/uP3/I48+naDf+i7l2votSxZidNQEx2kGot5KcaonpIU+cpc2nRANlaqPOs6t75YKqZ6I9Zko6Qg5+Y1tafsRyG8StzaREfWUBUsdNvQxELVSugw+qsqM3tFECevqE2UZxI5pR9RnNXJikRSuzGWT1452Gtbka/o1ElIXSoiG6pw8T94q5ZVV1jgMVC66obWaaWrDSMNY2+9dFmNNWWpFqdd5uLC06h2i4saZ1Y3PKCMulFbzfUyioee03pO4ktCrVvsxces4AarqU1NGY97YORt6qnfg1nEWWwXRPjz4jXoVM/6HAPyXAP6diHx2+7f/EZuH/LdF5IMAvgbgp67QVkRExFPCVVbj/w3wQAP8Rx9vdyIiIp4UdhtBJ4J8a6b0laMOiK7KXalajowjFuRPRjpRqeTxgd1XcIbWlGiiI3tcTmZ8lVr6JCETdKBSzIPTbm+NS+Ky3sgNaaf2eyPKZEpWeqFJYekqzu7rXCRVKXqdda/fy1w/OmLbQuuizgY1kzsqORTW1jQVippLnPnYCGu5q3nbV66/Kz2u65zgCFGANc+Fc6/WjX72Yh4DuXoTkFswtrrr3I985ty3u3o/5o3dJ6RZzyzuvLamekXiFb2bi4ru/Tk1kgZL803men90sHRpdj8kcohZbxERe4/4sEdE7Al2bMYHZNsIp87nwZAAxBBst3gxekwfgktAGVFZoNHUJrhcp1JIRaq/cXnn9N1Ibz53kQNlpWZVUmj7iUt6KIQj6Oy+FSVq5GKZgAsyJadTiigM9riB2igTa+JnFIaWJnpcX/iEHN3XOHMRJL7BzY9daSVknKhiJ3TUkMk50nOPB9vfmqLr8tzORUvvooYi3MrEVnElvQe0LvoSzE5MtU8SbD8SWpZqXbRhQkk4LAgCAAPdSyAWaW2JFrRUfmvmmShyc8Zneg/LTXt/rxNlAtLenqDaRlUGiWZ8RMTeIz7sERF7gviwR0TsCXZbsnkIqO/rWzu/qCDaKYP1uxIKux+omNmhcyGrsfo7s8r6uckBiQIckKhkYn3ZgqkLRyete/W1Qq2+26i1dExLtJMro2b0J2tXlyzrNXKrJv3wJLgswIn65WFtqcMlXU4+ocy52vrUOfmaWeuE+mmdoQf5qGL9xIIopLK2fWwo+6wiUczOrZEQ04nBXadwTTSK0KsddTWQuMR8ba9zWGo/6rm2sXYpdhSchsoJiKai91Xp1k/SVNcPEro3K5d9xmxh5qINaypVvaLwyGTlsjpTpmPPzL72dFOGPPQPfn/HN3tExJ4gPuwREXuC3erGB9WeK8YuuYMok3Lw2uJsEqopOT6wSRU3D/Xz9QMn1kB2VEmmeuc02TmBI53bPvZkZodB6aqFS1QpSMeud3p6h2PdJ6Wlcc4pAeUw1340rR2Paa60X5t66o1MZoqMS13UGag8UTp25jO1IaSFlzrTd6B+tYU3fdUc5bSS3Jm3DUUfLjKXaEOUoJzodtvbNhYXatL2K2vezpcs4EH0mnNrTteqY3feOr0+En33iSaBBCZaosMKWDqzI9egd48da9unRJ05VhVcpVlcbYVytLlujlb0iG/2iIg9QXzYIyL2BPFhj4jYE+zWZ4dAttrdQ2fpE4p0Rel+g2a1UmXTt6tfXrjwyirVff3C+srpTH2hlkQBSlc+d7WgLCyXbRZeo6y3qfqr65UN38xI13wlF2bfmrTiW7GZS6z5PV/SvsT67PVSMwZHuR3HLGMfm0KGR05ggxRBxk7AMWFBRNK9b0p3u3BWnRNRREmhtDWJRTrarF6QTy1W7Oj8jl7n4kLH+GJlMyaHTvtYOp3+k1Z9eCHq7dRF1ba9ZsEtz+yczYOuOiycKEWz0vtHSHh0WNn7Ks3V4a6dghtPTUf3x9L57CGhkPKVpUFnxebeeUjF5vhmj4jYF8SHPSJiT7DbrLdEkI02JmMqLoKOs7UOXdYbUS2HEzUDZwfXzHHVhEs32ciyjDLABmInzk8tVZGSybZa2n5UR2pvCVFGnJEFAAmVDQ4uu+rVtUZgtb3VOsuhfXnt7NXL7ZkzOe+O1Oy+fmS11Iqptj9aqdl3dOgyBDN1c7rEarolFFHXk8b+qHZ0EkWudbkbR7K000RN5OQ1ezHnczWzz+9Zd+jlxcuX2+s7ZMY7rfxA8z5ZuTLbVCKsIh3CG8fW3n35hCLXnJmdXNzVD2srenFB7lBKGX2ti+CUhjL/antfdXToiLTlGrHuSn+u13Iws66GFNt9D3l9xzd7RMSeID7sERF7gp2a8QkCZtvotcbpuwXS8hq5BPzZWGtGFpRskKR2BfhgrKb7aOY01yhSrinUZJum1mRbpmoqXRu5MkATkhSG2l4T2H60a10FH3XWVOcaR6/Mrcl5IfSZor0WSzse657NXbvqO+m1zwlVAZ3nt81xAydwjJyJX+oY5J2a7qGyDEe/psg1se6K0LLwkpJTTgcrxby60PGZX7xm2yctvDWN8XRkzfhydl37/rabZt+YIhiTQ3V/ZmKvuapobtdfM/tOl1rsKF+4+Vy+otu1tt+fOROcTpdVlv0YE0NRc6RdaY8ruZqxC4gMi417NPQPXo6Pb/aIiD1BfNgjIvYE8WGPiNgT7FhwMkG61W8vxVIwBak8FL3Ta6eyPZP0SI8b2SgiquqEo4UTOKAsuHxN55ra49Ij3TfGkd03pTYoQqrrLM3SXFO/buT0vdcrbf+Zif3e2Tlp0VOUX+ciBY9Ie11aS4e1J3rsqySEKekr5rhwqOsbVW1pSpAPmZYkCOmi5IQUOavW+vN1o2sCbaPj0br1hwXtsysYAGodn4NDphidvjxpsleZE+cksQmODFy5EkzZSNeFquk9sy85VLqtumevc9xrX05IRbXLXV0Eqh3Wuii/i0HHakzPwbi1mW1CZamCu78xeqbzxjUAACAASURBVPSj/Mg3u4hUIvK7IvKHIvJ5Efk7279/h4h8WkS+IiK/JeJzKCMiIt5KuIoZXwP4kRDC9wJ4L4D3i8gPAPglAL8cQvguACcAPvjkuhkREfFmcZVabwG4LDOZb/8FAD8C4K9v//5hAH8bwK8+tDERyFZrrnQiAKNSTZu0sKbShLTlcqK/rh/Y7l8PZP4707dISZt7pvsGJzyRLNSkXY6tKSbEeI1LjcbKnaZY2WkbjTNbh1ZNwja4MkOJXmdRqsmZOB39MZXHqnPr8rB2QUn6el3jqLEV19Gy7TcUQXY8pjZcNGDf6skGl1lyTtGAzam6PO3attES1VRldjy6Y53rhEQ0Fmsbacdafnlj9y17dV8OayrxdGDN/RVRVpnTdZ9QKadQuOQr0s7Pc6UiS6ch0VZUE8Bp+LckMtJSLYS8tudakoBh0doTrLeaguEhVVyvWp893VZwvQPg4wD+CMBpCJeq+C8CeMdV2oqIiHg6uNLDHkLoQwjvBfAsgO8H8N1XPYGIPC8inxGRzwwPKToXERHxZPEtUW8hhFMAnwTwgwCORC5rCD0L4BsP+M4LIYTnQgjPcRmdiIiI3eKRPruI3ATQhhBORWQE4MewWZz7JICfBPCbAD4A4COPPFsSgPHGf5s29tQJlUC+5upkTYlOqShbq0rtgZyFdWBLvSFJ1I9Ocz331Pk45/SxcxlxgYQhVvjm5fY3T6y/uiZRh/rC+agr/bxKbOhoMdA6AAlyHkyvm+OefYfSRNPSUnsliUAKiVYmg6Vqlr32uXYhvde4nt6cSi+PHPVG7vFpbinAhgQgzmqi4Rx9l5Mwx0Vnx2r5ovrbr1Km28WZDaudkzZ852jbotQJLSqdv8OJo23pReRDTk9pfBJHOqVjvTfTJY19YfuRkWjlurUWbkUWb0L14iSx5zL5jY77yrbrOCIPDpe9Cs9+G8CHRSTFxhL47RDCx0TkCwB+U0T+VwB/AODXrtBWRETEU8JVVuP/LYDve52/fxUb/z0iIuJPAXab9TYkmNQbszO/bk99jcoBJ6XTVSOTlhKykMOVbiLTZhhsVNjBWL/IGuqdW0dYkCm5WliTs2lVxOAlMmFPXrLRaa+QBvnqxOzCotMv9sGZc4lmrF0bab+e+XYrVFBQCeRqZs3R8VTbSEt1eYKLGAuk/daN7Rh053rd7bHu48xEAGhorLoLR4e1Gg+3oky/3pVnWlPS3qvftO7Ei5Q59updjWo7f9Wa8Xc70orvrFuTUmnqKV3neGbvv2dG6vdNEnvvJER55Y6mzKHmek9md+U06Fqi6JDY+2pFAhsjMuNrV9OgJZp17FyBYZt1+LBF8LhiFhGxJ4gPe0TEnmCnZnyaJZjc2Jg6o8GuYPcTNT9vuG5NaMWWBSrKwkZBZbkmrmRUqRUAEtKgQ6rfu+is+Tms1JT85rktJbQkU/Kr31T7fOWimS7man6NDq2ZvSTXYHAr5MWxjkkxVX29JLfX8rWvqTsxP7L9f8/b1bS+detdl9udK0e0opJDI7Hmc0IRi6GmiK7BXucZuQbBVWddkg5y11NCzmDN2zlFETZOB3lxpue7+6r28aJz4hVBx/jt3/ZOsy8lk/xrX/+i/t3pwK2PiYG4ZlfSx4GiKnNrgo/IrB+RuX+S2j4Kv1cHV0KKIiRzCoFMx06OmiSzc1cqa7Sdi/TBAXTxzR4RsS+ID3tExJ4gPuwREXuCnQtOjrHxS0sXiTSbUldcFFQo1GeakrBC5fyWqlKftwrWZ5pOlFqpKXNpcu58asoO686tD3l+RmWDSXzjGRfO9N3/kfqN3zizUXLNoOsKtfOvnrl943L7P372XZfb125YffyX/ujLl9vruY06O6WIvWtvp/UBWEpm0lG5Zafh31KJozGVfLpfuus+Cip3dJrbi1mtNLtv3en41M5f7UiL/mB2bPY984z2gzXUs7t2rYPXdP7cO77N7MtvEW1Lawyv3nnRHLegfbddJmFxpHRec2599pfXutZUUdRgvrZrUrVQpJ0rCVZSuXJQ5qY4mpKDTsWts2Rbqvkh1Z/imz0iYl8QH/aIiD3Bjqu4ArJNVuknM7dDTZZqart1QPpaJWmcp6n9rZJETaXpyEZSBaq6WolSS/OJi2LrVXf8WZdscEyUzDFFzYWR1apjjfqbxQ2zT6DmbXFsRTpuV0qxzW6re9GsrYjG8TNEy62clh/p8pWljkfttPhbYnVELMUzkLDFmro4zlyJJ9LtzxunhUeu14S2cWbNz/NWr23q9NTf+XZyXxK9zqOJbWM01vvjvpt42S+iQY+/TROKrh3bmgDIdAxu3Hqb2dWv1TieDdbEv06Voe6UGtl3MHHvUYpsC711ZYqWkpISqgrrxDw4gSssbT9W2/JeA96keEVERMSffsSHPSJiTxAf9oiIPcFudeOhmWplaUmCAwoJHeWWDpuSAGVG2WDitLILyparSpu5lFOmUcG1zRLXhuhxs1vfZfY1Mw2ffcd1rf/VujbmrdJfzcr6UN/57bom0DvuLZnodfYdXUtlj5ty9tPbbW2zg5tUZ26q7ZVnrqbdlEoDO4HFABL4pCysrLFjOlC9tNT5snlKdfcoM+zglj3ucKVrK6ypDwA16aRPj5SW616zZZP7goQh4NYVyD/+c5WOwdEz5jCESu+53pXIbif6h1Ww6wXTTGhb125OMleOm3Teaxe63NF6ykB+eVdbijEnijS49ZN8G3Ys0WePiIiID3tExJ5gp2Z8QEDbb8zCG8H+zmSU3B96a6aVZJ6HmiKWXFlmoeynzmVXHVJJ3oFMr+OZpc2aGVEkrixSOtPP87n2v4GNiGopsy1zP6fznkoruX3rBWWicUKfEyrIRkpJHVW2jyW5AtNEz7WcuPK/pN+eOYpHiIZqGoromllXoOyp3FFhx7tY6rFCPF8/tnTj9bFSk72Lelxd6CAIxYa9OnU1B6jM1ZDYe6JM9DqrQa8zda5XN+hx88HuaxrNcKxyOxeBNO7ynIU+bKbiUGh2Ytfa9ku67oIoxia1bQjty4Lv40YsJIQoXhERsfeID3tExJ5gx2Z8gkE25vRQulVwsmlbJ5KQtLQCPyF9NNd+Xehf8t5GSPWUXMNMQFlaszKv1QzqXKkfjgo7aLUffWJdgREJHFxk1qwaBzqf2weWteZ+DDbZpaDEj9mhm0IyVQdaOU57Z36SEELlboOBzHNhq9iJbQy9jk+ZOsEHKj3VEmtykNtz8QhPnfxycaxu2XpQV+mdnXOvKJHEuwIj6nPZqlvQTKwrsORSS3PbxpxYkxS2j0mvcz0tdJX9bmXbyJb6OTu1Lk9DFXVnVPW3HznXiFbaxUVOyv2JekgmTHyzR0TsCeLDHhGxJ4gPe0TEnmC3gpNhwKTZZDmVg6UV0jVrc9uMOM4iw0CcVGcFJ8tG/eGx82WzVtvoSDijcD5kT3505aKRyo5ouSMqs9tYXzZQNFkh3mfXz7lrf95rvxIaAtYt3zTKEW7293pMlMw5+au9LM1xTFH1rRXWLEmMkctRS2rFLVMOvHM+e0NrMqEhX7O115yOKDots+ssUxLcGOWasZY5wckFhbyNnOJi2qofPVAmWtnbawlUGgq9bf9koY7w2M1ZnlDZalC5rTOnG7+izMWxE2ehmgk1lbBOYe/vgUQsxTaBvtmcL4QHO+1XfrNvyzb/gYh8bPv5O0Tk0yLyFRH5LREnPRMREfGWwrdixv8sgC/S518C8MshhO8CcALgg4+zYxEREY8XVzLjReRZAP8FgL8L4L8XEQHwIwD++vaQDwP42wB+9REtAdtIuWaw5kZaqck2qVyZIaKhhomaQ+K06hrS9mrdvgFKb1ynKDamoABgRKZT40zkEVjznWxYfy0FRUQ5g6cnitGX6glEMaYUNZjDJ/VQpF1lTb2eNPrKU9ZudxVphWk5O1ZMYXZEI06cTn9K9JqsXSkucrcG6m+dOMqISyY5wYd2Ti4PmbBJaft7k4axC97lIbN7TW5YYenSlsQ3pLCJKgWVkGqd+Sx8v3C4ZGLntqZ7JHf7WqJxG3IrC7GUa1rSvZM4Wi7ZJEBJYhNwGFd9s/99AH8LuHSirgM4DeEyJepFAO+4YlsRERFPAY982EXkrwC4E0L4/TdyAhF5XkQ+IyKfadzbJSIiYne4ihn/QwD+qoj8OIAKm5rwvwLgSESy7dv9WQDfeL0vhxBeAPACAMyq8mFKtxEREU8QV6nP/gsAfgEAROR9AP6HEMLfEJF/BOAnAfwmgA8A+Mij2hpE0FwKL1jBhIzotSZYeuOQfiPY42s765+UK+Wrzirr7xyX6of1ldJQSWvpnqwj382lrAkJHOT0syWF89lJXz1rbTjunMQgElfvrqJMLPb0O6djnuXUZmF91G5JpZJLXd8YXHhlSlZW6+qSjXu9TqFw0GJur5MpUXGZYhkNUEP0V9dYyhXURnlm6bB2qqGjaUcCJqUV4OxoLUG8GH9K5blzvc68tj5v0lJGnL11kFLm3GRw2Zp0HycUjtvntlZ3k9M8DfbeF1obKgYSMOntcSmtzySu1HizXYMJeDJZbx/CZrHuK9j48L/2JtqKiIh4wviWgmpCCJ8C8Knt9lcBfP/j71JERMSTwI416ASyLQXUuwT+7kLNz8nYar4LUVkpRTMFl7G2IJGEg8Ga5yVRHz2ZUU7jAmmp5m4yOMqLnIhAIgxjZxKuKBtsLk7XnazMbrBaexmfjvpYFHasuDRwAxsZVzAtR+b4uXN5OHpPcnudLICQENfUHbg5I8s0c9pvgXQED1Y69mvn8oSUaL7SznvO9GOp7YfBRZYRlTpytFZKYhPDQudl5W3aUk31obXzWZM4RtNYGz8nzfqyIJN+bftREc3qS3GNKVPRjGLvREUoUrBurIlfbvXpHlKxOcbGR0TsC+LDHhGxJ9iteIUM6PPNymyaH5p9PZVhylJr5nAkW0oRaatza8LKWFfjl8EJBPAqO5WJKsLCHUfiD67iqFCkU9KRoEZlfzMHEqiYuRX3FYtSWKvVSC4nlKiSJdZk62kltnRaZDVJYbdLGoPGVQSlCqGJi2obUemsLtFxK9bWbTKJH4M1OTOKZjydan9HrWNfiXnJejeONQk+kOuydr5XCOqGNCM7Z9lCP6+pxJi46MWU9AaT4JK0UmJyxPV/oX2p52zG28P4yuranntNLkpKfSxLO+/rtY5xXtjxHi7HMUpJR0TsPeLDHhGxJ4gPe0TEnmC31FsIyMPWmXFUENYqTtDNXOQaUS0zdkkOHDVB/mrptNbXF/q5GBHN4vyiNtMyQ+OR7aM0minVjck/81lplCmGzPmQGZUXdtlyLUXKBaLQChfhJiR24MtXBRLwqOs7l9t+DaPj33mnNT5NOSuQaKGZPU5WJHJROl16WmdpiS4dCucrU9ZY39o2Dm5Q+9SnzJVFWhIl2rgIN6TafiAxyrx3JaZTXbsJg13HCR3NtdOl52zKQPdfUzh/+4QoXbcGU3C5LVrjKZ2wSk3X7eXh5WFKk/fP+8gjIiIi/kwgPuwREXuCnUfQpVvN77azZuU6qFk1ya0G3YR0wVmj69CVZ+onlDDjEv9BEXthqdvLxEbapT2VPkqdiUzVVAumbpye+gSsKWa7MRmrWVn31vTiXImcTPUhs+1Lx7/R9ve6g+rJ1UTj9LXTO29OL7dH+TWzr6GxmpC+euhsPxISngippUGTTOmrjBJQss7SfAm5UcnYjkdyQeWUDjiazlKAJVF0k8RRjGSS92SO14M143sywZPaJQ3RGNdr2/6K3NGekoEWrg1mxJrcUntcJ6FpiNLtHfUrei312pXAup+09Tg06CIiIv50Iz7sERF7gviwR0TsCXbqsw8IWMnGVyobS7N0c/VHzka2Wyn5xKNr/D3rEF+s9XvThd23mFKoJNFh46XzzyizqwuuHHKifmlGPukwsr+ZTJvJ4GJiTYN2XSFp6digvnfihD6MRrvTrE9PuVw0hVeKLYOdphQSe2D7P6l03SIrb15uj0bW7+9ID74cWz+0eU37P6L+dk5HvyaxifHI+uIgX5/XUmpH0WVEzfZe0JK11lfqp5dOCLRe6lysnb+9WKlfzmHXAHC2orUKWo8ITviS9UcGH8otJF5BlGCS+ixDqkfgFDYm/WYMkhguGxERER/2iIg9wU7NeAwBYasv/lphTaVxpxTYaOkUJSbazYbMlKG1pkxF5rRX4uqIiktIBCA4fTdQeaI+tSabECXVl9rHsdNdD5l+Tl35aVavyHrnrrTqynREwbDJDfgyyjbK75SorHChfRxctplQdFbpWKLpRM3iCd8ijvIqUqI6a0cjkqnKSWRyYuc2I3O07y19V9O5GxKeKEZOw40EK0ZO342k58GBa+vGmdLURu3bIA29ZmnnsyYXK1DUYPBSeNxmYu+XlLQOhYQyVqmdmJzKj/WFnYtmG1IXHhJJF9/sERF7gviwR0TsCXYrXhHCZbXJ0dyuVl6Q9HDihBYKSmYoyFRvXfJFS2IKeWL3pS2JH5CgwWhkTaWOhDOKxGqdtSQDjUaPawtrfiKo2Z27Cqwdh9Q5s7Xt1aTl/geX1BNoBb6vrasxJVNvQaa7G1LkK1qdv2WFRObURkKrz8nSmreBTN+7J1YGWhod10Wvssoht9eSrMn0HSxjkJH931B0Wt/YNoZApaYqu69b0yo7VXiViWMFNKAQSyfhzEknnYvQ49JZKzLpOxfJlpBrV7rHrqXoPb5fUnGlrHK6bzv7nk4677j+ScQ3e0TEniA+7BERe4L4sEdE7Al26rP3CDjfUhBlZ2mz0KlA4dpWzkFTqZ+0nhNNMXHlkFl40GeUEd0hU/JlHcs3Imqv84FrRC8JKQp2roRUTyKWael9KRJycH4W658vEqb2vJ+obXQue3De6jiGXPuY55aiGyijatFYvz+bU5bhgZ6rXFo+adGRlvvSRZZRFmNV63gsEquVfy3TOSyCXSOpz/XcxVTvl1C60k0kutkv7PtroGi4LmgbK3fc6kLHbXDZZjVd9/yuXeOpKett4PUYN1Yc6Bhy2/8RidjXE65pYB/PQ1qj6l0bQ9jsCw+JoLtqffY/BnCBjYZ9F0J4TkSuAfgtAO8C8McAfiqEcPKgNiIiIp4uvhUz/i+FEN4bQnhu+/nnAXwihPBuAJ/Yfo6IiHiL4s2Y8T8B4H3b7Q9jUwPuQw/7QghKSVy4iK7uVM2h4BJhajIr22tqqi9rqxU2pgqhdWEFGUaksx1q/Y3rxJpDC/o8Hmyk00D6YCnpxwUnyMDBgUwHAoCQ+9K3llrpSUutGlFlUldyqM+V2gtO/7yc0BiQZZ0U1rxb0s985ZJ1WAJ+3t3T9pz223lN0WOO20syve5F0OuauASUkDCNaK8zI3drRfTU5MKa+z1VWQ1uzsJKvycZzdnSR9DpYF2QSQ8Ar/ZKK94lcRAASFuabLo/gvVWkJAmfp+7KDcq8xQafQ6qiZ2zFYXluRwqrJNNmw8j4K76Zg8A/pWI/L6IPL/9260Qwkvb7ZcB3LpiWxEREU8BV32z/3AI4Rsi8gyAj4vIv+edIYQg4ktlbLD9cXgesDXYIyIidosrvdlDCN/Y/n8HwO9gU6r5FRG5DQDb/+884LsvhBCeCyE8l8SHPSLiqeGRb3YRmQBIQggX2+2/DOB/AfBRAB8A8Ivb/z/yyLZCQLqlQgYXTshlbJvCCk6uyWdfj6n0bW0FExKiqKRzIawN+WvsJ46sH1oQZVS7ur59qb5iTpRO5jLP2CdLnF8uJK7QOV+8Ix38nMJNh8IJVFAWlpTWf60aDTmtqRRz3Voeccoa9WLXPloQxdPocU3v9c4VZe603FN1WttOtxPnrwbKjlumLgSUhDZHF7rdOU32jr7XuXnvK6JZTykTL7Nj357RcbXthyx0fFonirIiKrWmWn1Y2RdbN9LvBVfTrqUQ2YTqBLawfSxzpXjFhYOXzea6H6YffxUz/haA39kWJsgA/J8hhH8hIr8H4LdF5IMAvgbgp67QVkRExFPCIx/2EMJXAXzv6/z9LoAffRKdioiIePzYcclmoMk2pvZ8bSOujqlsEc6tCXRGIhWTqZrupweWPrlNpqoXjWBZsYoyoxJHBa1InKAsbB8DRUUtx5St5SLcEtFhTXNnLg5qWnsKZkLRWIHM4mSwWnhpRabeymXOtbpPpmp+FrD0Ggcw5i67L6FstjGJLjQje1xGtFZT2rEaUabeiNZuu2DHqio4681dJ81FS4IVwdFOKXlRzltBt9ZzLxI9sF7YRpZrvZfOXUThvTPK4Gtt/3uiPju652qnhXdGPOgM7p6jyx7n6vJI6rPvqISUW/5Ktlp7D1gn3xzzwD0RERF/phAf9oiIPUF82CMi9gS7zXrrAs7ubfyOqRPhuHdB2UNjS2WNwo3L7bNX1S86OrI+zTIo9bEW5xsy9daqk+TFQLJUfbdebOhlRc6hEC23yK0vOyH6bt1YPxQ51VjL7L6C6BQWWxyV1kGbL7UfuRN6HJX6OV2o/3cR7JjOKFGvd+sbga5tIOdwOnaU0aDtF7nLNqPxD/ROSV2pZFMSurDXkpLgYrsiv9+XUaMafE5IBj1pz68v9IvnS7vW0RB92tf2vmqImsychn9DSjUtqcf4enEsoNNN7FrThOjNUUlKNSt7X+VjvZbOPbqyzXZ7WOHm+GaPiNgTxIc9ImJPsFvqDcB9vYO5OzP/6iytxYmTUrOOuHrQzZU1WpbN7cvtAY4Oo7JRWa5m2dDa9KRU1HRKnQXedSSAQSZn6czgcxIxKFyWVA49X+pKMXOAWkEUSu+EOEjXApI4eobOnU31mg8zG23Imvuhsu7KhM63IvM2W9noMZmy2WoHS8g/6kixceGvhb6WtC4zj46tyEANzvcKDVGArmRXLWSeU1Ri1dl+XNC5h2DnbKBSTktHy7W9tn9B9Jqr4o2W2p+JvfmHjKJHiQIsD602fEr1DobMjtXw0Hy3DeKbPSJiTxAf9oiIPcGOq7gC8631UbgoKKrwhN6ZQFx+55lWzexzK1WOs+Wrl9t1+p1m34pWrStKhJHCJT1QSSOfLAESthjW2o/MJXCMaem1T6w+XUftpy6abE2JFKwDngWbqDKQ5traRe+xMEc5p8QPV/VTqFQRfGIJmZnZSPubuwSRkgQ8xJWyqsm9EDKzK7vAjIFEKXzpooLmvSOze3AmeDvTcatghScyqtKbEJtQD9ZEFlCl1tq6NZwY4ypDYUTicmPq/2ps2xiR/n7n3rE5uYQVsRg5PO2gJ0/ECX1sx1983SlCfLNHROwJ4sMeEbEniA97RMSeYLclm4HLEB8n146MfCHxfhG5aCcU0XW8tj7Nak5ReL0ThqDPkjEN58oy95odJi6FKpBQxDjXfavCDmM26G9oVlhKJO3186mjDpOR9qUgv3Qxt30c5kxDWaopoQivOQmEJLXNestYT31ir3PgemNMYTpd94bKL7duTaBd6/cyFiapnCAIKNrQRa61FJXXLyjirbLrFNPm6HK7czRiKHV8Srp31mvb3+Zc5/DUrSssKNuscX28Q3TYPYqg6xb2nqBKzDhwopi58Jjo2Ofu/m7pQcjsLQHcX494yOs7vtkjIvYE8WGPiNgT7N6MfwA4aM4xb1iStTs5V421hRM7OKMSRKuFNZGvUxKBkC5cWFmTKj1U06l3EV0JNbkkfTdZuPJME21zaK0p1lB4oI/UWp7pdKxIE7+/a0MKA5U5xtiO1phCtzLSys/Se+a49IwcqRNLD2ZE/whReye5K4NNmvKd1+sjWm650v6fOkGGliipJrG3Y0o679Nralu//dhqFObkXnS97WNDVFygSMflYO+PkzmVdj6xNnJH9N3KRT2eL3g+tY1Rb+dsmTLda+/bnkqBM2WZZ7aPAyVmpbm9b9v7bpnTdmTEN3tExJ4gPuwREXuC+LBHROwJnprPnrvP7Am5IFUck9s7nxOVVVn/jzUsL1Y2xHRBGvMF/cbdLK2/3ax0SIrM6ZiT056TKEKbuppwRPNVbgFiGOmxi7kLh6Qsr4JG5CWXBti8rHHCiauP9ir5dRWVVMbs0ByX0rWVqQ0xnU40lHSx0n2J2BDTjkQV07kVg1iR39+utI9NZq+ZGbD+wN6ON64fX24fXX/mcjtz4pkNiVwgc9lfjV7nirTh69qSvzXNWeOuZU613+aNE/gkirGn+nGLxPrO7OoPjqYckSimXNMxHlz9PLSUdZnbcUwvS5LHcNmIiL1HfNgjIvYEOzfjxf1/H+vXOeY+Tmn7kCyg04U11Q+o9NGdc2v6Hh9RKWZKl1seWlGHGUW/Ibc9CaQ3NlB2Ue/IwozM3daVReLowKNDO/yrc2qfzLRb1+1v8ooEGvrg/ASiH8cJtecEH3KiofLJkdkHoscG0pnzlYanHFF3YOmwckRZZOR6rawngMNzNYuHA+tqlCOlBKcUTde6MtVFr/1YdU5IhKIg11Rqau406OqFjvepo83mQtTbmSufTWZ9vdL2M8tmosh0risndpJlOk8ZRc1lLpS0pT6elfaeqIbNuX0Jb8aV3uwiciQi/1hE/r2IfFFEflBEronIx0Xky9v/jx/dUkRExNPCVc34XwHwL0II341NKagvAvh5AJ8IIbwbwCe2nyMiIt6iuEoV10MAfxHAfwUAYVN2sxGRnwDwvu1hHwbwKQAfelR79y3Bhylm+RggNvHZCuToJQB4mTTAbp/afYtbFCG1JslmV5WT1JeRd3bF9oBEKnoSr0gmtsdkzeFGcJFlJGyRB6t/l7OmGwlIHHR2tf8OLcSen9iV9GqkbslA50qdq2FkrFM3G7326wh0nLsWbmJobBuBkkL6hDQEO3vcMNNGigMXoZfrsTW5V4Vb0QexArKy15kQk7GkiMj5hW3j3qn2cXXP7lte6L7GRURy5B0X7M1bm02zIs241JWG4rFK6Tp9qamKoiNzp+VXcclxfAAABzVJREFUYHO+BE7AkXCVN/t3AHgVwP8hIn8gIv/7tnTzrRDCS9tjXsam2mtERMRbFFd52DMAfx7Ar4YQvg/AAs5kDyEEPECfXkSeF5HPiMhn3mxnIyIi3jiu8rC/CODFEMKnt5//MTYP/ysichsAtv/feb0vhxBeCCE8F0J47nF0OCIi4o3hKvXZXxaRr4vIe0IIX8KmJvsXtv8+AOAXt/9/5Fs5cfeQfZ56Y4+VO9w4kYH+LumHX7MZZXcWSg0lJHAwcyWPk1JXBZaONpv0RLeRv1o4QQM0etzKlTSqSJhx2TldcM7MI0GD0Nh+HIlSVOPSttGTL7rM9NoOnA/JP/Pj7LrdN6IMrVL7259ahc+QUHRaYjPFMqK2MsrMSzMn3ECCn+3E3o5TLiVNmu9Jaq/lRPRcSW/3LRtt887JNy63v7k4N8edttrGa4O9zrNT9cvPnJAIe9UpfRhKS9FNaR0k96IlLHpKWYYT59v3LEbi7s37SYfDgwPorsyz/3cAfkNECgBfBfBfY3O7/LaIfBDA1wD81BXbioiIeAq40sMeQvgsgNczw3/08XYnIiLiSeEtI17B8Ct9bMywce4sJazO1Zy+s7D2zNFruq8r1BQ7Lb0QgjY6nrkIN6oWWq61vTa1lNFAlWC9OEZ3TNrwTm++IFqnJzu7CpauSkgLbzS29Mw9SkApO42MOxus+ZmzDnv+iu1/QvQdW7SVpXWk0YSUofT0nV6ncTUyp+s31rErVvZaOrLIs1znonGJQSU5fqeDpSJPSLuuvqfzklxYM35+QRTa2vajbnWszmpH6dJlT0X7kbm7uKHKrXlr7xdQRF1NZWhZy3DTD90ueiuAEYb6/gYehBgbHxGxJ4gPe0TEniA+7BERe4Kn5rP7X5mrhs+yt9O7qMk5CQbIyvpuCyi91NcqlHjhakfPjig7qbO+VU9+6LpQ6iOB84eplysnjsFijllmKZ4+YTqFKCkn6pCs9HxpYn23A+J/FiQCGZwQR7bkksp2faM5oZURyiJLxfqhPVNDraWC0rH2mcNDB19HjbTQq5EX+NS7oiXh9c7RTgtaP1k6n7q90Gt5bX1yuX1ybudl2WoGZb222ZRnNeVdOj2JrKe1lan2NynttUyIAswGe+NyvcGUahpIY8dbciqz7drIZ5vvSWqpZEZ8s0dE7Aniwx4RsSeQ8BCd6cd+MpFXsQnAuQHgtZ2d+PXxVugDEPvhEfth8a3249tDCDdfb8dOH/bLk4p85mnHyr8V+hD7Efuxy35EMz4iYk8QH/aIiD3B03rYX3hK52W8FfoAxH54xH5YPLZ+PBWfPSIiYveIZnxExJ5gpw+7iLxfRL4kIl8RkZ2p0YrIr4vIHRH5HP1t51LYIvJOEfmkiHxBRD4vIj/7NPoiIpWI/K6I/OG2H39n+/fvEJFPb+fnt7b6BU8cIpJu9Q0/9rT6ISJ/LCL/TkQ+e19C7SndI09Mtn1nD7uIpAD+NwD/OYDvAfDTIvI9Ozr9PwDwfve3pyGF3QH4uRDC9wD4AQA/sx2DXfelBvAjIYTvBfBeAO8XkR8A8EsAfjmE8F0ATgB88An34z5+Fht58vt4Wv34SyGE9xLV9TTukScn2x5C2Mk/AD8I4F/S518A8As7PP+7AHyOPn8JwO3t9m0AX9pVX6gPHwHwY0+zLwDGAP5fAH8Bm+CN7PXm6wme/9ntDfwjAD6GjSrZ0+jHHwO44f6203kBcAjg/8N2Le1x92OXZvw7AHydPr+4/dvTwlOVwhaRdwH4PgCffhp92ZrOn8VGKPTjAP4IwGkI4X5Wya7m5+8D+FvQXKjrT6kfAcC/EpHfF5Hnt3/b9bw8Udn2uECHh0thPwmIyBTAPwHwN0MIRjJlV30JIfQhhPdi82b9fgDf/aTP6SEifwXAnRDC7+/63K+DHw4h/Hls3MyfEZG/yDt3NC9vSrb9Udjlw/4NAO+kz89u//a0cCUp7McNEcmxedB/I4TwT59mXwAghHAK4JPYmMtHIpelaHYxPz8E4K+KyB8D+E1sTPlfeQr9QAjhG9v/7wD4HWx+AHc9L29Ktv1R2OXD/nsA3r1daS0A/DUAH93h+T0+io0ENvAGpLDfCEREAPwagC+GEP7e0+qLiNwUkaPt9gibdYMvYvPQ/+Su+hFC+IUQwrMhhHdhcz/83yGEv7HrfojIREQO7m8D+MsAPocdz0sI4WUAXxeR92z/dF+2/fH040kvfLiFhh8H8B+w8Q//px2e9x8CeAkb7coXsVndvY7NwtCXAfxfAK7toB8/jI0J9m8BfHb778d33RcA/wmAP9j243MA/uft378TwO8C+AqAfwSg3OEcvQ/Ax55GP7bn+8Ptv8/fvzef0j3yXgCf2c7NPwNw/Lj6ESPoIiL2BHGBLiJiTxAf9oiIPUF82CMi9gTxYY+I2BPEhz0iYk8QH/aIiD1BfNgjIvYE8WGPiNgT/P/I8c0ACaqG4wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ymCX2tT8yQe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}